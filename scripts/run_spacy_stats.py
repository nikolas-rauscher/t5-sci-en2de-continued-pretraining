#!/usr/bin/env python3
"""
Skript fÃ¼r spaCy-basierte DataTrove-Stats.
Dieses Skript verwendet die separate spaCy-Umgebung mit NumPy 1.x KompatibilitÃ¤t.
"""

import os
import sys
import logging
import shutil
import wandb
from datetime import datetime

# Projekt-Root ins PYTHONPATH
script_dir = os.path.dirname(__file__)
proj_root = os.path.abspath(os.path.join(script_dir, '..'))
sys.path.insert(0, proj_root)

import hydra
from hydra.core.hydra_config import HydraConfig
from omegaconf import DictConfig, OmegaConf
from datatrove.executor import LocalPipelineExecutor
from datatrove.pipeline.readers import ParquetReader

# Hydra-Logger verwenden
log = logging.getLogger(__name__)

def get_datatrove_logging_dir(pipeline_type: str, limit_docs: int) -> str:
    """Erstellt DataTrove-Logging-Verzeichnis basierend auf Hydra-Output-Dir."""
    try:
        # Hydra-Output-Verzeichnis verwenden
        hydra_cfg = HydraConfig.get()
        hydra_output_dir = hydra_cfg.runtime.output_dir
        
        # Parameter-String fÃ¼r bessere Identifikation
        if limit_docs == -1:
            param_str = "unlimited"
        else:
            param_str = f"limit{limit_docs}"
        
        # DataTrove-Logs als Subdirectory im Hydra-Output
        datatrove_dir = os.path.join(hydra_output_dir, f"datatrove_{pipeline_type}_{param_str}")
        
        log.info(f"Using Hydra output directory: {hydra_output_dir}")
        log.info(f"DataTrove logs will be saved to: {datatrove_dir}")
        
        return datatrove_dir
        
    except Exception as e:
        # Fallback zu altem System falls Hydra-Integration fehlt
        log.warning(f"Could not get Hydra output dir ({e}), using fallback")
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        fallback_dir = f"logs/{timestamp}_{pipeline_type}_{limit_docs}"
        return fallback_dir

def get_dual_output_dirs(cfg: DictConfig):
    """Erstellt sowohl Hydra-basierte als auch zentrale Output-Verzeichnisse."""
    try:
        hydra_cfg = HydraConfig.get()
        hydra_output_dir = hydra_cfg.runtime.output_dir
        
        # Primary: Stats im Hydra-Output (mit vollstÃ¤ndiger Historie)
        primary_stats_dir = os.path.join(hydra_output_dir, "stats")
        
        # Secondary: Beachte cfg.stats.paths.output_folder Override
        if hasattr(cfg.stats.paths, 'output_folder'):
            central_stats_dir = cfg.stats.paths.output_folder
        else:
            central_stats_dir = os.path.join(proj_root, "data", "statistics")
        
        os.makedirs(primary_stats_dir, exist_ok=True)
        os.makedirs(central_stats_dir, exist_ok=True)
        
        log.info(f"ğŸ“ Primary stats output: {primary_stats_dir}")
        log.info(f"ğŸ“ Central stats output: {central_stats_dir}")
        
        return primary_stats_dir, central_stats_dir
        
    except Exception as e:
        log.warning(f"Could not setup dual output ({e}), using fallback")
        if hasattr(cfg.stats.paths, 'output_folder'):
            fallback_dir = cfg.stats.paths.output_folder
        else:
            fallback_dir = os.path.join(proj_root, "data", "statistics")
        os.makedirs(fallback_dir, exist_ok=True)
        return fallback_dir, fallback_dir

def sync_stats_to_central(primary_dir: str, central_dir: str):
    """Synchronisiert Stats vom primÃ¤ren zum zentralen Verzeichnis."""
    try:
        log.info("ğŸ”„ Syncing stats to central directory...")
        
        # Kopiere alle Stats-Ordner
        for item in os.listdir(primary_dir):
            src_path = os.path.join(primary_dir, item)
            dst_path = os.path.join(central_dir, item)
            
            if os.path.isdir(src_path):
                if os.path.exists(dst_path):
                    shutil.rmtree(dst_path)  # Entferne alte Version
                shutil.copytree(src_path, dst_path)
                log.info(f"  âœ… Synced {item}")
        
        log.info(f"ğŸ“‹ Stats available at: {central_dir}")
        
    except Exception as e:
        log.error(f"âŒ Failed to sync stats: {e}")

@hydra.main(
    version_base="1.3",
    config_path="../configs",
    config_name="stats/compute_spacy_stats"
)
def main(cfg: DictConfig) -> None:
    log.info("ğŸš€ Starting spaCy Stats Pipeline")
    log.info("Configuration:")
    log.info(f"\n{OmegaConf.to_yaml(cfg)}")

    # Limit-Wert 
    limit_val = cfg.stats.limit_documents

    # Automatische Tag-Generierung basierend auf Run-Parametern
    base_tags = list(cfg.stats.logger.get("tags", []))
    
    if limit_val != -1:
        base_tags.extend(["test_run", f"limit-{limit_val}"])
    else:
        base_tags.extend(["full-dataset"])

    # Wandb initialisieren mit automatischen Tags
    logger_config = OmegaConf.to_container(cfg.stats.logger, resolve=True)
    logger_config["tags"] = base_tags
    
    wandb_run = wandb.init(
        **logger_config,
        config=OmegaConf.to_container(cfg, resolve=True)
    )
    log.info(f"ğŸŒ Wandb initialized: {wandb_run.url}")

    # ÃœberprÃ¼fe, ob die Konfiguration korrekt ist
    if not hasattr(cfg, 'stats'):
        raise ValueError("Die Konfiguration enthÃ¤lt keinen 'stats'-Abschnitt")
    
    # Dual-Output-System einrichten
    primary_stats_dir, central_stats_dir = get_dual_output_dirs(cfg)
    
    limit_val = cfg.stats.limit_documents
    log.info(f"ğŸ“„ Processing limit: {limit_val} documents")
    
    # Hydra-integriertes Logging-Verzeichnis
    datatrove_logging_dir = get_datatrove_logging_dir("spacy_stats", limit_val)
    
    # Pipeline aufbauen
    pipeline = [
        ParquetReader(
            data_folder=cfg.stats.paths.input_folder,
            glob_pattern=cfg.stats.paths.src_pattern,
            limit=limit_val,
            text_key=cfg.stats.reader.text_key,
            id_key=cfg.stats.reader.id_key,
            default_metadata=OmegaConf.to_container(cfg.stats.reader.default_metadata, resolve=True)
        ),
    ]
    
    # FÃ¼ge die spaCy-basierten Stats-Module zur Pipeline hinzu
    modules = cfg.stats.pipeline.stats_modules
    added_modules = []
    
    # SentenceStats
    if hasattr(modules, 'sentence_stats'):
        sentence_stats = hydra.utils.instantiate(
            modules.sentence_stats,
            output_folder=os.path.join(primary_stats_dir, modules.sentence_stats.output_folder)
        )
        pipeline.append(sentence_stats)
        added_modules.append("SentenceStats")
    
    # WordStats
    if hasattr(modules, 'word_stats'):
        word_stats = hydra.utils.instantiate(
            modules.word_stats,
            output_folder=os.path.join(primary_stats_dir, modules.word_stats.output_folder)
        )
        pipeline.append(word_stats)
        added_modules.append("WordStats")
    
    # LangStats
    if hasattr(modules, 'lang_stats'):
        lang_stats = hydra.utils.instantiate(
            modules.lang_stats,
            output_folder=os.path.join(primary_stats_dir, modules.lang_stats.output_folder)
        )
        pipeline.append(lang_stats)
        added_modules.append("LangStats")
    
    #Enriched documents mit Stats speichern
    if hasattr(cfg.stats, 'save_enriched_docs') and cfg.stats.save_enriched_docs:
        from datatrove.pipeline.writers import ParquetWriter
        enriched_output = os.path.join(primary_stats_dir, "enriched_documents_statistics_v2")
        pipeline.append(ParquetWriter(enriched_output))
        log.info(f"ğŸ’¾ Enriched documents will be saved to: {enriched_output}")
        added_modules.append("ParquetWriter(enriched)")
    
    log.info(f"ğŸ“Š Pipeline modules: {', '.join(added_modules)}")
    log.info(f"ğŸ”§ Total pipeline steps: {len(pipeline)}")
    
    # FÃ¼hre die Pipeline aus
    executor = LocalPipelineExecutor(
        pipeline=pipeline,
        tasks=cfg.stats.tasks,
        workers=cfg.stats.workers,
        logging_dir=datatrove_logging_dir
    )
    
    log.info("ğŸƒ Starting pipeline execution...")
    start_time = datetime.now()
    executor.run()
    execution_time = (datetime.now() - start_time).total_seconds()
    log.info("âœ… Pipeline completed successfully!")
    
    # Nur wichtigste Pipeline-Metriken fÃ¼r Vergleiche
    wandb.log({
        "execution_time": execution_time,
        "limit_documents": limit_val,
        "active_modules": added_modules,  # Liste der verwendeten Module
        
        # Pipeline-Parameter
        "tasks": cfg.stats.tasks,
        "workers": cfg.stats.workers,
        "save_enriched_docs": cfg.stats.get("save_enriched_docs", False),
        
        # Input-Parameter
        "input_folder": cfg.stats.paths.input_folder,
        "src_pattern": cfg.stats.paths.src_pattern,
        "text_key": cfg.stats.reader.text_key,
        "id_key": cfg.stats.reader.id_key,
    })
    
    # Synchronisiere Stats zum zentralen Verzeichnis
    if primary_stats_dir != central_stats_dir:
        sync_stats_to_central(primary_stats_dir, central_stats_dir)
    
    log.info(f"ğŸ“ Primary stats (with history): {primary_stats_dir}")
    log.info(f"ğŸ“ Central stats (latest): {central_stats_dir}")
    log.info(f"ğŸ“‹ Logs saved to: {datatrove_logging_dir}")
    wandb.finish()

if __name__ == "__main__":
    main() 