# German continued pretraining trainer configuration
_target_: lightning.pytorch.Trainer

# Training configuration optimized for German continued pretraining
max_steps: 15000  # Short continued pretraining (proven effective for transfer)
gradient_clip_val: 1.0  # Proven optimal value from English experiments

# Logging and checkpointing
log_every_n_steps: 50
val_check_interval: 1000  # Validate every 1k steps for monitoring

# Hardware configuration
accelerator: gpu
devices: 1  # 1 GPU sufficient for German continued pretraining
strategy: auto  # Single GPU training
precision: "bf16-mixed"  # Mixed precision for efficiency

# Checkpointing strategy
enable_checkpointing: true
default_root_dir: cross_lingual_transfer/logs

# Callbacks configuration
callbacks:
  - _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: "val/perplexity"
    mode: "min" 
    save_top_k: 3
    save_last: true
    filename: "step-{step}-val_ppl-{val/perplexity:.5f}"
    auto_insert_metric_name: false
    every_n_train_steps: 2500  # Save every 2.5k steps
    
  - _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: "val/perplexity"
    mode: "min"
    patience: 5  # Stop if no improvement for 5 validation cycles
    min_delta: 0.001
    
  - _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: "step"

# Memory and performance optimization
enable_model_summary: false
enable_progress_bar: true
deterministic: false  # Allow for performance optimization
use_distributed_sampler: false  # Use custom sampler for deterministic training