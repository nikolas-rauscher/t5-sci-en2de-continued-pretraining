{
  "experiment_metadata": {
    "name": "scientific_crosslingual_transfer_eval_full_15k",
    "description": "MMLU 0-shot Vergleich: HF-GermanT5, Wechsel-Init, German native 15k, Wechsel-Weitertraining 15k.",
    "timestamp": "20250923_193728",
    "config": {
      "task_name": "eval_pipeline",
      "experiment_name": "scientific_crosslingual_transfer_eval_full_15k",
      "description": "MMLU 0-shot Vergleich: HF-GermanT5, Wechsel-Init, German native 15k, Wechsel-Weitertraining 15k.",
      "logger": {
        "wandb": {
          "project": "BA-T5-CrossLingual",
          "entity": "nikolas-rauscher-dfki",
          "group": "crosslingual_transfer_eval_full_15k-final",
          "name": "scientific_crosslingual_transfer_eval_full_15k-final",
          "tags": [
            "global-mmlu",
            "crosslingual",
            "wechsel",
            "transfer",
            "15k",
            "loglikelihood"
          ]
        }
      },
      "models": [
        {
          "source_path": "GermanT5/t5-efficient-gc4-german-base-nl36",
          "name": "hf-germanT5-base"
        },
        {
          "source_path": "cross_lingual_transfer/logs/native_baseline/train/runs/2025-09-18_23-54-00/checkpoints/steps/step-step=015000.ckpt",
          "name": "hf-germanT5-continued-pretraind-on-german-15k"
        },
        {
          "source_path": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k",
          "name": "wechsel-transfer-en-487k"
        },
        {
          "source_path": "cross_lingual_transfer/logs/train/runs/2025-09-18_21-53-03/checkpoints/steps/step-step=015000.ckpt",
          "name": "wechsel-transfer-en-487k-continued-pretraind-on-german-15k"
        },
        {
          "source_path": "pretraining_logs_lr_001_OPTIMIZED_clean_restart/train/runs/2025-09-08_02-33-22/checkpoints/best/step-487500-val_ppl-3.72168.ckpt",
          "name": "gold-continued-pretraind-on-english-487k"
        },
        {
          "source_path": "t5-base",
          "name": "t5-base"
        }
      ],
      "benchmarks": [
        {
          "name": "global_mmlu_en",
          "tasks": [
            "global_mmlu_full_en"
          ],
          "shots": [
            0
          ],
          "seed": 42,
          "device": "cuda",
          "batch_size": "auto"
        },
        {
          "name": "global_mmlu_de",
          "tasks": [
            "global_mmlu_full_de"
          ],
          "shots": [
            0
          ],
          "seed": 42,
          "device": "cuda",
          "batch_size": "auto"
        }
      ],
      "eval": {
        "cleanup_temp_models": true
      },
      "device": "cuda",
      "batch_size": "auto",
      "seed": 42,
      "diagnostics": {
        "write_out": false
      },
      "paths": {
        "root_dir": "/netscratch/nrauscher/projects/BA-hydra",
        "data_dir": "/netscratch/nrauscher/projects/BA-hydra/data/",
        "log_dir": "/netscratch/nrauscher/projects/BA-hydra/logs/",
        "output_dir": "/netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53",
        "work_dir": "/netscratch/nrauscher/projects/BA-hydra"
      }
    }
  },
  "results": {
    "wechsel-transfer-en-487k-continued-pretraind-on-german-15k": {
      "model_config": {
        "source_path": "cross_lingual_transfer/logs/train/runs/2025-09-18_21-53-03/checkpoints/steps/step-step=015000.ckpt",
        "name": "wechsel-transfer-en-487k-continued-pretraind-on-german-15k"
      },
      "model_path": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
      "metadata": {
        "training_steps": 15000,
        "run_date": "2025-09-18",
        "run_time": "21:53:03",
        "run_directory": "2025-09-18_21-53-03",
        "learning_rate": null,
        "batch_size": null,
        "model_name": null,
        "actual_global_step": 15000,
        "epoch": 0,
        "lightning_version": "2.5.1.post0",
        "source_type": "checkpoint",
        "original_checkpoint": "/netscratch/nrauscher/projects/BA-hydra/cross_lingual_transfer/logs/train/runs/2025-09-18_21-53-03/checkpoints/steps/step-step=015000.ckpt"
      },
      "benchmarks": {
        "global_mmlu_en": {
          "0_shot": {
            "parse_status": "partial",
            "error": "Could not find primary metric",
            "raw_results": {
              "results": {
                "global_mmlu_full_en": {
                  "acc,none": 0.27382139296396524,
                  "acc_stderr,none": 0.0037313423151957062,
                  "alias": "global_mmlu_full_en"
                },
                "global_mmlu_full_en_humanities": {
                  "acc,none": 0.2558979808714134,
                  "acc_stderr,none": 0.006346600606083319,
                  "alias": " - global_mmlu_full_en_humanities"
                },
                "global_mmlu_full_en_formal_logic": {
                  "alias": "  - global_mmlu_full_en_formal_logic",
                  "acc,none": 0.373015873015873,
                  "acc_stderr,none": 0.04325506042017086
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "alias": "  - global_mmlu_full_en_high_school_european_history",
                  "acc,none": 0.3333333333333333,
                  "acc_stderr,none": 0.036810508691615486
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "alias": "  - global_mmlu_full_en_high_school_us_history",
                  "acc,none": 0.3137254901960784,
                  "acc_stderr,none": 0.03256685484460388
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "alias": "  - global_mmlu_full_en_high_school_world_history",
                  "acc,none": 0.23628691983122363,
                  "acc_stderr,none": 0.027652153144159267
                },
                "global_mmlu_full_en_international_law": {
                  "alias": "  - global_mmlu_full_en_international_law",
                  "acc,none": 0.1487603305785124,
                  "acc_stderr,none": 0.03248470083807195
                },
                "global_mmlu_full_en_jurisprudence": {
                  "alias": "  - global_mmlu_full_en_jurisprudence",
                  "acc,none": 0.25925925925925924,
                  "acc_stderr,none": 0.042365112580946315
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "alias": "  - global_mmlu_full_en_logical_fallacies",
                  "acc,none": 0.2331288343558282,
                  "acc_stderr,none": 0.0332201579577674
                },
                "global_mmlu_full_en_moral_disputes": {
                  "alias": "  - global_mmlu_full_en_moral_disputes",
                  "acc,none": 0.2254335260115607,
                  "acc_stderr,none": 0.022497230190967547
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "alias": "  - global_mmlu_full_en_moral_scenarios",
                  "acc,none": 0.27039106145251396,
                  "acc_stderr,none": 0.014854993938010085
                },
                "global_mmlu_full_en_philosophy": {
                  "alias": "  - global_mmlu_full_en_philosophy",
                  "acc,none": 0.2508038585209003,
                  "acc_stderr,none": 0.024619771956697168
                },
                "global_mmlu_full_en_prehistory": {
                  "alias": "  - global_mmlu_full_en_prehistory",
                  "acc,none": 0.23765432098765432,
                  "acc_stderr,none": 0.023683591837008546
                },
                "global_mmlu_full_en_professional_law": {
                  "alias": "  - global_mmlu_full_en_professional_law",
                  "acc,none": 0.2561929595827901,
                  "acc_stderr,none": 0.011149173153110582
                },
                "global_mmlu_full_en_world_religions": {
                  "alias": "  - global_mmlu_full_en_world_religions",
                  "acc,none": 0.17543859649122806,
                  "acc_stderr,none": 0.029170885500727686
                },
                "global_mmlu_full_en_other": {
                  "acc,none": 0.25619568715803026,
                  "acc_stderr,none": 0.007708835444228797,
                  "alias": " - global_mmlu_full_en_other"
                },
                "global_mmlu_full_en_business_ethics": {
                  "alias": "  - global_mmlu_full_en_business_ethics",
                  "acc,none": 0.23,
                  "acc_stderr,none": 0.042295258468165044
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "alias": "  - global_mmlu_full_en_clinical_knowledge",
                  "acc,none": 0.3169811320754717,
                  "acc_stderr,none": 0.028637235639800918
                },
                "global_mmlu_full_en_college_medicine": {
                  "alias": "  - global_mmlu_full_en_college_medicine",
                  "acc,none": 0.34104046242774566,
                  "acc_stderr,none": 0.036146654241808254
                },
                "global_mmlu_full_en_global_facts": {
                  "alias": "  - global_mmlu_full_en_global_facts",
                  "acc,none": 0.18,
                  "acc_stderr,none": 0.038612291966536955
                },
                "global_mmlu_full_en_human_aging": {
                  "alias": "  - global_mmlu_full_en_human_aging",
                  "acc,none": 0.10762331838565023,
                  "acc_stderr,none": 0.020799400082880008
                },
                "global_mmlu_full_en_management": {
                  "alias": "  - global_mmlu_full_en_management",
                  "acc,none": 0.3786407766990291,
                  "acc_stderr,none": 0.048026946982589726
                },
                "global_mmlu_full_en_marketing": {
                  "alias": "  - global_mmlu_full_en_marketing",
                  "acc,none": 0.19658119658119658,
                  "acc_stderr,none": 0.02603538609895129
                },
                "global_mmlu_full_en_medical_genetics": {
                  "alias": "  - global_mmlu_full_en_medical_genetics",
                  "acc,none": 0.26,
                  "acc_stderr,none": 0.0440844002276808
                },
                "global_mmlu_full_en_miscellaneous": {
                  "alias": "  - global_mmlu_full_en_miscellaneous",
                  "acc,none": 0.20434227330779056,
                  "acc_stderr,none": 0.0144191239809319
                },
                "global_mmlu_full_en_nutrition": {
                  "alias": "  - global_mmlu_full_en_nutrition",
                  "acc,none": 0.3006535947712418,
                  "acc_stderr,none": 0.026256053835718964
                },
                "global_mmlu_full_en_professional_accounting": {
                  "alias": "  - global_mmlu_full_en_professional_accounting",
                  "acc,none": 0.26595744680851063,
                  "acc_stderr,none": 0.026358065698880585
                },
                "global_mmlu_full_en_professional_medicine": {
                  "alias": "  - global_mmlu_full_en_professional_medicine",
                  "acc,none": 0.4227941176470588,
                  "acc_stderr,none": 0.030008562845003472
                },
                "global_mmlu_full_en_virology": {
                  "alias": "  - global_mmlu_full_en_virology",
                  "acc,none": 0.21084337349397592,
                  "acc_stderr,none": 0.0317555478662992
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc,none": 0.30581735456613585,
                  "acc_stderr,none": 0.008257234929370664,
                  "alias": " - global_mmlu_full_en_social_sciences"
                },
                "global_mmlu_full_en_econometrics": {
                  "alias": "  - global_mmlu_full_en_econometrics",
                  "acc,none": 0.21929824561403508,
                  "acc_stderr,none": 0.03892431106518754
                },
                "global_mmlu_full_en_high_school_geography": {
                  "alias": "  - global_mmlu_full_en_high_school_geography",
                  "acc,none": 0.3484848484848485,
                  "acc_stderr,none": 0.033948539651564025
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "alias": "  - global_mmlu_full_en_high_school_government_and_politics",
                  "acc,none": 0.3471502590673575,
                  "acc_stderr,none": 0.03435696168361355
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "alias": "  - global_mmlu_full_en_high_school_macroeconomics",
                  "acc,none": 0.3564102564102564,
                  "acc_stderr,none": 0.024283140529467298
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "alias": "  - global_mmlu_full_en_high_school_microeconomics",
                  "acc,none": 0.3319327731092437,
                  "acc_stderr,none": 0.030588697013783663
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "alias": "  - global_mmlu_full_en_high_school_psychology",
                  "acc,none": 0.3504587155963303,
                  "acc_stderr,none": 0.020456077599824457
                },
                "global_mmlu_full_en_human_sexuality": {
                  "alias": "  - global_mmlu_full_en_human_sexuality",
                  "acc,none": 0.26717557251908397,
                  "acc_stderr,none": 0.03880848301082394
                },
                "global_mmlu_full_en_professional_psychology": {
                  "alias": "  - global_mmlu_full_en_professional_psychology",
                  "acc,none": 0.2238562091503268,
                  "acc_stderr,none": 0.016863008585416617
                },
                "global_mmlu_full_en_public_relations": {
                  "alias": "  - global_mmlu_full_en_public_relations",
                  "acc,none": 0.22727272727272727,
                  "acc_stderr,none": 0.04013964554072773
                },
                "global_mmlu_full_en_security_studies": {
                  "alias": "  - global_mmlu_full_en_security_studies",
                  "acc,none": 0.37142857142857144,
                  "acc_stderr,none": 0.030932858792789855
                },
                "global_mmlu_full_en_sociology": {
                  "alias": "  - global_mmlu_full_en_sociology",
                  "acc,none": 0.2885572139303483,
                  "acc_stderr,none": 0.03203841040213324
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "alias": "  - global_mmlu_full_en_us_foreign_policy",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.04351941398892446
                },
                "global_mmlu_full_en_stem": {
                  "acc,none": 0.28671106882334285,
                  "acc_stderr,none": 0.007988842996145709,
                  "alias": " - global_mmlu_full_en_stem"
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "alias": "  - global_mmlu_full_en_abstract_algebra",
                  "acc,none": 0.21,
                  "acc_stderr,none": 0.040936018074033256
                },
                "global_mmlu_full_en_anatomy": {
                  "alias": "  - global_mmlu_full_en_anatomy",
                  "acc,none": 0.24444444444444444,
                  "acc_stderr,none": 0.03712537833614865
                },
                "global_mmlu_full_en_astronomy": {
                  "alias": "  - global_mmlu_full_en_astronomy",
                  "acc,none": 0.3355263157894737,
                  "acc_stderr,none": 0.038424985593952694
                },
                "global_mmlu_full_en_college_biology": {
                  "alias": "  - global_mmlu_full_en_college_biology",
                  "acc,none": 0.3055555555555556,
                  "acc_stderr,none": 0.03852084696008534
                },
                "global_mmlu_full_en_college_chemistry": {
                  "alias": "  - global_mmlu_full_en_college_chemistry",
                  "acc,none": 0.45,
                  "acc_stderr,none": 0.05
                },
                "global_mmlu_full_en_college_computer_science": {
                  "alias": "  - global_mmlu_full_en_college_computer_science",
                  "acc,none": 0.36,
                  "acc_stderr,none": 0.048241815132442176
                },
                "global_mmlu_full_en_college_mathematics": {
                  "alias": "  - global_mmlu_full_en_college_mathematics",
                  "acc,none": 0.29,
                  "acc_stderr,none": 0.045604802157206845
                },
                "global_mmlu_full_en_college_physics": {
                  "alias": "  - global_mmlu_full_en_college_physics",
                  "acc,none": 0.3431372549019608,
                  "acc_stderr,none": 0.04724007352383889
                },
                "global_mmlu_full_en_computer_security": {
                  "alias": "  - global_mmlu_full_en_computer_security",
                  "acc,none": 0.23,
                  "acc_stderr,none": 0.04229525846816506
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "alias": "  - global_mmlu_full_en_conceptual_physics",
                  "acc,none": 0.20851063829787234,
                  "acc_stderr,none": 0.026556982117838728
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "alias": "  - global_mmlu_full_en_electrical_engineering",
                  "acc,none": 0.2413793103448276,
                  "acc_stderr,none": 0.03565998174135302
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "alias": "  - global_mmlu_full_en_elementary_mathematics",
                  "acc,none": 0.2671957671957672,
                  "acc_stderr,none": 0.02278967314577657
                },
                "global_mmlu_full_en_high_school_biology": {
                  "alias": "  - global_mmlu_full_en_high_school_biology",
                  "acc,none": 0.2967741935483871,
                  "acc_stderr,none": 0.025988500792411898
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "alias": "  - global_mmlu_full_en_high_school_chemistry",
                  "acc,none": 0.2660098522167488,
                  "acc_stderr,none": 0.031089826002937523
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "alias": "  - global_mmlu_full_en_high_school_computer_science",
                  "acc,none": 0.24,
                  "acc_stderr,none": 0.04292346959909283
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "alias": "  - global_mmlu_full_en_high_school_mathematics",
                  "acc,none": 0.25555555555555554,
                  "acc_stderr,none": 0.026593939101844065
                },
                "global_mmlu_full_en_high_school_physics": {
                  "alias": "  - global_mmlu_full_en_high_school_physics",
                  "acc,none": 0.3509933774834437,
                  "acc_stderr,none": 0.03896981964257374
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "alias": "  - global_mmlu_full_en_high_school_statistics",
                  "acc,none": 0.42592592592592593,
                  "acc_stderr,none": 0.03372343271653063
                },
                "global_mmlu_full_en_machine_learning": {
                  "alias": "  - global_mmlu_full_en_machine_learning",
                  "acc,none": 0.16071428571428573,
                  "acc_stderr,none": 0.0348594609647574
                }
              },
              "groups": {
                "global_mmlu_full_en": {
                  "acc,none": 0.27382139296396524,
                  "acc_stderr,none": 0.0037313423151957062,
                  "alias": "global_mmlu_full_en"
                },
                "global_mmlu_full_en_humanities": {
                  "acc,none": 0.2558979808714134,
                  "acc_stderr,none": 0.006346600606083319,
                  "alias": " - global_mmlu_full_en_humanities"
                },
                "global_mmlu_full_en_other": {
                  "acc,none": 0.25619568715803026,
                  "acc_stderr,none": 0.007708835444228797,
                  "alias": " - global_mmlu_full_en_other"
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc,none": 0.30581735456613585,
                  "acc_stderr,none": 0.008257234929370664,
                  "alias": " - global_mmlu_full_en_social_sciences"
                },
                "global_mmlu_full_en_stem": {
                  "acc,none": 0.28671106882334285,
                  "acc_stderr,none": 0.007988842996145709,
                  "alias": " - global_mmlu_full_en_stem"
                }
              },
              "group_subtasks": {
                "global_mmlu_full_en_humanities": [
                  "global_mmlu_full_en_world_religions",
                  "global_mmlu_full_en_international_law",
                  "global_mmlu_full_en_moral_scenarios",
                  "global_mmlu_full_en_high_school_us_history",
                  "global_mmlu_full_en_jurisprudence",
                  "global_mmlu_full_en_high_school_world_history",
                  "global_mmlu_full_en_logical_fallacies",
                  "global_mmlu_full_en_philosophy",
                  "global_mmlu_full_en_professional_law",
                  "global_mmlu_full_en_high_school_european_history",
                  "global_mmlu_full_en_formal_logic",
                  "global_mmlu_full_en_moral_disputes",
                  "global_mmlu_full_en_prehistory"
                ],
                "global_mmlu_full_en_social_sciences": [
                  "global_mmlu_full_en_us_foreign_policy",
                  "global_mmlu_full_en_public_relations",
                  "global_mmlu_full_en_econometrics",
                  "global_mmlu_full_en_high_school_microeconomics",
                  "global_mmlu_full_en_human_sexuality",
                  "global_mmlu_full_en_security_studies",
                  "global_mmlu_full_en_professional_psychology",
                  "global_mmlu_full_en_high_school_geography",
                  "global_mmlu_full_en_high_school_government_and_politics",
                  "global_mmlu_full_en_high_school_psychology",
                  "global_mmlu_full_en_high_school_macroeconomics",
                  "global_mmlu_full_en_sociology"
                ],
                "global_mmlu_full_en_other": [
                  "global_mmlu_full_en_clinical_knowledge",
                  "global_mmlu_full_en_medical_genetics",
                  "global_mmlu_full_en_professional_medicine",
                  "global_mmlu_full_en_business_ethics",
                  "global_mmlu_full_en_marketing",
                  "global_mmlu_full_en_global_facts",
                  "global_mmlu_full_en_college_medicine",
                  "global_mmlu_full_en_human_aging",
                  "global_mmlu_full_en_management",
                  "global_mmlu_full_en_nutrition",
                  "global_mmlu_full_en_virology",
                  "global_mmlu_full_en_professional_accounting",
                  "global_mmlu_full_en_miscellaneous"
                ],
                "global_mmlu_full_en_stem": [
                  "global_mmlu_full_en_high_school_statistics",
                  "global_mmlu_full_en_computer_security",
                  "global_mmlu_full_en_machine_learning",
                  "global_mmlu_full_en_abstract_algebra",
                  "global_mmlu_full_en_astronomy",
                  "global_mmlu_full_en_college_computer_science",
                  "global_mmlu_full_en_high_school_biology",
                  "global_mmlu_full_en_high_school_computer_science",
                  "global_mmlu_full_en_high_school_chemistry",
                  "global_mmlu_full_en_elementary_mathematics",
                  "global_mmlu_full_en_college_physics",
                  "global_mmlu_full_en_high_school_physics",
                  "global_mmlu_full_en_college_chemistry",
                  "global_mmlu_full_en_college_biology",
                  "global_mmlu_full_en_college_mathematics",
                  "global_mmlu_full_en_conceptual_physics",
                  "global_mmlu_full_en_anatomy",
                  "global_mmlu_full_en_high_school_mathematics",
                  "global_mmlu_full_en_electrical_engineering"
                ],
                "global_mmlu_full_en": [
                  "global_mmlu_full_en_stem",
                  "global_mmlu_full_en_other",
                  "global_mmlu_full_en_social_sciences",
                  "global_mmlu_full_en_humanities"
                ]
              },
              "configs": {
                "global_mmlu_full_en_abstract_algebra": {
                  "task": "global_mmlu_full_en_abstract_algebra",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x708490241bd0>, subject='abstract_algebra')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_anatomy": {
                  "task": "global_mmlu_full_en_anatomy",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x708490240dc0>, subject='anatomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_astronomy": {
                  "task": "global_mmlu_full_en_astronomy",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x708490242ef0>, subject='astronomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_business_ethics": {
                  "task": "global_mmlu_full_en_business_ethics",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c8217490>, subject='business_ethics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "task": "global_mmlu_full_en_clinical_knowledge",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x708490240700>, subject='clinical_knowledge')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_biology": {
                  "task": "global_mmlu_full_en_college_biology",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084902400d0>, subject='college_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_chemistry": {
                  "task": "global_mmlu_full_en_college_chemistry",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x708490241240>, subject='college_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_computer_science": {
                  "task": "global_mmlu_full_en_college_computer_science",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x708490240820>, subject='college_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_mathematics": {
                  "task": "global_mmlu_full_en_college_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x708490242440>, subject='college_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_medicine": {
                  "task": "global_mmlu_full_en_college_medicine",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c8217520>, subject='college_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_physics": {
                  "task": "global_mmlu_full_en_college_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x708490243130>, subject='college_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_computer_security": {
                  "task": "global_mmlu_full_en_computer_security",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084902437f0>, subject='computer_security')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "task": "global_mmlu_full_en_conceptual_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x708490241870>, subject='conceptual_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_econometrics": {
                  "task": "global_mmlu_full_en_econometrics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c8215900>, subject='econometrics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "task": "global_mmlu_full_en_electrical_engineering",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x708490241750>, subject='electrical_engineering')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "task": "global_mmlu_full_en_elementary_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084902430a0>, subject='elementary_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_formal_logic": {
                  "task": "global_mmlu_full_en_formal_logic",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x70871ecd9990>, subject='formal_logic')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_global_facts": {
                  "task": "global_mmlu_full_en_global_facts",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c8216b00>, subject='global_facts')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_biology": {
                  "task": "global_mmlu_full_en_high_school_biology",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x708490240790>, subject='high_school_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "task": "global_mmlu_full_en_high_school_chemistry",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x708490242dd0>, subject='high_school_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "task": "global_mmlu_full_en_high_school_computer_science",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x708490240a60>, subject='high_school_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "task": "global_mmlu_full_en_high_school_european_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x70871c1084c0>, subject='high_school_european_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_geography": {
                  "task": "global_mmlu_full_en_high_school_geography",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x70871c10ae60>, subject='high_school_geography')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "task": "global_mmlu_full_en_high_school_government_and_politics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c8215fc0>, subject='high_school_government_and_politics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "task": "global_mmlu_full_en_high_school_macroeconomics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c8215120>, subject='high_school_macroeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "task": "global_mmlu_full_en_high_school_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x708490241ea0>, subject='high_school_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "task": "global_mmlu_full_en_high_school_microeconomics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c8216200>, subject='high_school_microeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_physics": {
                  "task": "global_mmlu_full_en_high_school_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084902415a0>, subject='high_school_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "task": "global_mmlu_full_en_high_school_psychology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c8214820>, subject='high_school_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "task": "global_mmlu_full_en_high_school_statistics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x708490242e60>, subject='high_school_statistics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "task": "global_mmlu_full_en_high_school_us_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x70871c109090>, subject='high_school_us_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "task": "global_mmlu_full_en_high_school_world_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x70871c10bac0>, subject='high_school_world_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_human_aging": {
                  "task": "global_mmlu_full_en_human_aging",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c8216e60>, subject='human_aging')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_human_sexuality": {
                  "task": "global_mmlu_full_en_human_sexuality",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c8215e10>, subject='human_sexuality')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_international_law": {
                  "task": "global_mmlu_full_en_international_law",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x70871c1083a0>, subject='international_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_jurisprudence": {
                  "task": "global_mmlu_full_en_jurisprudence",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x70871c108b80>, subject='jurisprudence')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "task": "global_mmlu_full_en_logical_fallacies",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x70871c10add0>, subject='logical_fallacies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_machine_learning": {
                  "task": "global_mmlu_full_en_machine_learning",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x708490243880>, subject='machine_learning')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_management": {
                  "task": "global_mmlu_full_en_management",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c8214e50>, subject='management')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_marketing": {
                  "task": "global_mmlu_full_en_marketing",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c82171c0>, subject='marketing')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_medical_genetics": {
                  "task": "global_mmlu_full_en_medical_genetics",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x708490241510>, subject='medical_genetics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_miscellaneous": {
                  "task": "global_mmlu_full_en_miscellaneous",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c8217250>, subject='miscellaneous')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_moral_disputes": {
                  "task": "global_mmlu_full_en_moral_disputes",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x70871c1088b0>, subject='moral_disputes')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "task": "global_mmlu_full_en_moral_scenarios",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x70871c10bf40>, subject='moral_scenarios')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_nutrition": {
                  "task": "global_mmlu_full_en_nutrition",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c82155a0>, subject='nutrition')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_philosophy": {
                  "task": "global_mmlu_full_en_philosophy",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x70871c1095a0>, subject='philosophy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_prehistory": {
                  "task": "global_mmlu_full_en_prehistory",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084f9dc8160>, subject='prehistory')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_professional_accounting": {
                  "task": "global_mmlu_full_en_professional_accounting",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c8216c20>, subject='professional_accounting')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_professional_law": {
                  "task": "global_mmlu_full_en_professional_law",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x70871c108ca0>, subject='professional_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_professional_medicine": {
                  "task": "global_mmlu_full_en_professional_medicine",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084902408b0>, subject='professional_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_professional_psychology": {
                  "task": "global_mmlu_full_en_professional_psychology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x70871c10b760>, subject='professional_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_public_relations": {
                  "task": "global_mmlu_full_en_public_relations",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c8217400>, subject='public_relations')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_security_studies": {
                  "task": "global_mmlu_full_en_security_studies",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c82145e0>, subject='security_studies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_sociology": {
                  "task": "global_mmlu_full_en_sociology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c8214ca0>, subject='sociology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "task": "global_mmlu_full_en_us_foreign_policy",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c8215000>, subject='us_foreign_policy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_virology": {
                  "task": "global_mmlu_full_en_virology",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7084c8214ee0>, subject='virology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_world_religions": {
                  "task": "global_mmlu_full_en_world_religions",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x70871c10b370>, subject='world_religions')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                }
              },
              "versions": {
                "global_mmlu_full_en": 0.0,
                "global_mmlu_full_en_abstract_algebra": 0.0,
                "global_mmlu_full_en_anatomy": 0.0,
                "global_mmlu_full_en_astronomy": 0.0,
                "global_mmlu_full_en_business_ethics": 0.0,
                "global_mmlu_full_en_clinical_knowledge": 0.0,
                "global_mmlu_full_en_college_biology": 0.0,
                "global_mmlu_full_en_college_chemistry": 0.0,
                "global_mmlu_full_en_college_computer_science": 0.0,
                "global_mmlu_full_en_college_mathematics": 0.0,
                "global_mmlu_full_en_college_medicine": 0.0,
                "global_mmlu_full_en_college_physics": 0.0,
                "global_mmlu_full_en_computer_security": 0.0,
                "global_mmlu_full_en_conceptual_physics": 0.0,
                "global_mmlu_full_en_econometrics": 0.0,
                "global_mmlu_full_en_electrical_engineering": 0.0,
                "global_mmlu_full_en_elementary_mathematics": 0.0,
                "global_mmlu_full_en_formal_logic": 0.0,
                "global_mmlu_full_en_global_facts": 0.0,
                "global_mmlu_full_en_high_school_biology": 0.0,
                "global_mmlu_full_en_high_school_chemistry": 0.0,
                "global_mmlu_full_en_high_school_computer_science": 0.0,
                "global_mmlu_full_en_high_school_european_history": 0.0,
                "global_mmlu_full_en_high_school_geography": 0.0,
                "global_mmlu_full_en_high_school_government_and_politics": 0.0,
                "global_mmlu_full_en_high_school_macroeconomics": 0.0,
                "global_mmlu_full_en_high_school_mathematics": 0.0,
                "global_mmlu_full_en_high_school_microeconomics": 0.0,
                "global_mmlu_full_en_high_school_physics": 0.0,
                "global_mmlu_full_en_high_school_psychology": 0.0,
                "global_mmlu_full_en_high_school_statistics": 0.0,
                "global_mmlu_full_en_high_school_us_history": 0.0,
                "global_mmlu_full_en_high_school_world_history": 0.0,
                "global_mmlu_full_en_human_aging": 0.0,
                "global_mmlu_full_en_human_sexuality": 0.0,
                "global_mmlu_full_en_humanities": 0.0,
                "global_mmlu_full_en_international_law": 0.0,
                "global_mmlu_full_en_jurisprudence": 0.0,
                "global_mmlu_full_en_logical_fallacies": 0.0,
                "global_mmlu_full_en_machine_learning": 0.0,
                "global_mmlu_full_en_management": 0.0,
                "global_mmlu_full_en_marketing": 0.0,
                "global_mmlu_full_en_medical_genetics": 0.0,
                "global_mmlu_full_en_miscellaneous": 0.0,
                "global_mmlu_full_en_moral_disputes": 0.0,
                "global_mmlu_full_en_moral_scenarios": 0.0,
                "global_mmlu_full_en_nutrition": 0.0,
                "global_mmlu_full_en_other": 0.0,
                "global_mmlu_full_en_philosophy": 0.0,
                "global_mmlu_full_en_prehistory": 0.0,
                "global_mmlu_full_en_professional_accounting": 0.0,
                "global_mmlu_full_en_professional_law": 0.0,
                "global_mmlu_full_en_professional_medicine": 0.0,
                "global_mmlu_full_en_professional_psychology": 0.0,
                "global_mmlu_full_en_public_relations": 0.0,
                "global_mmlu_full_en_security_studies": 0.0,
                "global_mmlu_full_en_social_sciences": 0.0,
                "global_mmlu_full_en_sociology": 0.0,
                "global_mmlu_full_en_stem": 0.0,
                "global_mmlu_full_en_us_foreign_policy": 0.0,
                "global_mmlu_full_en_virology": 0.0,
                "global_mmlu_full_en_world_religions": 0.0
              },
              "n-shot": {
                "global_mmlu_full_en_abstract_algebra": 0,
                "global_mmlu_full_en_anatomy": 0,
                "global_mmlu_full_en_astronomy": 0,
                "global_mmlu_full_en_business_ethics": 0,
                "global_mmlu_full_en_clinical_knowledge": 0,
                "global_mmlu_full_en_college_biology": 0,
                "global_mmlu_full_en_college_chemistry": 0,
                "global_mmlu_full_en_college_computer_science": 0,
                "global_mmlu_full_en_college_mathematics": 0,
                "global_mmlu_full_en_college_medicine": 0,
                "global_mmlu_full_en_college_physics": 0,
                "global_mmlu_full_en_computer_security": 0,
                "global_mmlu_full_en_conceptual_physics": 0,
                "global_mmlu_full_en_econometrics": 0,
                "global_mmlu_full_en_electrical_engineering": 0,
                "global_mmlu_full_en_elementary_mathematics": 0,
                "global_mmlu_full_en_formal_logic": 0,
                "global_mmlu_full_en_global_facts": 0,
                "global_mmlu_full_en_high_school_biology": 0,
                "global_mmlu_full_en_high_school_chemistry": 0,
                "global_mmlu_full_en_high_school_computer_science": 0,
                "global_mmlu_full_en_high_school_european_history": 0,
                "global_mmlu_full_en_high_school_geography": 0,
                "global_mmlu_full_en_high_school_government_and_politics": 0,
                "global_mmlu_full_en_high_school_macroeconomics": 0,
                "global_mmlu_full_en_high_school_mathematics": 0,
                "global_mmlu_full_en_high_school_microeconomics": 0,
                "global_mmlu_full_en_high_school_physics": 0,
                "global_mmlu_full_en_high_school_psychology": 0,
                "global_mmlu_full_en_high_school_statistics": 0,
                "global_mmlu_full_en_high_school_us_history": 0,
                "global_mmlu_full_en_high_school_world_history": 0,
                "global_mmlu_full_en_human_aging": 0,
                "global_mmlu_full_en_human_sexuality": 0,
                "global_mmlu_full_en_international_law": 0,
                "global_mmlu_full_en_jurisprudence": 0,
                "global_mmlu_full_en_logical_fallacies": 0,
                "global_mmlu_full_en_machine_learning": 0,
                "global_mmlu_full_en_management": 0,
                "global_mmlu_full_en_marketing": 0,
                "global_mmlu_full_en_medical_genetics": 0,
                "global_mmlu_full_en_miscellaneous": 0,
                "global_mmlu_full_en_moral_disputes": 0,
                "global_mmlu_full_en_moral_scenarios": 0,
                "global_mmlu_full_en_nutrition": 0,
                "global_mmlu_full_en_philosophy": 0,
                "global_mmlu_full_en_prehistory": 0,
                "global_mmlu_full_en_professional_accounting": 0,
                "global_mmlu_full_en_professional_law": 0,
                "global_mmlu_full_en_professional_medicine": 0,
                "global_mmlu_full_en_professional_psychology": 0,
                "global_mmlu_full_en_public_relations": 0,
                "global_mmlu_full_en_security_studies": 0,
                "global_mmlu_full_en_sociology": 0,
                "global_mmlu_full_en_us_foreign_policy": 0,
                "global_mmlu_full_en_virology": 0,
                "global_mmlu_full_en_world_religions": 0
              },
              "higher_is_better": {
                "global_mmlu_full_en": {
                  "acc": true
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "acc": true
                },
                "global_mmlu_full_en_anatomy": {
                  "acc": true
                },
                "global_mmlu_full_en_astronomy": {
                  "acc": true
                },
                "global_mmlu_full_en_business_ethics": {
                  "acc": true
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "acc": true
                },
                "global_mmlu_full_en_college_biology": {
                  "acc": true
                },
                "global_mmlu_full_en_college_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_en_college_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_en_college_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_college_medicine": {
                  "acc": true
                },
                "global_mmlu_full_en_college_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_computer_security": {
                  "acc": true
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_econometrics": {
                  "acc": true
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "acc": true
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_formal_logic": {
                  "acc": true
                },
                "global_mmlu_full_en_global_facts": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_biology": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_geography": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "acc": true
                },
                "global_mmlu_full_en_human_aging": {
                  "acc": true
                },
                "global_mmlu_full_en_human_sexuality": {
                  "acc": true
                },
                "global_mmlu_full_en_humanities": {
                  "acc": true
                },
                "global_mmlu_full_en_international_law": {
                  "acc": true
                },
                "global_mmlu_full_en_jurisprudence": {
                  "acc": true
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "acc": true
                },
                "global_mmlu_full_en_machine_learning": {
                  "acc": true
                },
                "global_mmlu_full_en_management": {
                  "acc": true
                },
                "global_mmlu_full_en_marketing": {
                  "acc": true
                },
                "global_mmlu_full_en_medical_genetics": {
                  "acc": true
                },
                "global_mmlu_full_en_miscellaneous": {
                  "acc": true
                },
                "global_mmlu_full_en_moral_disputes": {
                  "acc": true
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "acc": true
                },
                "global_mmlu_full_en_nutrition": {
                  "acc": true
                },
                "global_mmlu_full_en_other": {
                  "acc": true
                },
                "global_mmlu_full_en_philosophy": {
                  "acc": true
                },
                "global_mmlu_full_en_prehistory": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_accounting": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_law": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_medicine": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_psychology": {
                  "acc": true
                },
                "global_mmlu_full_en_public_relations": {
                  "acc": true
                },
                "global_mmlu_full_en_security_studies": {
                  "acc": true
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc": true
                },
                "global_mmlu_full_en_sociology": {
                  "acc": true
                },
                "global_mmlu_full_en_stem": {
                  "acc": true
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "acc": true
                },
                "global_mmlu_full_en_virology": {
                  "acc": true
                },
                "global_mmlu_full_en_world_religions": {
                  "acc": true
                }
              },
              "n-samples": {
                "global_mmlu_full_en_high_school_statistics": {
                  "original": 216,
                  "effective": 216
                },
                "global_mmlu_full_en_computer_security": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_machine_learning": {
                  "original": 112,
                  "effective": 112
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_astronomy": {
                  "original": 152,
                  "effective": 152
                },
                "global_mmlu_full_en_college_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_high_school_biology": {
                  "original": 310,
                  "effective": 310
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "original": 203,
                  "effective": 203
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "original": 378,
                  "effective": 378
                },
                "global_mmlu_full_en_college_physics": {
                  "original": 102,
                  "effective": 102
                },
                "global_mmlu_full_en_high_school_physics": {
                  "original": 151,
                  "effective": 151
                },
                "global_mmlu_full_en_college_chemistry": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_college_biology": {
                  "original": 144,
                  "effective": 144
                },
                "global_mmlu_full_en_college_mathematics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "original": 235,
                  "effective": 235
                },
                "global_mmlu_full_en_anatomy": {
                  "original": 135,
                  "effective": 135
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "original": 270,
                  "effective": 270
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "original": 145,
                  "effective": 145
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "original": 265,
                  "effective": 265
                },
                "global_mmlu_full_en_medical_genetics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_professional_medicine": {
                  "original": 272,
                  "effective": 272
                },
                "global_mmlu_full_en_business_ethics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_marketing": {
                  "original": 234,
                  "effective": 234
                },
                "global_mmlu_full_en_global_facts": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_college_medicine": {
                  "original": 173,
                  "effective": 173
                },
                "global_mmlu_full_en_human_aging": {
                  "original": 223,
                  "effective": 223
                },
                "global_mmlu_full_en_management": {
                  "original": 103,
                  "effective": 103
                },
                "global_mmlu_full_en_nutrition": {
                  "original": 306,
                  "effective": 306
                },
                "global_mmlu_full_en_virology": {
                  "original": 166,
                  "effective": 166
                },
                "global_mmlu_full_en_professional_accounting": {
                  "original": 282,
                  "effective": 282
                },
                "global_mmlu_full_en_miscellaneous": {
                  "original": 783,
                  "effective": 783
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_public_relations": {
                  "original": 110,
                  "effective": 110
                },
                "global_mmlu_full_en_econometrics": {
                  "original": 114,
                  "effective": 114
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "original": 238,
                  "effective": 238
                },
                "global_mmlu_full_en_human_sexuality": {
                  "original": 131,
                  "effective": 131
                },
                "global_mmlu_full_en_security_studies": {
                  "original": 245,
                  "effective": 245
                },
                "global_mmlu_full_en_professional_psychology": {
                  "original": 612,
                  "effective": 612
                },
                "global_mmlu_full_en_high_school_geography": {
                  "original": 198,
                  "effective": 198
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "original": 193,
                  "effective": 193
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "original": 545,
                  "effective": 545
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "original": 390,
                  "effective": 390
                },
                "global_mmlu_full_en_sociology": {
                  "original": 201,
                  "effective": 201
                },
                "global_mmlu_full_en_world_religions": {
                  "original": 171,
                  "effective": 171
                },
                "global_mmlu_full_en_international_law": {
                  "original": 121,
                  "effective": 121
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "original": 895,
                  "effective": 895
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "original": 204,
                  "effective": 204
                },
                "global_mmlu_full_en_jurisprudence": {
                  "original": 108,
                  "effective": 108
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "original": 237,
                  "effective": 237
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "original": 163,
                  "effective": 163
                },
                "global_mmlu_full_en_philosophy": {
                  "original": 311,
                  "effective": 311
                },
                "global_mmlu_full_en_professional_law": {
                  "original": 1534,
                  "effective": 1534
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "original": 165,
                  "effective": 165
                },
                "global_mmlu_full_en_formal_logic": {
                  "original": 126,
                  "effective": 126
                },
                "global_mmlu_full_en_moral_disputes": {
                  "original": 346,
                  "effective": 346
                },
                "global_mmlu_full_en_prehistory": {
                  "original": 324,
                  "effective": 324
                }
              },
              "config": {
                "model": "hf",
                "model_args": "pretrained=/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k,trust_remote_code=True,local_files_only=True,trust_remote_code=True",
                "model_num_parameters": 222882048,
                "model_dtype": "torch.float32",
                "model_revision": "main",
                "model_sha": "",
                "batch_size": "auto",
                "batch_sizes": [
                  64
                ],
                "device": "cuda",
                "use_cache": null,
                "limit": null,
                "bootstrap_iters": 100000,
                "gen_kwargs": null,
                "random_seed": 42,
                "numpy_seed": 42,
                "torch_seed": 42,
                "fewshot_seed": 42
              },
              "git_hash": "a4103c7",
              "date": 1758646961.773979,
              "pretty_env_info": "PyTorch version: 2.7.1+cu126\nIs debug build: False\nCUDA used to build PyTorch: 12.6\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.2 LTS (x86_64)\nGCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\nClang version: Could not collect\nCMake version: version 3.24.1\nLibc version: glibc-2.35\n\nPython version: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] (64-bit runtime)\nPython platform: Linux-6.8.0-1036-nvidia-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H200\nNvidia driver version: 580.82.07\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               224\nOn-line CPU(s) list:                  0-223\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480C\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   56\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          3800.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            5.3 MiB (112 instances)\nL1i cache:                            3.5 MiB (112 instances)\nL2 cache:                             224 MiB (112 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-55,112-167\nNUMA node1 CPU(s):                    56-111,168-223\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pytorch-lightning==2.5.2\n[pip3] torch==2.7.1\n[pip3] torchmetrics==1.8.0\n[pip3] triton==3.3.1\n[conda] Could not collect",
              "transformers_version": "4.54.1",
              "lm_eval_version": "0.4.9",
              "upper_git_hash": null,
              "tokenizer_pad_token": [
                "<pad>",
                "0"
              ],
              "tokenizer_eos_token": [
                "</s>",
                "1"
              ],
              "tokenizer_bos_token": [
                null,
                "None"
              ],
              "eot_token_id": 1,
              "max_length": 512,
              "task_hashes": {},
              "model_source": "hf",
              "model_name": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
              "model_name_sanitized": "__netscratch__nrauscher__projects__BA-hydra__evaluation__models__temp__wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
              "system_instruction": null,
              "system_instruction_sha": null,
              "fewshot_as_multiturn": false,
              "chat_template": null,
              "chat_template_sha": null,
              "start_time": 633106.266064426,
              "end_time": 633776.993102481,
              "total_evaluation_time_seconds": "670.7270380549598"
            },
            "duration_seconds": 718.4150624275208,
            "duration_minutes": 11.973584373792013,
            "command": "python -m lm_eval --model hf --model_args pretrained=/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k,trust_remote_code=True,local_files_only=True --tasks global_mmlu_full_en --device cuda --batch_size auto --output_path /netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/wechsel-transfer-en-487k-continued-pretraind-on-german-15k_global_mmlu_en_0shot_20250923_190110 --num_fewshot 0 --trust_remote_code --seed 42 --wandb_args project=BA-T5-CrossLingual,entity=nikolas-rauscher-dfki,group=crosslingual_transfer_eval_full_15k-final,name=wechsel-transfer-en-487k-continued-pretraind-on-german-15k_global_mmlu_en_0shot",
            "output_dir": "/netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/wechsel-transfer-en-487k-continued-pretraind-on-german-15k_global_mmlu_en_0shot_20250923_190110",
            "status": "success"
          }
        },
        "global_mmlu_de": {
          "0_shot": {
            "parse_status": "partial",
            "error": "Could not find primary metric",
            "raw_results": {
              "results": {
                "global_mmlu_full_de": {
                  "acc,none": 0.26997578692493945,
                  "acc_stderr,none": 0.0037145750696365785,
                  "alias": "global_mmlu_full_de"
                },
                "global_mmlu_full_de_humanities": {
                  "acc,none": 0.25356004250797026,
                  "acc_stderr,none": 0.006329457761941455,
                  "alias": " - global_mmlu_full_de_humanities"
                },
                "global_mmlu_full_de_formal_logic": {
                  "alias": "  - global_mmlu_full_de_formal_logic",
                  "acc,none": 0.36507936507936506,
                  "acc_stderr,none": 0.04306241259127153
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "alias": "  - global_mmlu_full_de_high_school_european_history",
                  "acc,none": 0.2727272727272727,
                  "acc_stderr,none": 0.0347769116216366
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "alias": "  - global_mmlu_full_de_high_school_us_history",
                  "acc,none": 0.22549019607843138,
                  "acc_stderr,none": 0.02933116229425172
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "alias": "  - global_mmlu_full_de_high_school_world_history",
                  "acc,none": 0.25738396624472576,
                  "acc_stderr,none": 0.0284588209914603
                },
                "global_mmlu_full_de_international_law": {
                  "alias": "  - global_mmlu_full_de_international_law",
                  "acc,none": 0.12396694214876033,
                  "acc_stderr,none": 0.030083098716035227
                },
                "global_mmlu_full_de_jurisprudence": {
                  "alias": "  - global_mmlu_full_de_jurisprudence",
                  "acc,none": 0.21296296296296297,
                  "acc_stderr,none": 0.039578354719809826
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "alias": "  - global_mmlu_full_de_logical_fallacies",
                  "acc,none": 0.25153374233128833,
                  "acc_stderr,none": 0.03408997886857529
                },
                "global_mmlu_full_de_moral_disputes": {
                  "alias": "  - global_mmlu_full_de_moral_disputes",
                  "acc,none": 0.23410404624277456,
                  "acc_stderr,none": 0.02279711027807113
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "alias": "  - global_mmlu_full_de_moral_scenarios",
                  "acc,none": 0.2748603351955307,
                  "acc_stderr,none": 0.01493131670322051
                },
                "global_mmlu_full_de_philosophy": {
                  "alias": "  - global_mmlu_full_de_philosophy",
                  "acc,none": 0.2347266881028939,
                  "acc_stderr,none": 0.024071805887677048
                },
                "global_mmlu_full_de_prehistory": {
                  "alias": "  - global_mmlu_full_de_prehistory",
                  "acc,none": 0.2345679012345679,
                  "acc_stderr,none": 0.02357688174400571
                },
                "global_mmlu_full_de_professional_law": {
                  "alias": "  - global_mmlu_full_de_professional_law",
                  "acc,none": 0.26727509778357234,
                  "acc_stderr,none": 0.011302607515637516
                },
                "global_mmlu_full_de_world_religions": {
                  "alias": "  - global_mmlu_full_de_world_religions",
                  "acc,none": 0.17543859649122806,
                  "acc_stderr,none": 0.029170885500727686
                },
                "global_mmlu_full_de_other": {
                  "acc,none": 0.2442870936594786,
                  "acc_stderr,none": 0.007614623782438532,
                  "alias": " - global_mmlu_full_de_other"
                },
                "global_mmlu_full_de_business_ethics": {
                  "alias": "  - global_mmlu_full_de_business_ethics",
                  "acc,none": 0.22,
                  "acc_stderr,none": 0.041633319989322695
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "alias": "  - global_mmlu_full_de_clinical_knowledge",
                  "acc,none": 0.2830188679245283,
                  "acc_stderr,none": 0.027724236492700907
                },
                "global_mmlu_full_de_college_medicine": {
                  "alias": "  - global_mmlu_full_de_college_medicine",
                  "acc,none": 0.3352601156069364,
                  "acc_stderr,none": 0.03599586301247078
                },
                "global_mmlu_full_de_global_facts": {
                  "alias": "  - global_mmlu_full_de_global_facts",
                  "acc,none": 0.18,
                  "acc_stderr,none": 0.038612291966536955
                },
                "global_mmlu_full_de_human_aging": {
                  "alias": "  - global_mmlu_full_de_human_aging",
                  "acc,none": 0.1210762331838565,
                  "acc_stderr,none": 0.02189417411318577
                },
                "global_mmlu_full_de_management": {
                  "alias": "  - global_mmlu_full_de_management",
                  "acc,none": 0.3786407766990291,
                  "acc_stderr,none": 0.048026946982589726
                },
                "global_mmlu_full_de_marketing": {
                  "alias": "  - global_mmlu_full_de_marketing",
                  "acc,none": 0.2222222222222222,
                  "acc_stderr,none": 0.0272360139461967
                },
                "global_mmlu_full_de_medical_genetics": {
                  "alias": "  - global_mmlu_full_de_medical_genetics",
                  "acc,none": 0.22,
                  "acc_stderr,none": 0.041633319989322695
                },
                "global_mmlu_full_de_miscellaneous": {
                  "alias": "  - global_mmlu_full_de_miscellaneous",
                  "acc,none": 0.20178799489144317,
                  "acc_stderr,none": 0.014351702181636863
                },
                "global_mmlu_full_de_nutrition": {
                  "alias": "  - global_mmlu_full_de_nutrition",
                  "acc,none": 0.29411764705882354,
                  "acc_stderr,none": 0.026090162504279032
                },
                "global_mmlu_full_de_professional_accounting": {
                  "alias": "  - global_mmlu_full_de_professional_accounting",
                  "acc,none": 0.2375886524822695,
                  "acc_stderr,none": 0.025389512552729903
                },
                "global_mmlu_full_de_professional_medicine": {
                  "alias": "  - global_mmlu_full_de_professional_medicine",
                  "acc,none": 0.38235294117647056,
                  "acc_stderr,none": 0.029520095697687754
                },
                "global_mmlu_full_de_virology": {
                  "alias": "  - global_mmlu_full_de_virology",
                  "acc,none": 0.16265060240963855,
                  "acc_stderr,none": 0.028730237892613808
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc,none": 0.3054923626909327,
                  "acc_stderr,none": 0.008231966534078917,
                  "alias": " - global_mmlu_full_de_social_sciences"
                },
                "global_mmlu_full_de_econometrics": {
                  "alias": "  - global_mmlu_full_de_econometrics",
                  "acc,none": 0.23684210526315788,
                  "acc_stderr,none": 0.03999423879281336
                },
                "global_mmlu_full_de_high_school_geography": {
                  "alias": "  - global_mmlu_full_de_high_school_geography",
                  "acc,none": 0.3484848484848485,
                  "acc_stderr,none": 0.033948539651564025
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "alias": "  - global_mmlu_full_de_high_school_government_and_politics",
                  "acc,none": 0.3471502590673575,
                  "acc_stderr,none": 0.03435696168361355
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "alias": "  - global_mmlu_full_de_high_school_macroeconomics",
                  "acc,none": 0.35384615384615387,
                  "acc_stderr,none": 0.02424378399406217
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "alias": "  - global_mmlu_full_de_high_school_microeconomics",
                  "acc,none": 0.35294117647058826,
                  "acc_stderr,none": 0.031041941304059278
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "alias": "  - global_mmlu_full_de_high_school_psychology",
                  "acc,none": 0.3541284403669725,
                  "acc_stderr,none": 0.020504729013829104
                },
                "global_mmlu_full_de_human_sexuality": {
                  "alias": "  - global_mmlu_full_de_human_sexuality",
                  "acc,none": 0.2366412213740458,
                  "acc_stderr,none": 0.03727673575596917
                },
                "global_mmlu_full_de_professional_psychology": {
                  "alias": "  - global_mmlu_full_de_professional_psychology",
                  "acc,none": 0.2238562091503268,
                  "acc_stderr,none": 0.016863008585416613
                },
                "global_mmlu_full_de_public_relations": {
                  "alias": "  - global_mmlu_full_de_public_relations",
                  "acc,none": 0.21818181818181817,
                  "acc_stderr,none": 0.03955932861795833
                },
                "global_mmlu_full_de_security_studies": {
                  "alias": "  - global_mmlu_full_de_security_studies",
                  "acc,none": 0.40816326530612246,
                  "acc_stderr,none": 0.03146465712827424
                },
                "global_mmlu_full_de_sociology": {
                  "alias": "  - global_mmlu_full_de_sociology",
                  "acc,none": 0.22388059701492538,
                  "acc_stderr,none": 0.02947525023601718
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "alias": "  - global_mmlu_full_de_us_foreign_policy",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.04351941398892446
                },
                "global_mmlu_full_de_stem": {
                  "acc,none": 0.2851252775134792,
                  "acc_stderr,none": 0.007976719146405102,
                  "alias": " - global_mmlu_full_de_stem"
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "alias": "  - global_mmlu_full_de_abstract_algebra",
                  "acc,none": 0.24,
                  "acc_stderr,none": 0.042923469599092816
                },
                "global_mmlu_full_de_anatomy": {
                  "alias": "  - global_mmlu_full_de_anatomy",
                  "acc,none": 0.22962962962962963,
                  "acc_stderr,none": 0.03633384414073465
                },
                "global_mmlu_full_de_astronomy": {
                  "alias": "  - global_mmlu_full_de_astronomy",
                  "acc,none": 0.32894736842105265,
                  "acc_stderr,none": 0.03823428969926605
                },
                "global_mmlu_full_de_college_biology": {
                  "alias": "  - global_mmlu_full_de_college_biology",
                  "acc,none": 0.2777777777777778,
                  "acc_stderr,none": 0.03745554791462457
                },
                "global_mmlu_full_de_college_chemistry": {
                  "alias": "  - global_mmlu_full_de_college_chemistry",
                  "acc,none": 0.41,
                  "acc_stderr,none": 0.04943110704237103
                },
                "global_mmlu_full_de_college_computer_science": {
                  "alias": "  - global_mmlu_full_de_college_computer_science",
                  "acc,none": 0.33,
                  "acc_stderr,none": 0.047258156262526045
                },
                "global_mmlu_full_de_college_mathematics": {
                  "alias": "  - global_mmlu_full_de_college_mathematics",
                  "acc,none": 0.3,
                  "acc_stderr,none": 0.046056618647183814
                },
                "global_mmlu_full_de_college_physics": {
                  "alias": "  - global_mmlu_full_de_college_physics",
                  "acc,none": 0.38235294117647056,
                  "acc_stderr,none": 0.04835503696107224
                },
                "global_mmlu_full_de_computer_security": {
                  "alias": "  - global_mmlu_full_de_computer_security",
                  "acc,none": 0.19,
                  "acc_stderr,none": 0.039427724440366234
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "alias": "  - global_mmlu_full_de_conceptual_physics",
                  "acc,none": 0.2127659574468085,
                  "acc_stderr,none": 0.026754391348039776
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "alias": "  - global_mmlu_full_de_electrical_engineering",
                  "acc,none": 0.2620689655172414,
                  "acc_stderr,none": 0.036646663372252565
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "alias": "  - global_mmlu_full_de_elementary_mathematics",
                  "acc,none": 0.26455026455026454,
                  "acc_stderr,none": 0.022717467897708617
                },
                "global_mmlu_full_de_high_school_biology": {
                  "alias": "  - global_mmlu_full_de_high_school_biology",
                  "acc,none": 0.3096774193548387,
                  "acc_stderr,none": 0.026302774983517418
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "alias": "  - global_mmlu_full_de_high_school_chemistry",
                  "acc,none": 0.27586206896551724,
                  "acc_stderr,none": 0.03144712581678241
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "alias": "  - global_mmlu_full_de_high_school_computer_science",
                  "acc,none": 0.2,
                  "acc_stderr,none": 0.04020151261036846
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "alias": "  - global_mmlu_full_de_high_school_mathematics",
                  "acc,none": 0.26296296296296295,
                  "acc_stderr,none": 0.026842057873833706
                },
                "global_mmlu_full_de_high_school_physics": {
                  "alias": "  - global_mmlu_full_de_high_school_physics",
                  "acc,none": 0.32450331125827814,
                  "acc_stderr,none": 0.038227469376587525
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "alias": "  - global_mmlu_full_de_high_school_statistics",
                  "acc,none": 0.4351851851851852,
                  "acc_stderr,none": 0.03381200005643525
                },
                "global_mmlu_full_de_machine_learning": {
                  "alias": "  - global_mmlu_full_de_machine_learning",
                  "acc,none": 0.16071428571428573,
                  "acc_stderr,none": 0.0348594609647574
                }
              },
              "groups": {
                "global_mmlu_full_de": {
                  "acc,none": 0.26997578692493945,
                  "acc_stderr,none": 0.0037145750696365785,
                  "alias": "global_mmlu_full_de"
                },
                "global_mmlu_full_de_humanities": {
                  "acc,none": 0.25356004250797026,
                  "acc_stderr,none": 0.006329457761941455,
                  "alias": " - global_mmlu_full_de_humanities"
                },
                "global_mmlu_full_de_other": {
                  "acc,none": 0.2442870936594786,
                  "acc_stderr,none": 0.007614623782438532,
                  "alias": " - global_mmlu_full_de_other"
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc,none": 0.3054923626909327,
                  "acc_stderr,none": 0.008231966534078917,
                  "alias": " - global_mmlu_full_de_social_sciences"
                },
                "global_mmlu_full_de_stem": {
                  "acc,none": 0.2851252775134792,
                  "acc_stderr,none": 0.007976719146405102,
                  "alias": " - global_mmlu_full_de_stem"
                }
              },
              "group_subtasks": {
                "global_mmlu_full_de_humanities": [
                  "global_mmlu_full_de_professional_law",
                  "global_mmlu_full_de_international_law",
                  "global_mmlu_full_de_prehistory",
                  "global_mmlu_full_de_world_religions",
                  "global_mmlu_full_de_high_school_world_history",
                  "global_mmlu_full_de_high_school_us_history",
                  "global_mmlu_full_de_jurisprudence",
                  "global_mmlu_full_de_formal_logic",
                  "global_mmlu_full_de_logical_fallacies",
                  "global_mmlu_full_de_high_school_european_history",
                  "global_mmlu_full_de_philosophy",
                  "global_mmlu_full_de_moral_scenarios",
                  "global_mmlu_full_de_moral_disputes"
                ],
                "global_mmlu_full_de_social_sciences": [
                  "global_mmlu_full_de_high_school_psychology",
                  "global_mmlu_full_de_professional_psychology",
                  "global_mmlu_full_de_high_school_government_and_politics",
                  "global_mmlu_full_de_high_school_macroeconomics",
                  "global_mmlu_full_de_high_school_microeconomics",
                  "global_mmlu_full_de_econometrics",
                  "global_mmlu_full_de_high_school_geography",
                  "global_mmlu_full_de_security_studies",
                  "global_mmlu_full_de_us_foreign_policy",
                  "global_mmlu_full_de_public_relations",
                  "global_mmlu_full_de_human_sexuality",
                  "global_mmlu_full_de_sociology"
                ],
                "global_mmlu_full_de_other": [
                  "global_mmlu_full_de_nutrition",
                  "global_mmlu_full_de_professional_medicine",
                  "global_mmlu_full_de_college_medicine",
                  "global_mmlu_full_de_professional_accounting",
                  "global_mmlu_full_de_marketing",
                  "global_mmlu_full_de_miscellaneous",
                  "global_mmlu_full_de_business_ethics",
                  "global_mmlu_full_de_virology",
                  "global_mmlu_full_de_medical_genetics",
                  "global_mmlu_full_de_human_aging",
                  "global_mmlu_full_de_global_facts",
                  "global_mmlu_full_de_clinical_knowledge",
                  "global_mmlu_full_de_management"
                ],
                "global_mmlu_full_de_stem": [
                  "global_mmlu_full_de_computer_security",
                  "global_mmlu_full_de_college_biology",
                  "global_mmlu_full_de_college_mathematics",
                  "global_mmlu_full_de_conceptual_physics",
                  "global_mmlu_full_de_elementary_mathematics",
                  "global_mmlu_full_de_astronomy",
                  "global_mmlu_full_de_high_school_physics",
                  "global_mmlu_full_de_machine_learning",
                  "global_mmlu_full_de_college_chemistry",
                  "global_mmlu_full_de_high_school_chemistry",
                  "global_mmlu_full_de_college_computer_science",
                  "global_mmlu_full_de_high_school_computer_science",
                  "global_mmlu_full_de_high_school_mathematics",
                  "global_mmlu_full_de_abstract_algebra",
                  "global_mmlu_full_de_high_school_biology",
                  "global_mmlu_full_de_electrical_engineering",
                  "global_mmlu_full_de_college_physics",
                  "global_mmlu_full_de_anatomy",
                  "global_mmlu_full_de_high_school_statistics"
                ],
                "global_mmlu_full_de": [
                  "global_mmlu_full_de_stem",
                  "global_mmlu_full_de_other",
                  "global_mmlu_full_de_social_sciences",
                  "global_mmlu_full_de_humanities"
                ]
              },
              "configs": {
                "global_mmlu_full_de_abstract_algebra": {
                  "task": "global_mmlu_full_de_abstract_algebra",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c1835b0>, subject='abstract_algebra')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_anatomy": {
                  "task": "global_mmlu_full_de_anatomy",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c182290>, subject='anatomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_astronomy": {
                  "task": "global_mmlu_full_de_astronomy",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c181120>, subject='astronomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_business_ethics": {
                  "task": "global_mmlu_full_de_business_ethics",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f011ed40>, subject='business_ethics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "task": "global_mmlu_full_de_clinical_knowledge",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f011dfc0>, subject='clinical_knowledge')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_biology": {
                  "task": "global_mmlu_full_de_college_biology",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c182560>, subject='college_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_chemistry": {
                  "task": "global_mmlu_full_de_college_chemistry",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c183910>, subject='college_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_computer_science": {
                  "task": "global_mmlu_full_de_college_computer_science",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c1836d0>, subject='college_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_mathematics": {
                  "task": "global_mmlu_full_de_college_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f13b81f0>, subject='college_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_medicine": {
                  "task": "global_mmlu_full_de_college_medicine",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c180dc0>, subject='college_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_physics": {
                  "task": "global_mmlu_full_de_college_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c182e60>, subject='college_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_computer_security": {
                  "task": "global_mmlu_full_de_computer_security",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c180c10>, subject='computer_security')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "task": "global_mmlu_full_de_conceptual_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f13b9480>, subject='conceptual_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_econometrics": {
                  "task": "global_mmlu_full_de_econometrics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f011d480>, subject='econometrics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "task": "global_mmlu_full_de_electrical_engineering",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c182170>, subject='electrical_engineering')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "task": "global_mmlu_full_de_elementary_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c1828c0>, subject='elementary_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_formal_logic": {
                  "task": "global_mmlu_full_de_formal_logic",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f155c790>, subject='formal_logic')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_global_facts": {
                  "task": "global_mmlu_full_de_global_facts",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f011d900>, subject='global_facts')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_biology": {
                  "task": "global_mmlu_full_de_high_school_biology",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c181990>, subject='high_school_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "task": "global_mmlu_full_de_high_school_chemistry",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c1832e0>, subject='high_school_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "task": "global_mmlu_full_de_high_school_computer_science",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c180700>, subject='high_school_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "task": "global_mmlu_full_de_high_school_european_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f155d750>, subject='high_school_european_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_geography": {
                  "task": "global_mmlu_full_de_high_school_geography",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f155e950>, subject='high_school_geography')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "task": "global_mmlu_full_de_high_school_government_and_politics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f011caf0>, subject='high_school_government_and_politics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "task": "global_mmlu_full_de_high_school_macroeconomics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f011e680>, subject='high_school_macroeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "task": "global_mmlu_full_de_high_school_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c1829e0>, subject='high_school_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "task": "global_mmlu_full_de_high_school_microeconomics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f011cf70>, subject='high_school_microeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_physics": {
                  "task": "global_mmlu_full_de_high_school_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c183f40>, subject='high_school_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "task": "global_mmlu_full_de_high_school_psychology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f011f0a0>, subject='high_school_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "task": "global_mmlu_full_de_high_school_statistics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c1815a0>, subject='high_school_statistics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "task": "global_mmlu_full_de_high_school_us_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f155eb00>, subject='high_school_us_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "task": "global_mmlu_full_de_high_school_world_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f155caf0>, subject='high_school_world_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_human_aging": {
                  "task": "global_mmlu_full_de_human_aging",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f011c040>, subject='human_aging')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_human_sexuality": {
                  "task": "global_mmlu_full_de_human_sexuality",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f155e290>, subject='human_sexuality')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_international_law": {
                  "task": "global_mmlu_full_de_international_law",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f155ff40>, subject='international_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_jurisprudence": {
                  "task": "global_mmlu_full_de_jurisprudence",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f155f1c0>, subject='jurisprudence')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "task": "global_mmlu_full_de_logical_fallacies",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f155cdc0>, subject='logical_fallacies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_machine_learning": {
                  "task": "global_mmlu_full_de_machine_learning",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c183250>, subject='machine_learning')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_management": {
                  "task": "global_mmlu_full_de_management",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f011c940>, subject='management')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_marketing": {
                  "task": "global_mmlu_full_de_marketing",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f011e8c0>, subject='marketing')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_medical_genetics": {
                  "task": "global_mmlu_full_de_medical_genetics",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f011da20>, subject='medical_genetics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_miscellaneous": {
                  "task": "global_mmlu_full_de_miscellaneous",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f011f520>, subject='miscellaneous')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_moral_disputes": {
                  "task": "global_mmlu_full_de_moral_disputes",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f3304280>, subject='moral_disputes')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "task": "global_mmlu_full_de_moral_scenarios",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f155d090>, subject='moral_scenarios')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_nutrition": {
                  "task": "global_mmlu_full_de_nutrition",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c181360>, subject='nutrition')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_philosophy": {
                  "task": "global_mmlu_full_de_philosophy",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d71c26d990>, subject='philosophy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_prehistory": {
                  "task": "global_mmlu_full_de_prehistory",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f155d1b0>, subject='prehistory')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_professional_accounting": {
                  "task": "global_mmlu_full_de_professional_accounting",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f011f1c0>, subject='professional_accounting')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_professional_law": {
                  "task": "global_mmlu_full_de_professional_law",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f155dfc0>, subject='professional_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_professional_medicine": {
                  "task": "global_mmlu_full_de_professional_medicine",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d48c180430>, subject='professional_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_professional_psychology": {
                  "task": "global_mmlu_full_de_professional_psychology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f011d090>, subject='professional_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_public_relations": {
                  "task": "global_mmlu_full_de_public_relations",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f155f0a0>, subject='public_relations')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_security_studies": {
                  "task": "global_mmlu_full_de_security_studies",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f155dcf0>, subject='security_studies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_sociology": {
                  "task": "global_mmlu_full_de_sociology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f155ee60>, subject='sociology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "task": "global_mmlu_full_de_us_foreign_policy",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f011c4c0>, subject='us_foreign_policy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_virology": {
                  "task": "global_mmlu_full_de_virology",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f011d870>, subject='virology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_world_religions": {
                  "task": "global_mmlu_full_de_world_religions",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x76d4f155e710>, subject='world_religions')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                }
              },
              "versions": {
                "global_mmlu_full_de": 0.0,
                "global_mmlu_full_de_abstract_algebra": 0.0,
                "global_mmlu_full_de_anatomy": 0.0,
                "global_mmlu_full_de_astronomy": 0.0,
                "global_mmlu_full_de_business_ethics": 0.0,
                "global_mmlu_full_de_clinical_knowledge": 0.0,
                "global_mmlu_full_de_college_biology": 0.0,
                "global_mmlu_full_de_college_chemistry": 0.0,
                "global_mmlu_full_de_college_computer_science": 0.0,
                "global_mmlu_full_de_college_mathematics": 0.0,
                "global_mmlu_full_de_college_medicine": 0.0,
                "global_mmlu_full_de_college_physics": 0.0,
                "global_mmlu_full_de_computer_security": 0.0,
                "global_mmlu_full_de_conceptual_physics": 0.0,
                "global_mmlu_full_de_econometrics": 0.0,
                "global_mmlu_full_de_electrical_engineering": 0.0,
                "global_mmlu_full_de_elementary_mathematics": 0.0,
                "global_mmlu_full_de_formal_logic": 0.0,
                "global_mmlu_full_de_global_facts": 0.0,
                "global_mmlu_full_de_high_school_biology": 0.0,
                "global_mmlu_full_de_high_school_chemistry": 0.0,
                "global_mmlu_full_de_high_school_computer_science": 0.0,
                "global_mmlu_full_de_high_school_european_history": 0.0,
                "global_mmlu_full_de_high_school_geography": 0.0,
                "global_mmlu_full_de_high_school_government_and_politics": 0.0,
                "global_mmlu_full_de_high_school_macroeconomics": 0.0,
                "global_mmlu_full_de_high_school_mathematics": 0.0,
                "global_mmlu_full_de_high_school_microeconomics": 0.0,
                "global_mmlu_full_de_high_school_physics": 0.0,
                "global_mmlu_full_de_high_school_psychology": 0.0,
                "global_mmlu_full_de_high_school_statistics": 0.0,
                "global_mmlu_full_de_high_school_us_history": 0.0,
                "global_mmlu_full_de_high_school_world_history": 0.0,
                "global_mmlu_full_de_human_aging": 0.0,
                "global_mmlu_full_de_human_sexuality": 0.0,
                "global_mmlu_full_de_humanities": 0.0,
                "global_mmlu_full_de_international_law": 0.0,
                "global_mmlu_full_de_jurisprudence": 0.0,
                "global_mmlu_full_de_logical_fallacies": 0.0,
                "global_mmlu_full_de_machine_learning": 0.0,
                "global_mmlu_full_de_management": 0.0,
                "global_mmlu_full_de_marketing": 0.0,
                "global_mmlu_full_de_medical_genetics": 0.0,
                "global_mmlu_full_de_miscellaneous": 0.0,
                "global_mmlu_full_de_moral_disputes": 0.0,
                "global_mmlu_full_de_moral_scenarios": 0.0,
                "global_mmlu_full_de_nutrition": 0.0,
                "global_mmlu_full_de_other": 0.0,
                "global_mmlu_full_de_philosophy": 0.0,
                "global_mmlu_full_de_prehistory": 0.0,
                "global_mmlu_full_de_professional_accounting": 0.0,
                "global_mmlu_full_de_professional_law": 0.0,
                "global_mmlu_full_de_professional_medicine": 0.0,
                "global_mmlu_full_de_professional_psychology": 0.0,
                "global_mmlu_full_de_public_relations": 0.0,
                "global_mmlu_full_de_security_studies": 0.0,
                "global_mmlu_full_de_social_sciences": 0.0,
                "global_mmlu_full_de_sociology": 0.0,
                "global_mmlu_full_de_stem": 0.0,
                "global_mmlu_full_de_us_foreign_policy": 0.0,
                "global_mmlu_full_de_virology": 0.0,
                "global_mmlu_full_de_world_religions": 0.0
              },
              "n-shot": {
                "global_mmlu_full_de_abstract_algebra": 0,
                "global_mmlu_full_de_anatomy": 0,
                "global_mmlu_full_de_astronomy": 0,
                "global_mmlu_full_de_business_ethics": 0,
                "global_mmlu_full_de_clinical_knowledge": 0,
                "global_mmlu_full_de_college_biology": 0,
                "global_mmlu_full_de_college_chemistry": 0,
                "global_mmlu_full_de_college_computer_science": 0,
                "global_mmlu_full_de_college_mathematics": 0,
                "global_mmlu_full_de_college_medicine": 0,
                "global_mmlu_full_de_college_physics": 0,
                "global_mmlu_full_de_computer_security": 0,
                "global_mmlu_full_de_conceptual_physics": 0,
                "global_mmlu_full_de_econometrics": 0,
                "global_mmlu_full_de_electrical_engineering": 0,
                "global_mmlu_full_de_elementary_mathematics": 0,
                "global_mmlu_full_de_formal_logic": 0,
                "global_mmlu_full_de_global_facts": 0,
                "global_mmlu_full_de_high_school_biology": 0,
                "global_mmlu_full_de_high_school_chemistry": 0,
                "global_mmlu_full_de_high_school_computer_science": 0,
                "global_mmlu_full_de_high_school_european_history": 0,
                "global_mmlu_full_de_high_school_geography": 0,
                "global_mmlu_full_de_high_school_government_and_politics": 0,
                "global_mmlu_full_de_high_school_macroeconomics": 0,
                "global_mmlu_full_de_high_school_mathematics": 0,
                "global_mmlu_full_de_high_school_microeconomics": 0,
                "global_mmlu_full_de_high_school_physics": 0,
                "global_mmlu_full_de_high_school_psychology": 0,
                "global_mmlu_full_de_high_school_statistics": 0,
                "global_mmlu_full_de_high_school_us_history": 0,
                "global_mmlu_full_de_high_school_world_history": 0,
                "global_mmlu_full_de_human_aging": 0,
                "global_mmlu_full_de_human_sexuality": 0,
                "global_mmlu_full_de_international_law": 0,
                "global_mmlu_full_de_jurisprudence": 0,
                "global_mmlu_full_de_logical_fallacies": 0,
                "global_mmlu_full_de_machine_learning": 0,
                "global_mmlu_full_de_management": 0,
                "global_mmlu_full_de_marketing": 0,
                "global_mmlu_full_de_medical_genetics": 0,
                "global_mmlu_full_de_miscellaneous": 0,
                "global_mmlu_full_de_moral_disputes": 0,
                "global_mmlu_full_de_moral_scenarios": 0,
                "global_mmlu_full_de_nutrition": 0,
                "global_mmlu_full_de_philosophy": 0,
                "global_mmlu_full_de_prehistory": 0,
                "global_mmlu_full_de_professional_accounting": 0,
                "global_mmlu_full_de_professional_law": 0,
                "global_mmlu_full_de_professional_medicine": 0,
                "global_mmlu_full_de_professional_psychology": 0,
                "global_mmlu_full_de_public_relations": 0,
                "global_mmlu_full_de_security_studies": 0,
                "global_mmlu_full_de_sociology": 0,
                "global_mmlu_full_de_us_foreign_policy": 0,
                "global_mmlu_full_de_virology": 0,
                "global_mmlu_full_de_world_religions": 0
              },
              "higher_is_better": {
                "global_mmlu_full_de": {
                  "acc": true
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "acc": true
                },
                "global_mmlu_full_de_anatomy": {
                  "acc": true
                },
                "global_mmlu_full_de_astronomy": {
                  "acc": true
                },
                "global_mmlu_full_de_business_ethics": {
                  "acc": true
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "acc": true
                },
                "global_mmlu_full_de_college_biology": {
                  "acc": true
                },
                "global_mmlu_full_de_college_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_de_college_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_de_college_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_college_medicine": {
                  "acc": true
                },
                "global_mmlu_full_de_college_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_computer_security": {
                  "acc": true
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_econometrics": {
                  "acc": true
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "acc": true
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_formal_logic": {
                  "acc": true
                },
                "global_mmlu_full_de_global_facts": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_biology": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_geography": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "acc": true
                },
                "global_mmlu_full_de_human_aging": {
                  "acc": true
                },
                "global_mmlu_full_de_human_sexuality": {
                  "acc": true
                },
                "global_mmlu_full_de_humanities": {
                  "acc": true
                },
                "global_mmlu_full_de_international_law": {
                  "acc": true
                },
                "global_mmlu_full_de_jurisprudence": {
                  "acc": true
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "acc": true
                },
                "global_mmlu_full_de_machine_learning": {
                  "acc": true
                },
                "global_mmlu_full_de_management": {
                  "acc": true
                },
                "global_mmlu_full_de_marketing": {
                  "acc": true
                },
                "global_mmlu_full_de_medical_genetics": {
                  "acc": true
                },
                "global_mmlu_full_de_miscellaneous": {
                  "acc": true
                },
                "global_mmlu_full_de_moral_disputes": {
                  "acc": true
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "acc": true
                },
                "global_mmlu_full_de_nutrition": {
                  "acc": true
                },
                "global_mmlu_full_de_other": {
                  "acc": true
                },
                "global_mmlu_full_de_philosophy": {
                  "acc": true
                },
                "global_mmlu_full_de_prehistory": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_accounting": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_law": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_medicine": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_psychology": {
                  "acc": true
                },
                "global_mmlu_full_de_public_relations": {
                  "acc": true
                },
                "global_mmlu_full_de_security_studies": {
                  "acc": true
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc": true
                },
                "global_mmlu_full_de_sociology": {
                  "acc": true
                },
                "global_mmlu_full_de_stem": {
                  "acc": true
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "acc": true
                },
                "global_mmlu_full_de_virology": {
                  "acc": true
                },
                "global_mmlu_full_de_world_religions": {
                  "acc": true
                }
              },
              "n-samples": {
                "global_mmlu_full_de_computer_security": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_college_biology": {
                  "original": 144,
                  "effective": 144
                },
                "global_mmlu_full_de_college_mathematics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "original": 235,
                  "effective": 235
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "original": 378,
                  "effective": 378
                },
                "global_mmlu_full_de_astronomy": {
                  "original": 152,
                  "effective": 152
                },
                "global_mmlu_full_de_high_school_physics": {
                  "original": 151,
                  "effective": 151
                },
                "global_mmlu_full_de_machine_learning": {
                  "original": 112,
                  "effective": 112
                },
                "global_mmlu_full_de_college_chemistry": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "original": 203,
                  "effective": 203
                },
                "global_mmlu_full_de_college_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "original": 270,
                  "effective": 270
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_biology": {
                  "original": 310,
                  "effective": 310
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "original": 145,
                  "effective": 145
                },
                "global_mmlu_full_de_college_physics": {
                  "original": 102,
                  "effective": 102
                },
                "global_mmlu_full_de_anatomy": {
                  "original": 135,
                  "effective": 135
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "original": 216,
                  "effective": 216
                },
                "global_mmlu_full_de_nutrition": {
                  "original": 306,
                  "effective": 306
                },
                "global_mmlu_full_de_professional_medicine": {
                  "original": 272,
                  "effective": 272
                },
                "global_mmlu_full_de_college_medicine": {
                  "original": 173,
                  "effective": 173
                },
                "global_mmlu_full_de_professional_accounting": {
                  "original": 282,
                  "effective": 282
                },
                "global_mmlu_full_de_marketing": {
                  "original": 234,
                  "effective": 234
                },
                "global_mmlu_full_de_miscellaneous": {
                  "original": 783,
                  "effective": 783
                },
                "global_mmlu_full_de_business_ethics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_virology": {
                  "original": 166,
                  "effective": 166
                },
                "global_mmlu_full_de_medical_genetics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_human_aging": {
                  "original": 223,
                  "effective": 223
                },
                "global_mmlu_full_de_global_facts": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "original": 265,
                  "effective": 265
                },
                "global_mmlu_full_de_management": {
                  "original": 103,
                  "effective": 103
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "original": 545,
                  "effective": 545
                },
                "global_mmlu_full_de_professional_psychology": {
                  "original": 612,
                  "effective": 612
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "original": 193,
                  "effective": 193
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "original": 390,
                  "effective": 390
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "original": 238,
                  "effective": 238
                },
                "global_mmlu_full_de_econometrics": {
                  "original": 114,
                  "effective": 114
                },
                "global_mmlu_full_de_high_school_geography": {
                  "original": 198,
                  "effective": 198
                },
                "global_mmlu_full_de_security_studies": {
                  "original": 245,
                  "effective": 245
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_public_relations": {
                  "original": 110,
                  "effective": 110
                },
                "global_mmlu_full_de_human_sexuality": {
                  "original": 131,
                  "effective": 131
                },
                "global_mmlu_full_de_sociology": {
                  "original": 201,
                  "effective": 201
                },
                "global_mmlu_full_de_professional_law": {
                  "original": 1534,
                  "effective": 1534
                },
                "global_mmlu_full_de_international_law": {
                  "original": 121,
                  "effective": 121
                },
                "global_mmlu_full_de_prehistory": {
                  "original": 324,
                  "effective": 324
                },
                "global_mmlu_full_de_world_religions": {
                  "original": 171,
                  "effective": 171
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "original": 237,
                  "effective": 237
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "original": 204,
                  "effective": 204
                },
                "global_mmlu_full_de_jurisprudence": {
                  "original": 108,
                  "effective": 108
                },
                "global_mmlu_full_de_formal_logic": {
                  "original": 126,
                  "effective": 126
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "original": 163,
                  "effective": 163
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "original": 165,
                  "effective": 165
                },
                "global_mmlu_full_de_philosophy": {
                  "original": 311,
                  "effective": 311
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "original": 895,
                  "effective": 895
                },
                "global_mmlu_full_de_moral_disputes": {
                  "original": 346,
                  "effective": 346
                }
              },
              "config": {
                "model": "hf",
                "model_args": "pretrained=/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k,trust_remote_code=True,local_files_only=True,trust_remote_code=True",
                "model_num_parameters": 222882048,
                "model_dtype": "torch.float32",
                "model_revision": "main",
                "model_sha": "",
                "batch_size": "auto",
                "batch_sizes": [
                  64
                ],
                "device": "cuda",
                "use_cache": null,
                "limit": null,
                "bootstrap_iters": 100000,
                "gen_kwargs": null,
                "random_seed": 42,
                "numpy_seed": 42,
                "torch_seed": 42,
                "fewshot_seed": 42
              },
              "git_hash": "a4103c7",
              "date": 1758647683.1742077,
              "pretty_env_info": "PyTorch version: 2.7.1+cu126\nIs debug build: False\nCUDA used to build PyTorch: 12.6\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.2 LTS (x86_64)\nGCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\nClang version: Could not collect\nCMake version: version 3.24.1\nLibc version: glibc-2.35\n\nPython version: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] (64-bit runtime)\nPython platform: Linux-6.8.0-1036-nvidia-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H200\nNvidia driver version: 580.82.07\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               224\nOn-line CPU(s) list:                  0-223\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480C\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   56\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          3800.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            5.3 MiB (112 instances)\nL1i cache:                            3.5 MiB (112 instances)\nL2 cache:                             224 MiB (112 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-55,112-167\nNUMA node1 CPU(s):                    56-111,168-223\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pytorch-lightning==2.5.2\n[pip3] torch==2.7.1\n[pip3] torchmetrics==1.8.0\n[pip3] triton==3.3.1\n[conda] Could not collect",
              "transformers_version": "4.54.1",
              "lm_eval_version": "0.4.9",
              "upper_git_hash": null,
              "tokenizer_pad_token": [
                "<pad>",
                "0"
              ],
              "tokenizer_eos_token": [
                "</s>",
                "1"
              ],
              "tokenizer_bos_token": [
                null,
                "None"
              ],
              "eot_token_id": 1,
              "max_length": 512,
              "task_hashes": {},
              "model_source": "hf",
              "model_name": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
              "model_name_sanitized": "__netscratch__nrauscher__projects__BA-hydra__evaluation__models__temp__wechsel-transfer-en-487k-continued-pretraind-on-german-15k",
              "system_instruction": null,
              "system_instruction_sha": null,
              "fewshot_as_multiturn": false,
              "chat_template": null,
              "chat_template_sha": null,
              "start_time": 633826.669510732,
              "end_time": 634458.443191587,
              "total_evaluation_time_seconds": "631.773680854938"
            },
            "duration_seconds": 680.0001924037933,
            "duration_minutes": 11.333336540063222,
            "command": "python -m lm_eval --model hf --model_args pretrained=/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/wechsel-transfer-en-487k-continued-pretraind-on-german-15k,trust_remote_code=True,local_files_only=True --tasks global_mmlu_full_de --device cuda --batch_size auto --output_path /netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/wechsel-transfer-en-487k-continued-pretraind-on-german-15k_global_mmlu_de_0shot_20250923_191308 --num_fewshot 0 --trust_remote_code --seed 42 --wandb_args project=BA-T5-CrossLingual,entity=nikolas-rauscher-dfki,group=crosslingual_transfer_eval_full_15k-final,name=wechsel-transfer-en-487k-continued-pretraind-on-german-15k_global_mmlu_de_0shot",
            "output_dir": "/netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/wechsel-transfer-en-487k-continued-pretraind-on-german-15k_global_mmlu_de_0shot_20250923_191308",
            "status": "success"
          }
        }
      }
    },
    "wechsel-transfer-en-487k": {
      "model_config": {
        "source_path": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k",
        "name": "wechsel-transfer-en-487k"
      },
      "model_path": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k",
      "metadata": {
        "source_type": "huggingface",
        "original_source": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k"
      },
      "benchmarks": {
        "global_mmlu_en": {
          "0_shot": {
            "parse_status": "partial",
            "error": "Could not find primary metric",
            "raw_results": {
              "results": {
                "global_mmlu_full_en": {
                  "acc,none": 0.24341261928500213,
                  "acc_stderr,none": 0.0036203050708889834,
                  "alias": "global_mmlu_full_en"
                },
                "global_mmlu_full_en_humanities": {
                  "acc,none": 0.24845908607863976,
                  "acc_stderr,none": 0.006298672612867114,
                  "alias": " - global_mmlu_full_en_humanities"
                },
                "global_mmlu_full_en_formal_logic": {
                  "alias": "  - global_mmlu_full_en_formal_logic",
                  "acc,none": 0.30158730158730157,
                  "acc_stderr,none": 0.04104947269903394
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "alias": "  - global_mmlu_full_en_high_school_european_history",
                  "acc,none": 0.21818181818181817,
                  "acc_stderr,none": 0.03225078108306289
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "alias": "  - global_mmlu_full_en_high_school_us_history",
                  "acc,none": 0.24019607843137256,
                  "acc_stderr,none": 0.02998373305591361
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "alias": "  - global_mmlu_full_en_high_school_world_history",
                  "acc,none": 0.27848101265822783,
                  "acc_stderr,none": 0.029178682304842562
                },
                "global_mmlu_full_en_international_law": {
                  "alias": "  - global_mmlu_full_en_international_law",
                  "acc,none": 0.2892561983471074,
                  "acc_stderr,none": 0.041391127276354626
                },
                "global_mmlu_full_en_jurisprudence": {
                  "alias": "  - global_mmlu_full_en_jurisprudence",
                  "acc,none": 0.2777777777777778,
                  "acc_stderr,none": 0.04330043749650742
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "alias": "  - global_mmlu_full_en_logical_fallacies",
                  "acc,none": 0.18404907975460122,
                  "acc_stderr,none": 0.030446777687971716
                },
                "global_mmlu_full_en_moral_disputes": {
                  "alias": "  - global_mmlu_full_en_moral_disputes",
                  "acc,none": 0.2543352601156069,
                  "acc_stderr,none": 0.023445826276545543
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "alias": "  - global_mmlu_full_en_moral_scenarios",
                  "acc,none": 0.24022346368715083,
                  "acc_stderr,none": 0.014288343803925307
                },
                "global_mmlu_full_en_philosophy": {
                  "alias": "  - global_mmlu_full_en_philosophy",
                  "acc,none": 0.21543408360128619,
                  "acc_stderr,none": 0.023350225475471414
                },
                "global_mmlu_full_en_prehistory": {
                  "alias": "  - global_mmlu_full_en_prehistory",
                  "acc,none": 0.2808641975308642,
                  "acc_stderr,none": 0.025006469755799208
                },
                "global_mmlu_full_en_professional_law": {
                  "alias": "  - global_mmlu_full_en_professional_law",
                  "acc,none": 0.24445893089960888,
                  "acc_stderr,none": 0.010976425013113912
                },
                "global_mmlu_full_en_world_religions": {
                  "alias": "  - global_mmlu_full_en_world_religions",
                  "acc,none": 0.28654970760233917,
                  "acc_stderr,none": 0.03467826685703826
                },
                "global_mmlu_full_en_other": {
                  "acc,none": 0.251689732861281,
                  "acc_stderr,none": 0.007770989042640891,
                  "alias": " - global_mmlu_full_en_other"
                },
                "global_mmlu_full_en_business_ethics": {
                  "alias": "  - global_mmlu_full_en_business_ethics",
                  "acc,none": 0.3,
                  "acc_stderr,none": 0.046056618647183814
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "alias": "  - global_mmlu_full_en_clinical_knowledge",
                  "acc,none": 0.18867924528301888,
                  "acc_stderr,none": 0.024079995130062246
                },
                "global_mmlu_full_en_college_medicine": {
                  "alias": "  - global_mmlu_full_en_college_medicine",
                  "acc,none": 0.2543352601156069,
                  "acc_stderr,none": 0.0332055644308557
                },
                "global_mmlu_full_en_global_facts": {
                  "alias": "  - global_mmlu_full_en_global_facts",
                  "acc,none": 0.34,
                  "acc_stderr,none": 0.04760952285695235
                },
                "global_mmlu_full_en_human_aging": {
                  "alias": "  - global_mmlu_full_en_human_aging",
                  "acc,none": 0.273542600896861,
                  "acc_stderr,none": 0.029918586707798827
                },
                "global_mmlu_full_en_management": {
                  "alias": "  - global_mmlu_full_en_management",
                  "acc,none": 0.21359223300970873,
                  "acc_stderr,none": 0.04058042015646035
                },
                "global_mmlu_full_en_marketing": {
                  "alias": "  - global_mmlu_full_en_marketing",
                  "acc,none": 0.2905982905982906,
                  "acc_stderr,none": 0.029745048572674043
                },
                "global_mmlu_full_en_medical_genetics": {
                  "alias": "  - global_mmlu_full_en_medical_genetics",
                  "acc,none": 0.26,
                  "acc_stderr,none": 0.044084400227680794
                },
                "global_mmlu_full_en_miscellaneous": {
                  "alias": "  - global_mmlu_full_en_miscellaneous",
                  "acc,none": 0.2669220945083014,
                  "acc_stderr,none": 0.01581845089477756
                },
                "global_mmlu_full_en_nutrition": {
                  "alias": "  - global_mmlu_full_en_nutrition",
                  "acc,none": 0.21241830065359477,
                  "acc_stderr,none": 0.02342037547829613
                },
                "global_mmlu_full_en_professional_accounting": {
                  "alias": "  - global_mmlu_full_en_professional_accounting",
                  "acc,none": 0.2765957446808511,
                  "acc_stderr,none": 0.026684564340461008
                },
                "global_mmlu_full_en_professional_medicine": {
                  "alias": "  - global_mmlu_full_en_professional_medicine",
                  "acc,none": 0.1875,
                  "acc_stderr,none": 0.023709788253811766
                },
                "global_mmlu_full_en_virology": {
                  "alias": "  - global_mmlu_full_en_virology",
                  "acc,none": 0.26506024096385544,
                  "acc_stderr,none": 0.03436024037944967
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc,none": 0.2317192070198245,
                  "acc_stderr,none": 0.007612595647039741,
                  "alias": " - global_mmlu_full_en_social_sciences"
                },
                "global_mmlu_full_en_econometrics": {
                  "alias": "  - global_mmlu_full_en_econometrics",
                  "acc,none": 0.22807017543859648,
                  "acc_stderr,none": 0.03947152782669415
                },
                "global_mmlu_full_en_high_school_geography": {
                  "alias": "  - global_mmlu_full_en_high_school_geography",
                  "acc,none": 0.21717171717171718,
                  "acc_stderr,none": 0.029376616484945644
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "alias": "  - global_mmlu_full_en_high_school_government_and_politics",
                  "acc,none": 0.24352331606217617,
                  "acc_stderr,none": 0.03097543638684543
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "alias": "  - global_mmlu_full_en_high_school_macroeconomics",
                  "acc,none": 0.23846153846153847,
                  "acc_stderr,none": 0.021606294494647727
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "alias": "  - global_mmlu_full_en_high_school_microeconomics",
                  "acc,none": 0.22268907563025211,
                  "acc_stderr,none": 0.027025433498882378
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "alias": "  - global_mmlu_full_en_high_school_psychology",
                  "acc,none": 0.2036697247706422,
                  "acc_stderr,none": 0.017266742087630793
                },
                "global_mmlu_full_en_human_sexuality": {
                  "alias": "  - global_mmlu_full_en_human_sexuality",
                  "acc,none": 0.21374045801526717,
                  "acc_stderr,none": 0.0359546161177469
                },
                "global_mmlu_full_en_professional_psychology": {
                  "alias": "  - global_mmlu_full_en_professional_psychology",
                  "acc,none": 0.25980392156862747,
                  "acc_stderr,none": 0.017740899509177795
                },
                "global_mmlu_full_en_public_relations": {
                  "alias": "  - global_mmlu_full_en_public_relations",
                  "acc,none": 0.22727272727272727,
                  "acc_stderr,none": 0.040139645540727735
                },
                "global_mmlu_full_en_security_studies": {
                  "alias": "  - global_mmlu_full_en_security_studies",
                  "acc,none": 0.22040816326530613,
                  "acc_stderr,none": 0.026537045312145287
                },
                "global_mmlu_full_en_sociology": {
                  "alias": "  - global_mmlu_full_en_sociology",
                  "acc,none": 0.23383084577114427,
                  "acc_stderr,none": 0.029929415408348398
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "alias": "  - global_mmlu_full_en_us_foreign_policy",
                  "acc,none": 0.27,
                  "acc_stderr,none": 0.0446196043338474
                },
                "global_mmlu_full_en_stem": {
                  "acc,none": 0.2391373295274342,
                  "acc_stderr,none": 0.007601372814717036,
                  "alias": " - global_mmlu_full_en_stem"
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "alias": "  - global_mmlu_full_en_abstract_algebra",
                  "acc,none": 0.28,
                  "acc_stderr,none": 0.04512608598542127
                },
                "global_mmlu_full_en_anatomy": {
                  "alias": "  - global_mmlu_full_en_anatomy",
                  "acc,none": 0.3111111111111111,
                  "acc_stderr,none": 0.039992628766177214
                },
                "global_mmlu_full_en_astronomy": {
                  "alias": "  - global_mmlu_full_en_astronomy",
                  "acc,none": 0.19736842105263158,
                  "acc_stderr,none": 0.03238981601699397
                },
                "global_mmlu_full_en_college_biology": {
                  "alias": "  - global_mmlu_full_en_college_biology",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.03621034121889507
                },
                "global_mmlu_full_en_college_chemistry": {
                  "alias": "  - global_mmlu_full_en_college_chemistry",
                  "acc,none": 0.21,
                  "acc_stderr,none": 0.040936018074033256
                },
                "global_mmlu_full_en_college_computer_science": {
                  "alias": "  - global_mmlu_full_en_college_computer_science",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.04351941398892446
                },
                "global_mmlu_full_en_college_mathematics": {
                  "alias": "  - global_mmlu_full_en_college_mathematics",
                  "acc,none": 0.2,
                  "acc_stderr,none": 0.04020151261036845
                },
                "global_mmlu_full_en_college_physics": {
                  "alias": "  - global_mmlu_full_en_college_physics",
                  "acc,none": 0.22549019607843138,
                  "acc_stderr,none": 0.04158307533083286
                },
                "global_mmlu_full_en_computer_security": {
                  "alias": "  - global_mmlu_full_en_computer_security",
                  "acc,none": 0.26,
                  "acc_stderr,none": 0.044084400227680794
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "alias": "  - global_mmlu_full_en_conceptual_physics",
                  "acc,none": 0.23829787234042554,
                  "acc_stderr,none": 0.027851252973889795
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "alias": "  - global_mmlu_full_en_electrical_engineering",
                  "acc,none": 0.2206896551724138,
                  "acc_stderr,none": 0.03455930201924812
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "alias": "  - global_mmlu_full_en_elementary_mathematics",
                  "acc,none": 0.2671957671957672,
                  "acc_stderr,none": 0.02278967314577656
                },
                "global_mmlu_full_en_high_school_biology": {
                  "alias": "  - global_mmlu_full_en_high_school_biology",
                  "acc,none": 0.20967741935483872,
                  "acc_stderr,none": 0.02315787934908351
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "alias": "  - global_mmlu_full_en_high_school_chemistry",
                  "acc,none": 0.2660098522167488,
                  "acc_stderr,none": 0.03108982600293752
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "alias": "  - global_mmlu_full_en_high_school_computer_science",
                  "acc,none": 0.24,
                  "acc_stderr,none": 0.042923469599092816
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "alias": "  - global_mmlu_full_en_high_school_mathematics",
                  "acc,none": 0.2518518518518518,
                  "acc_stderr,none": 0.026466117538959912
                },
                "global_mmlu_full_en_high_school_physics": {
                  "alias": "  - global_mmlu_full_en_high_school_physics",
                  "acc,none": 0.2251655629139073,
                  "acc_stderr,none": 0.03410435282008937
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "alias": "  - global_mmlu_full_en_high_school_statistics",
                  "acc,none": 0.19444444444444445,
                  "acc_stderr,none": 0.026991454502036726
                },
                "global_mmlu_full_en_machine_learning": {
                  "alias": "  - global_mmlu_full_en_machine_learning",
                  "acc,none": 0.24107142857142858,
                  "acc_stderr,none": 0.04059867246952687
                }
              },
              "groups": {
                "global_mmlu_full_en": {
                  "acc,none": 0.24341261928500213,
                  "acc_stderr,none": 0.0036203050708889834,
                  "alias": "global_mmlu_full_en"
                },
                "global_mmlu_full_en_humanities": {
                  "acc,none": 0.24845908607863976,
                  "acc_stderr,none": 0.006298672612867114,
                  "alias": " - global_mmlu_full_en_humanities"
                },
                "global_mmlu_full_en_other": {
                  "acc,none": 0.251689732861281,
                  "acc_stderr,none": 0.007770989042640891,
                  "alias": " - global_mmlu_full_en_other"
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc,none": 0.2317192070198245,
                  "acc_stderr,none": 0.007612595647039741,
                  "alias": " - global_mmlu_full_en_social_sciences"
                },
                "global_mmlu_full_en_stem": {
                  "acc,none": 0.2391373295274342,
                  "acc_stderr,none": 0.007601372814717036,
                  "alias": " - global_mmlu_full_en_stem"
                }
              },
              "group_subtasks": {
                "global_mmlu_full_en_humanities": [
                  "global_mmlu_full_en_world_religions",
                  "global_mmlu_full_en_international_law",
                  "global_mmlu_full_en_moral_scenarios",
                  "global_mmlu_full_en_high_school_us_history",
                  "global_mmlu_full_en_jurisprudence",
                  "global_mmlu_full_en_high_school_world_history",
                  "global_mmlu_full_en_logical_fallacies",
                  "global_mmlu_full_en_philosophy",
                  "global_mmlu_full_en_professional_law",
                  "global_mmlu_full_en_high_school_european_history",
                  "global_mmlu_full_en_formal_logic",
                  "global_mmlu_full_en_moral_disputes",
                  "global_mmlu_full_en_prehistory"
                ],
                "global_mmlu_full_en_social_sciences": [
                  "global_mmlu_full_en_us_foreign_policy",
                  "global_mmlu_full_en_public_relations",
                  "global_mmlu_full_en_econometrics",
                  "global_mmlu_full_en_high_school_microeconomics",
                  "global_mmlu_full_en_human_sexuality",
                  "global_mmlu_full_en_security_studies",
                  "global_mmlu_full_en_professional_psychology",
                  "global_mmlu_full_en_high_school_geography",
                  "global_mmlu_full_en_high_school_government_and_politics",
                  "global_mmlu_full_en_high_school_psychology",
                  "global_mmlu_full_en_high_school_macroeconomics",
                  "global_mmlu_full_en_sociology"
                ],
                "global_mmlu_full_en_other": [
                  "global_mmlu_full_en_clinical_knowledge",
                  "global_mmlu_full_en_medical_genetics",
                  "global_mmlu_full_en_professional_medicine",
                  "global_mmlu_full_en_business_ethics",
                  "global_mmlu_full_en_marketing",
                  "global_mmlu_full_en_global_facts",
                  "global_mmlu_full_en_college_medicine",
                  "global_mmlu_full_en_human_aging",
                  "global_mmlu_full_en_management",
                  "global_mmlu_full_en_nutrition",
                  "global_mmlu_full_en_virology",
                  "global_mmlu_full_en_professional_accounting",
                  "global_mmlu_full_en_miscellaneous"
                ],
                "global_mmlu_full_en_stem": [
                  "global_mmlu_full_en_high_school_statistics",
                  "global_mmlu_full_en_computer_security",
                  "global_mmlu_full_en_machine_learning",
                  "global_mmlu_full_en_abstract_algebra",
                  "global_mmlu_full_en_astronomy",
                  "global_mmlu_full_en_college_computer_science",
                  "global_mmlu_full_en_high_school_biology",
                  "global_mmlu_full_en_high_school_computer_science",
                  "global_mmlu_full_en_high_school_chemistry",
                  "global_mmlu_full_en_elementary_mathematics",
                  "global_mmlu_full_en_college_physics",
                  "global_mmlu_full_en_high_school_physics",
                  "global_mmlu_full_en_college_chemistry",
                  "global_mmlu_full_en_college_biology",
                  "global_mmlu_full_en_college_mathematics",
                  "global_mmlu_full_en_conceptual_physics",
                  "global_mmlu_full_en_anatomy",
                  "global_mmlu_full_en_high_school_mathematics",
                  "global_mmlu_full_en_electrical_engineering"
                ],
                "global_mmlu_full_en": [
                  "global_mmlu_full_en_stem",
                  "global_mmlu_full_en_other",
                  "global_mmlu_full_en_social_sciences",
                  "global_mmlu_full_en_humanities"
                ]
              },
              "configs": {
                "global_mmlu_full_en_abstract_algebra": {
                  "task": "global_mmlu_full_en_abstract_algebra",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164197250>, subject='abstract_algebra')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_anatomy": {
                  "task": "global_mmlu_full_en_anatomy",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e81641945e0>, subject='anatomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_astronomy": {
                  "task": "global_mmlu_full_en_astronomy",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164196b00>, subject='astronomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_business_ethics": {
                  "task": "global_mmlu_full_en_business_ethics",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e81642153f0>, subject='business_ethics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "task": "global_mmlu_full_en_clinical_knowledge",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164195240>, subject='clinical_knowledge')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_college_biology": {
                  "task": "global_mmlu_full_en_college_biology",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164195000>, subject='college_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_college_chemistry": {
                  "task": "global_mmlu_full_en_college_chemistry",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164194af0>, subject='college_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_college_computer_science": {
                  "task": "global_mmlu_full_en_college_computer_science",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e81641972e0>, subject='college_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_college_mathematics": {
                  "task": "global_mmlu_full_en_college_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164194ca0>, subject='college_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_college_medicine": {
                  "task": "global_mmlu_full_en_college_medicine",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164214700>, subject='college_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_college_physics": {
                  "task": "global_mmlu_full_en_college_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164196950>, subject='college_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_computer_security": {
                  "task": "global_mmlu_full_en_computer_security",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164197880>, subject='computer_security')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "task": "global_mmlu_full_en_conceptual_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164195990>, subject='conceptual_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_econometrics": {
                  "task": "global_mmlu_full_en_econometrics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164215ea0>, subject='econometrics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "task": "global_mmlu_full_en_electrical_engineering",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164216560>, subject='electrical_engineering')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "task": "global_mmlu_full_en_elementary_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e81641963b0>, subject='elementary_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_formal_logic": {
                  "task": "global_mmlu_full_en_formal_logic",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8182e6d870>, subject='formal_logic')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_global_facts": {
                  "task": "global_mmlu_full_en_global_facts",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e81641957e0>, subject='global_facts')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_high_school_biology": {
                  "task": "global_mmlu_full_en_high_school_biology",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164194ee0>, subject='high_school_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "task": "global_mmlu_full_en_high_school_chemistry",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e81641943a0>, subject='high_school_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "task": "global_mmlu_full_en_high_school_computer_science",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164195360>, subject='high_school_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "task": "global_mmlu_full_en_high_school_european_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164375090>, subject='high_school_european_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_high_school_geography": {
                  "task": "global_mmlu_full_en_high_school_geography",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164377130>, subject='high_school_geography')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "task": "global_mmlu_full_en_high_school_government_and_politics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164214670>, subject='high_school_government_and_politics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "task": "global_mmlu_full_en_high_school_macroeconomics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164214310>, subject='high_school_macroeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "task": "global_mmlu_full_en_high_school_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164194d30>, subject='high_school_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "task": "global_mmlu_full_en_high_school_microeconomics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164214280>, subject='high_school_microeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_high_school_physics": {
                  "task": "global_mmlu_full_en_high_school_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e81641960e0>, subject='high_school_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "task": "global_mmlu_full_en_high_school_psychology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164214430>, subject='high_school_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "task": "global_mmlu_full_en_high_school_statistics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164196050>, subject='high_school_statistics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "task": "global_mmlu_full_en_high_school_us_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e81643767a0>, subject='high_school_us_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "task": "global_mmlu_full_en_high_school_world_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164376b00>, subject='high_school_world_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_human_aging": {
                  "task": "global_mmlu_full_en_human_aging",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e81642170a0>, subject='human_aging')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_human_sexuality": {
                  "task": "global_mmlu_full_en_human_sexuality",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e81642157e0>, subject='human_sexuality')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_international_law": {
                  "task": "global_mmlu_full_en_international_law",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e81643745e0>, subject='international_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_jurisprudence": {
                  "task": "global_mmlu_full_en_jurisprudence",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164377be0>, subject='jurisprudence')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "task": "global_mmlu_full_en_logical_fallacies",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e81643775b0>, subject='logical_fallacies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_machine_learning": {
                  "task": "global_mmlu_full_en_machine_learning",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164197640>, subject='machine_learning')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_management": {
                  "task": "global_mmlu_full_en_management",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164216b90>, subject='management')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_marketing": {
                  "task": "global_mmlu_full_en_marketing",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164215090>, subject='marketing')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_medical_genetics": {
                  "task": "global_mmlu_full_en_medical_genetics",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164194040>, subject='medical_genetics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_miscellaneous": {
                  "task": "global_mmlu_full_en_miscellaneous",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164217880>, subject='miscellaneous')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_moral_disputes": {
                  "task": "global_mmlu_full_en_moral_disputes",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164374af0>, subject='moral_disputes')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "task": "global_mmlu_full_en_moral_scenarios",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164375360>, subject='moral_scenarios')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_nutrition": {
                  "task": "global_mmlu_full_en_nutrition",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e81642164d0>, subject='nutrition')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_philosophy": {
                  "task": "global_mmlu_full_en_philosophy",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164375480>, subject='philosophy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_prehistory": {
                  "task": "global_mmlu_full_en_prehistory",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e816615c3a0>, subject='prehistory')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_professional_accounting": {
                  "task": "global_mmlu_full_en_professional_accounting",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164215990>, subject='professional_accounting')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_professional_law": {
                  "task": "global_mmlu_full_en_professional_law",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164376c20>, subject='professional_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_professional_medicine": {
                  "task": "global_mmlu_full_en_professional_medicine",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e81641955a0>, subject='professional_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_professional_psychology": {
                  "task": "global_mmlu_full_en_professional_psychology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e81643760e0>, subject='professional_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_public_relations": {
                  "task": "global_mmlu_full_en_public_relations",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164217250>, subject='public_relations')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_security_studies": {
                  "task": "global_mmlu_full_en_security_studies",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164216710>, subject='security_studies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_sociology": {
                  "task": "global_mmlu_full_en_sociology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e81643772e0>, subject='sociology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "task": "global_mmlu_full_en_us_foreign_policy",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164214e50>, subject='us_foreign_policy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_virology": {
                  "task": "global_mmlu_full_en_virology",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164216b00>, subject='virology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_en_world_religions": {
                  "task": "global_mmlu_full_en_world_religions",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7e8164377e20>, subject='world_religions')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                }
              },
              "versions": {
                "global_mmlu_full_en": 0.0,
                "global_mmlu_full_en_abstract_algebra": 0.0,
                "global_mmlu_full_en_anatomy": 0.0,
                "global_mmlu_full_en_astronomy": 0.0,
                "global_mmlu_full_en_business_ethics": 0.0,
                "global_mmlu_full_en_clinical_knowledge": 0.0,
                "global_mmlu_full_en_college_biology": 0.0,
                "global_mmlu_full_en_college_chemistry": 0.0,
                "global_mmlu_full_en_college_computer_science": 0.0,
                "global_mmlu_full_en_college_mathematics": 0.0,
                "global_mmlu_full_en_college_medicine": 0.0,
                "global_mmlu_full_en_college_physics": 0.0,
                "global_mmlu_full_en_computer_security": 0.0,
                "global_mmlu_full_en_conceptual_physics": 0.0,
                "global_mmlu_full_en_econometrics": 0.0,
                "global_mmlu_full_en_electrical_engineering": 0.0,
                "global_mmlu_full_en_elementary_mathematics": 0.0,
                "global_mmlu_full_en_formal_logic": 0.0,
                "global_mmlu_full_en_global_facts": 0.0,
                "global_mmlu_full_en_high_school_biology": 0.0,
                "global_mmlu_full_en_high_school_chemistry": 0.0,
                "global_mmlu_full_en_high_school_computer_science": 0.0,
                "global_mmlu_full_en_high_school_european_history": 0.0,
                "global_mmlu_full_en_high_school_geography": 0.0,
                "global_mmlu_full_en_high_school_government_and_politics": 0.0,
                "global_mmlu_full_en_high_school_macroeconomics": 0.0,
                "global_mmlu_full_en_high_school_mathematics": 0.0,
                "global_mmlu_full_en_high_school_microeconomics": 0.0,
                "global_mmlu_full_en_high_school_physics": 0.0,
                "global_mmlu_full_en_high_school_psychology": 0.0,
                "global_mmlu_full_en_high_school_statistics": 0.0,
                "global_mmlu_full_en_high_school_us_history": 0.0,
                "global_mmlu_full_en_high_school_world_history": 0.0,
                "global_mmlu_full_en_human_aging": 0.0,
                "global_mmlu_full_en_human_sexuality": 0.0,
                "global_mmlu_full_en_humanities": 0.0,
                "global_mmlu_full_en_international_law": 0.0,
                "global_mmlu_full_en_jurisprudence": 0.0,
                "global_mmlu_full_en_logical_fallacies": 0.0,
                "global_mmlu_full_en_machine_learning": 0.0,
                "global_mmlu_full_en_management": 0.0,
                "global_mmlu_full_en_marketing": 0.0,
                "global_mmlu_full_en_medical_genetics": 0.0,
                "global_mmlu_full_en_miscellaneous": 0.0,
                "global_mmlu_full_en_moral_disputes": 0.0,
                "global_mmlu_full_en_moral_scenarios": 0.0,
                "global_mmlu_full_en_nutrition": 0.0,
                "global_mmlu_full_en_other": 0.0,
                "global_mmlu_full_en_philosophy": 0.0,
                "global_mmlu_full_en_prehistory": 0.0,
                "global_mmlu_full_en_professional_accounting": 0.0,
                "global_mmlu_full_en_professional_law": 0.0,
                "global_mmlu_full_en_professional_medicine": 0.0,
                "global_mmlu_full_en_professional_psychology": 0.0,
                "global_mmlu_full_en_public_relations": 0.0,
                "global_mmlu_full_en_security_studies": 0.0,
                "global_mmlu_full_en_social_sciences": 0.0,
                "global_mmlu_full_en_sociology": 0.0,
                "global_mmlu_full_en_stem": 0.0,
                "global_mmlu_full_en_us_foreign_policy": 0.0,
                "global_mmlu_full_en_virology": 0.0,
                "global_mmlu_full_en_world_religions": 0.0
              },
              "n-shot": {
                "global_mmlu_full_en_abstract_algebra": 0,
                "global_mmlu_full_en_anatomy": 0,
                "global_mmlu_full_en_astronomy": 0,
                "global_mmlu_full_en_business_ethics": 0,
                "global_mmlu_full_en_clinical_knowledge": 0,
                "global_mmlu_full_en_college_biology": 0,
                "global_mmlu_full_en_college_chemistry": 0,
                "global_mmlu_full_en_college_computer_science": 0,
                "global_mmlu_full_en_college_mathematics": 0,
                "global_mmlu_full_en_college_medicine": 0,
                "global_mmlu_full_en_college_physics": 0,
                "global_mmlu_full_en_computer_security": 0,
                "global_mmlu_full_en_conceptual_physics": 0,
                "global_mmlu_full_en_econometrics": 0,
                "global_mmlu_full_en_electrical_engineering": 0,
                "global_mmlu_full_en_elementary_mathematics": 0,
                "global_mmlu_full_en_formal_logic": 0,
                "global_mmlu_full_en_global_facts": 0,
                "global_mmlu_full_en_high_school_biology": 0,
                "global_mmlu_full_en_high_school_chemistry": 0,
                "global_mmlu_full_en_high_school_computer_science": 0,
                "global_mmlu_full_en_high_school_european_history": 0,
                "global_mmlu_full_en_high_school_geography": 0,
                "global_mmlu_full_en_high_school_government_and_politics": 0,
                "global_mmlu_full_en_high_school_macroeconomics": 0,
                "global_mmlu_full_en_high_school_mathematics": 0,
                "global_mmlu_full_en_high_school_microeconomics": 0,
                "global_mmlu_full_en_high_school_physics": 0,
                "global_mmlu_full_en_high_school_psychology": 0,
                "global_mmlu_full_en_high_school_statistics": 0,
                "global_mmlu_full_en_high_school_us_history": 0,
                "global_mmlu_full_en_high_school_world_history": 0,
                "global_mmlu_full_en_human_aging": 0,
                "global_mmlu_full_en_human_sexuality": 0,
                "global_mmlu_full_en_international_law": 0,
                "global_mmlu_full_en_jurisprudence": 0,
                "global_mmlu_full_en_logical_fallacies": 0,
                "global_mmlu_full_en_machine_learning": 0,
                "global_mmlu_full_en_management": 0,
                "global_mmlu_full_en_marketing": 0,
                "global_mmlu_full_en_medical_genetics": 0,
                "global_mmlu_full_en_miscellaneous": 0,
                "global_mmlu_full_en_moral_disputes": 0,
                "global_mmlu_full_en_moral_scenarios": 0,
                "global_mmlu_full_en_nutrition": 0,
                "global_mmlu_full_en_philosophy": 0,
                "global_mmlu_full_en_prehistory": 0,
                "global_mmlu_full_en_professional_accounting": 0,
                "global_mmlu_full_en_professional_law": 0,
                "global_mmlu_full_en_professional_medicine": 0,
                "global_mmlu_full_en_professional_psychology": 0,
                "global_mmlu_full_en_public_relations": 0,
                "global_mmlu_full_en_security_studies": 0,
                "global_mmlu_full_en_sociology": 0,
                "global_mmlu_full_en_us_foreign_policy": 0,
                "global_mmlu_full_en_virology": 0,
                "global_mmlu_full_en_world_religions": 0
              },
              "higher_is_better": {
                "global_mmlu_full_en": {
                  "acc": true
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "acc": true
                },
                "global_mmlu_full_en_anatomy": {
                  "acc": true
                },
                "global_mmlu_full_en_astronomy": {
                  "acc": true
                },
                "global_mmlu_full_en_business_ethics": {
                  "acc": true
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "acc": true
                },
                "global_mmlu_full_en_college_biology": {
                  "acc": true
                },
                "global_mmlu_full_en_college_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_en_college_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_en_college_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_college_medicine": {
                  "acc": true
                },
                "global_mmlu_full_en_college_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_computer_security": {
                  "acc": true
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_econometrics": {
                  "acc": true
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "acc": true
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_formal_logic": {
                  "acc": true
                },
                "global_mmlu_full_en_global_facts": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_biology": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_geography": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "acc": true
                },
                "global_mmlu_full_en_human_aging": {
                  "acc": true
                },
                "global_mmlu_full_en_human_sexuality": {
                  "acc": true
                },
                "global_mmlu_full_en_humanities": {
                  "acc": true
                },
                "global_mmlu_full_en_international_law": {
                  "acc": true
                },
                "global_mmlu_full_en_jurisprudence": {
                  "acc": true
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "acc": true
                },
                "global_mmlu_full_en_machine_learning": {
                  "acc": true
                },
                "global_mmlu_full_en_management": {
                  "acc": true
                },
                "global_mmlu_full_en_marketing": {
                  "acc": true
                },
                "global_mmlu_full_en_medical_genetics": {
                  "acc": true
                },
                "global_mmlu_full_en_miscellaneous": {
                  "acc": true
                },
                "global_mmlu_full_en_moral_disputes": {
                  "acc": true
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "acc": true
                },
                "global_mmlu_full_en_nutrition": {
                  "acc": true
                },
                "global_mmlu_full_en_other": {
                  "acc": true
                },
                "global_mmlu_full_en_philosophy": {
                  "acc": true
                },
                "global_mmlu_full_en_prehistory": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_accounting": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_law": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_medicine": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_psychology": {
                  "acc": true
                },
                "global_mmlu_full_en_public_relations": {
                  "acc": true
                },
                "global_mmlu_full_en_security_studies": {
                  "acc": true
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc": true
                },
                "global_mmlu_full_en_sociology": {
                  "acc": true
                },
                "global_mmlu_full_en_stem": {
                  "acc": true
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "acc": true
                },
                "global_mmlu_full_en_virology": {
                  "acc": true
                },
                "global_mmlu_full_en_world_religions": {
                  "acc": true
                }
              },
              "n-samples": {
                "global_mmlu_full_en_high_school_statistics": {
                  "original": 216,
                  "effective": 216
                },
                "global_mmlu_full_en_computer_security": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_machine_learning": {
                  "original": 112,
                  "effective": 112
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_astronomy": {
                  "original": 152,
                  "effective": 152
                },
                "global_mmlu_full_en_college_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_high_school_biology": {
                  "original": 310,
                  "effective": 310
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "original": 203,
                  "effective": 203
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "original": 378,
                  "effective": 378
                },
                "global_mmlu_full_en_college_physics": {
                  "original": 102,
                  "effective": 102
                },
                "global_mmlu_full_en_high_school_physics": {
                  "original": 151,
                  "effective": 151
                },
                "global_mmlu_full_en_college_chemistry": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_college_biology": {
                  "original": 144,
                  "effective": 144
                },
                "global_mmlu_full_en_college_mathematics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "original": 235,
                  "effective": 235
                },
                "global_mmlu_full_en_anatomy": {
                  "original": 135,
                  "effective": 135
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "original": 270,
                  "effective": 270
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "original": 145,
                  "effective": 145
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "original": 265,
                  "effective": 265
                },
                "global_mmlu_full_en_medical_genetics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_professional_medicine": {
                  "original": 272,
                  "effective": 272
                },
                "global_mmlu_full_en_business_ethics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_marketing": {
                  "original": 234,
                  "effective": 234
                },
                "global_mmlu_full_en_global_facts": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_college_medicine": {
                  "original": 173,
                  "effective": 173
                },
                "global_mmlu_full_en_human_aging": {
                  "original": 223,
                  "effective": 223
                },
                "global_mmlu_full_en_management": {
                  "original": 103,
                  "effective": 103
                },
                "global_mmlu_full_en_nutrition": {
                  "original": 306,
                  "effective": 306
                },
                "global_mmlu_full_en_virology": {
                  "original": 166,
                  "effective": 166
                },
                "global_mmlu_full_en_professional_accounting": {
                  "original": 282,
                  "effective": 282
                },
                "global_mmlu_full_en_miscellaneous": {
                  "original": 783,
                  "effective": 783
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_public_relations": {
                  "original": 110,
                  "effective": 110
                },
                "global_mmlu_full_en_econometrics": {
                  "original": 114,
                  "effective": 114
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "original": 238,
                  "effective": 238
                },
                "global_mmlu_full_en_human_sexuality": {
                  "original": 131,
                  "effective": 131
                },
                "global_mmlu_full_en_security_studies": {
                  "original": 245,
                  "effective": 245
                },
                "global_mmlu_full_en_professional_psychology": {
                  "original": 612,
                  "effective": 612
                },
                "global_mmlu_full_en_high_school_geography": {
                  "original": 198,
                  "effective": 198
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "original": 193,
                  "effective": 193
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "original": 545,
                  "effective": 545
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "original": 390,
                  "effective": 390
                },
                "global_mmlu_full_en_sociology": {
                  "original": 201,
                  "effective": 201
                },
                "global_mmlu_full_en_world_religions": {
                  "original": 171,
                  "effective": 171
                },
                "global_mmlu_full_en_international_law": {
                  "original": 121,
                  "effective": 121
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "original": 895,
                  "effective": 895
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "original": 204,
                  "effective": 204
                },
                "global_mmlu_full_en_jurisprudence": {
                  "original": 108,
                  "effective": 108
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "original": 237,
                  "effective": 237
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "original": 163,
                  "effective": 163
                },
                "global_mmlu_full_en_philosophy": {
                  "original": 311,
                  "effective": 311
                },
                "global_mmlu_full_en_professional_law": {
                  "original": 1534,
                  "effective": 1534
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "original": 165,
                  "effective": 165
                },
                "global_mmlu_full_en_formal_logic": {
                  "original": 126,
                  "effective": 126
                },
                "global_mmlu_full_en_moral_disputes": {
                  "original": 346,
                  "effective": 346
                },
                "global_mmlu_full_en_prehistory": {
                  "original": 324,
                  "effective": 324
                }
              },
              "config": {
                "model": "hf",
                "model_args": "pretrained=cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model,trust_remote_code=True,local_files_only=True,tokenizer=cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer,trust_remote_code=True",
                "model_num_parameters": 222882048,
                "model_dtype": "torch.float32",
                "model_revision": "main",
                "model_sha": "",
                "batch_size": "auto",
                "batch_sizes": [
                  64
                ],
                "device": "cuda",
                "use_cache": null,
                "limit": null,
                "bootstrap_iters": 100000,
                "gen_kwargs": null,
                "random_seed": 42,
                "numpy_seed": 42,
                "torch_seed": 42,
                "fewshot_seed": 42
              },
              "git_hash": "a4103c7",
              "date": 1758646945.3427887,
              "pretty_env_info": "PyTorch version: 2.7.1+cu126\nIs debug build: False\nCUDA used to build PyTorch: 12.6\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.2 LTS (x86_64)\nGCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\nClang version: Could not collect\nCMake version: version 3.24.1\nLibc version: glibc-2.35\n\nPython version: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] (64-bit runtime)\nPython platform: Linux-6.8.0-1036-nvidia-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H200\nNvidia driver version: 580.82.07\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               224\nOn-line CPU(s) list:                  0-223\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480C\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   56\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          3800.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            5.3 MiB (112 instances)\nL1i cache:                            3.5 MiB (112 instances)\nL2 cache:                             224 MiB (112 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-55,112-167\nNUMA node1 CPU(s):                    56-111,168-223\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pytorch-lightning==2.5.2\n[pip3] torch==2.7.1\n[pip3] torchmetrics==1.8.0\n[pip3] triton==3.3.1\n[conda] Could not collect",
              "transformers_version": "4.54.1",
              "lm_eval_version": "0.4.9",
              "upper_git_hash": null,
              "tokenizer_pad_token": [
                "<pad>",
                "0"
              ],
              "tokenizer_eos_token": [
                "</s>",
                "1"
              ],
              "tokenizer_bos_token": [
                null,
                "None"
              ],
              "eot_token_id": 1,
              "max_length": 512,
              "task_hashes": {},
              "model_source": "hf",
              "model_name": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
              "model_name_sanitized": "cross_lingual_transfer__models__german_T5_Optimized_50Olap_clean_restart_487k__model",
              "system_instruction": null,
              "system_instruction_sha": null,
              "fewshot_as_multiturn": false,
              "chat_template": null,
              "chat_template_sha": null,
              "start_time": 633091.38129153,
              "end_time": 633783.737988087,
              "total_evaluation_time_seconds": "692.3566965570208"
            },
            "duration_seconds": 739.3223133087158,
            "duration_minutes": 12.322038555145264,
            "command": "python -m lm_eval --model hf --model_args pretrained=cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model,trust_remote_code=True,local_files_only=True,tokenizer=cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer --tasks global_mmlu_full_en --device cuda --batch_size auto --output_path /netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/wechsel-transfer-en-487k_global_mmlu_en_0shot_20250923_190053 --num_fewshot 0 --trust_remote_code --seed 42 --wandb_args project=BA-T5-CrossLingual,entity=nikolas-rauscher-dfki,group=crosslingual_transfer_eval_full_15k-final,name=wechsel-transfer-en-487k_global_mmlu_en_0shot",
            "output_dir": "/netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/wechsel-transfer-en-487k_global_mmlu_en_0shot_20250923_190053",
            "status": "success"
          }
        },
        "global_mmlu_de": {
          "0_shot": {
            "parse_status": "partial",
            "error": "Could not find primary metric",
            "raw_results": {
              "results": {
                "global_mmlu_full_de": {
                  "acc,none": 0.24633243127759577,
                  "acc_stderr,none": 0.0036310532133785956,
                  "alias": "global_mmlu_full_de"
                },
                "global_mmlu_full_de_humanities": {
                  "acc,none": 0.2558979808714134,
                  "acc_stderr,none": 0.006355990915587137,
                  "alias": " - global_mmlu_full_de_humanities"
                },
                "global_mmlu_full_de_formal_logic": {
                  "alias": "  - global_mmlu_full_de_formal_logic",
                  "acc,none": 0.2857142857142857,
                  "acc_stderr,none": 0.04040610178208841
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "alias": "  - global_mmlu_full_de_high_school_european_history",
                  "acc,none": 0.21818181818181817,
                  "acc_stderr,none": 0.03225078108306289
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "alias": "  - global_mmlu_full_de_high_school_us_history",
                  "acc,none": 0.25980392156862747,
                  "acc_stderr,none": 0.030778554678693264
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "alias": "  - global_mmlu_full_de_high_school_world_history",
                  "acc,none": 0.2911392405063291,
                  "acc_stderr,none": 0.029571601065753374
                },
                "global_mmlu_full_de_international_law": {
                  "alias": "  - global_mmlu_full_de_international_law",
                  "acc,none": 0.3140495867768595,
                  "acc_stderr,none": 0.04236964753041019
                },
                "global_mmlu_full_de_jurisprudence": {
                  "alias": "  - global_mmlu_full_de_jurisprudence",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.04186091791394607
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "alias": "  - global_mmlu_full_de_logical_fallacies",
                  "acc,none": 0.1901840490797546,
                  "acc_stderr,none": 0.03083349114628123
                },
                "global_mmlu_full_de_moral_disputes": {
                  "alias": "  - global_mmlu_full_de_moral_disputes",
                  "acc,none": 0.2658959537572254,
                  "acc_stderr,none": 0.023786203255508283
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "alias": "  - global_mmlu_full_de_moral_scenarios",
                  "acc,none": 0.2346368715083799,
                  "acc_stderr,none": 0.014173044098303679
                },
                "global_mmlu_full_de_philosophy": {
                  "alias": "  - global_mmlu_full_de_philosophy",
                  "acc,none": 0.21864951768488747,
                  "acc_stderr,none": 0.02347558141786111
                },
                "global_mmlu_full_de_prehistory": {
                  "alias": "  - global_mmlu_full_de_prehistory",
                  "acc,none": 0.3148148148148148,
                  "acc_stderr,none": 0.025842248700902164
                },
                "global_mmlu_full_de_professional_law": {
                  "alias": "  - global_mmlu_full_de_professional_law",
                  "acc,none": 0.25488917861799215,
                  "acc_stderr,none": 0.011130509812662977
                },
                "global_mmlu_full_de_world_religions": {
                  "alias": "  - global_mmlu_full_de_world_religions",
                  "acc,none": 0.2982456140350877,
                  "acc_stderr,none": 0.03508771929824565
                },
                "global_mmlu_full_de_other": {
                  "acc,none": 0.2491149018345671,
                  "acc_stderr,none": 0.007738161328877288,
                  "alias": " - global_mmlu_full_de_other"
                },
                "global_mmlu_full_de_business_ethics": {
                  "alias": "  - global_mmlu_full_de_business_ethics",
                  "acc,none": 0.34,
                  "acc_stderr,none": 0.04760952285695236
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "alias": "  - global_mmlu_full_de_clinical_knowledge",
                  "acc,none": 0.21509433962264152,
                  "acc_stderr,none": 0.02528839450289137
                },
                "global_mmlu_full_de_college_medicine": {
                  "alias": "  - global_mmlu_full_de_college_medicine",
                  "acc,none": 0.20809248554913296,
                  "acc_stderr,none": 0.03095289021774988
                },
                "global_mmlu_full_de_global_facts": {
                  "alias": "  - global_mmlu_full_de_global_facts",
                  "acc,none": 0.27,
                  "acc_stderr,none": 0.044619604333847394
                },
                "global_mmlu_full_de_human_aging": {
                  "alias": "  - global_mmlu_full_de_human_aging",
                  "acc,none": 0.26905829596412556,
                  "acc_stderr,none": 0.029763779406874975
                },
                "global_mmlu_full_de_management": {
                  "alias": "  - global_mmlu_full_de_management",
                  "acc,none": 0.1553398058252427,
                  "acc_stderr,none": 0.03586594738573972
                },
                "global_mmlu_full_de_marketing": {
                  "alias": "  - global_mmlu_full_de_marketing",
                  "acc,none": 0.31196581196581197,
                  "acc_stderr,none": 0.03035152732334496
                },
                "global_mmlu_full_de_medical_genetics": {
                  "alias": "  - global_mmlu_full_de_medical_genetics",
                  "acc,none": 0.3,
                  "acc_stderr,none": 0.046056618647183814
                },
                "global_mmlu_full_de_miscellaneous": {
                  "alias": "  - global_mmlu_full_de_miscellaneous",
                  "acc,none": 0.2656449553001277,
                  "acc_stderr,none": 0.015794302487888715
                },
                "global_mmlu_full_de_nutrition": {
                  "alias": "  - global_mmlu_full_de_nutrition",
                  "acc,none": 0.24183006535947713,
                  "acc_stderr,none": 0.024518195641879334
                },
                "global_mmlu_full_de_professional_accounting": {
                  "alias": "  - global_mmlu_full_de_professional_accounting",
                  "acc,none": 0.26595744680851063,
                  "acc_stderr,none": 0.0263580656988806
                },
                "global_mmlu_full_de_professional_medicine": {
                  "alias": "  - global_mmlu_full_de_professional_medicine",
                  "acc,none": 0.17647058823529413,
                  "acc_stderr,none": 0.023157468308559373
                },
                "global_mmlu_full_de_virology": {
                  "alias": "  - global_mmlu_full_de_virology",
                  "acc,none": 0.21686746987951808,
                  "acc_stderr,none": 0.03208284450356365
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc,none": 0.23074423139421515,
                  "acc_stderr,none": 0.0075951368878739885,
                  "alias": " - global_mmlu_full_de_social_sciences"
                },
                "global_mmlu_full_de_econometrics": {
                  "alias": "  - global_mmlu_full_de_econometrics",
                  "acc,none": 0.2543859649122807,
                  "acc_stderr,none": 0.040969851398436716
                },
                "global_mmlu_full_de_high_school_geography": {
                  "alias": "  - global_mmlu_full_de_high_school_geography",
                  "acc,none": 0.23232323232323232,
                  "acc_stderr,none": 0.030088629490217483
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "alias": "  - global_mmlu_full_de_high_school_government_and_politics",
                  "acc,none": 0.2538860103626943,
                  "acc_stderr,none": 0.0314102478056532
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "alias": "  - global_mmlu_full_de_high_school_macroeconomics",
                  "acc,none": 0.24871794871794872,
                  "acc_stderr,none": 0.021916957709213803
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "alias": "  - global_mmlu_full_de_high_school_microeconomics",
                  "acc,none": 0.21008403361344538,
                  "acc_stderr,none": 0.026461398717471874
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "alias": "  - global_mmlu_full_de_high_school_psychology",
                  "acc,none": 0.1834862385321101,
                  "acc_stderr,none": 0.01659525971039932
                },
                "global_mmlu_full_de_human_sexuality": {
                  "alias": "  - global_mmlu_full_de_human_sexuality",
                  "acc,none": 0.2748091603053435,
                  "acc_stderr,none": 0.03915345408847835
                },
                "global_mmlu_full_de_professional_psychology": {
                  "alias": "  - global_mmlu_full_de_professional_psychology",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.01751781884501444
                },
                "global_mmlu_full_de_public_relations": {
                  "alias": "  - global_mmlu_full_de_public_relations",
                  "acc,none": 0.21818181818181817,
                  "acc_stderr,none": 0.03955932861795833
                },
                "global_mmlu_full_de_security_studies": {
                  "alias": "  - global_mmlu_full_de_security_studies",
                  "acc,none": 0.22448979591836735,
                  "acc_stderr,none": 0.026711430555538426
                },
                "global_mmlu_full_de_sociology": {
                  "alias": "  - global_mmlu_full_de_sociology",
                  "acc,none": 0.23880597014925373,
                  "acc_stderr,none": 0.030147775935409217
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "alias": "  - global_mmlu_full_de_us_foreign_policy",
                  "acc,none": 0.23,
                  "acc_stderr,none": 0.04229525846816505
                },
                "global_mmlu_full_de_stem": {
                  "acc,none": 0.2445290199809705,
                  "acc_stderr,none": 0.007645783312687244,
                  "alias": " - global_mmlu_full_de_stem"
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "alias": "  - global_mmlu_full_de_abstract_algebra",
                  "acc,none": 0.24,
                  "acc_stderr,none": 0.04292346959909284
                },
                "global_mmlu_full_de_anatomy": {
                  "alias": "  - global_mmlu_full_de_anatomy",
                  "acc,none": 0.2518518518518518,
                  "acc_stderr,none": 0.03749850709174022
                },
                "global_mmlu_full_de_astronomy": {
                  "alias": "  - global_mmlu_full_de_astronomy",
                  "acc,none": 0.23026315789473684,
                  "acc_stderr,none": 0.03426059424403165
                },
                "global_mmlu_full_de_college_biology": {
                  "alias": "  - global_mmlu_full_de_college_biology",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.03621034121889507
                },
                "global_mmlu_full_de_college_chemistry": {
                  "alias": "  - global_mmlu_full_de_college_chemistry",
                  "acc,none": 0.21,
                  "acc_stderr,none": 0.040936018074033256
                },
                "global_mmlu_full_de_college_computer_science": {
                  "alias": "  - global_mmlu_full_de_college_computer_science",
                  "acc,none": 0.2,
                  "acc_stderr,none": 0.040201512610368445
                },
                "global_mmlu_full_de_college_mathematics": {
                  "alias": "  - global_mmlu_full_de_college_mathematics",
                  "acc,none": 0.17,
                  "acc_stderr,none": 0.0377525168068637
                },
                "global_mmlu_full_de_college_physics": {
                  "alias": "  - global_mmlu_full_de_college_physics",
                  "acc,none": 0.21568627450980393,
                  "acc_stderr,none": 0.04092563958237657
                },
                "global_mmlu_full_de_computer_security": {
                  "alias": "  - global_mmlu_full_de_computer_security",
                  "acc,none": 0.3,
                  "acc_stderr,none": 0.046056618647183814
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "alias": "  - global_mmlu_full_de_conceptual_physics",
                  "acc,none": 0.24680851063829787,
                  "acc_stderr,none": 0.028185441301234074
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "alias": "  - global_mmlu_full_de_electrical_engineering",
                  "acc,none": 0.2689655172413793,
                  "acc_stderr,none": 0.03695183311650232
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "alias": "  - global_mmlu_full_de_elementary_mathematics",
                  "acc,none": 0.2619047619047619,
                  "acc_stderr,none": 0.022644212615525218
                },
                "global_mmlu_full_de_high_school_biology": {
                  "alias": "  - global_mmlu_full_de_high_school_biology",
                  "acc,none": 0.21935483870967742,
                  "acc_stderr,none": 0.023540799358723316
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "alias": "  - global_mmlu_full_de_high_school_chemistry",
                  "acc,none": 0.2561576354679803,
                  "acc_stderr,none": 0.030712730070982592
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "alias": "  - global_mmlu_full_de_high_school_computer_science",
                  "acc,none": 0.35,
                  "acc_stderr,none": 0.0479372485441102
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "alias": "  - global_mmlu_full_de_high_school_mathematics",
                  "acc,none": 0.26666666666666666,
                  "acc_stderr,none": 0.02696242432507384
                },
                "global_mmlu_full_de_high_school_physics": {
                  "alias": "  - global_mmlu_full_de_high_school_physics",
                  "acc,none": 0.25165562913907286,
                  "acc_stderr,none": 0.03543304234389986
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "alias": "  - global_mmlu_full_de_high_school_statistics",
                  "acc,none": 0.16666666666666666,
                  "acc_stderr,none": 0.025416428388767478
                },
                "global_mmlu_full_de_machine_learning": {
                  "alias": "  - global_mmlu_full_de_machine_learning",
                  "acc,none": 0.3125,
                  "acc_stderr,none": 0.043994650575715215
                }
              },
              "groups": {
                "global_mmlu_full_de": {
                  "acc,none": 0.24633243127759577,
                  "acc_stderr,none": 0.0036310532133785956,
                  "alias": "global_mmlu_full_de"
                },
                "global_mmlu_full_de_humanities": {
                  "acc,none": 0.2558979808714134,
                  "acc_stderr,none": 0.006355990915587137,
                  "alias": " - global_mmlu_full_de_humanities"
                },
                "global_mmlu_full_de_other": {
                  "acc,none": 0.2491149018345671,
                  "acc_stderr,none": 0.007738161328877288,
                  "alias": " - global_mmlu_full_de_other"
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc,none": 0.23074423139421515,
                  "acc_stderr,none": 0.0075951368878739885,
                  "alias": " - global_mmlu_full_de_social_sciences"
                },
                "global_mmlu_full_de_stem": {
                  "acc,none": 0.2445290199809705,
                  "acc_stderr,none": 0.007645783312687244,
                  "alias": " - global_mmlu_full_de_stem"
                }
              },
              "group_subtasks": {
                "global_mmlu_full_de_humanities": [
                  "global_mmlu_full_de_professional_law",
                  "global_mmlu_full_de_international_law",
                  "global_mmlu_full_de_prehistory",
                  "global_mmlu_full_de_world_religions",
                  "global_mmlu_full_de_high_school_world_history",
                  "global_mmlu_full_de_high_school_us_history",
                  "global_mmlu_full_de_jurisprudence",
                  "global_mmlu_full_de_formal_logic",
                  "global_mmlu_full_de_logical_fallacies",
                  "global_mmlu_full_de_high_school_european_history",
                  "global_mmlu_full_de_philosophy",
                  "global_mmlu_full_de_moral_scenarios",
                  "global_mmlu_full_de_moral_disputes"
                ],
                "global_mmlu_full_de_social_sciences": [
                  "global_mmlu_full_de_high_school_psychology",
                  "global_mmlu_full_de_professional_psychology",
                  "global_mmlu_full_de_high_school_government_and_politics",
                  "global_mmlu_full_de_high_school_macroeconomics",
                  "global_mmlu_full_de_high_school_microeconomics",
                  "global_mmlu_full_de_econometrics",
                  "global_mmlu_full_de_high_school_geography",
                  "global_mmlu_full_de_security_studies",
                  "global_mmlu_full_de_us_foreign_policy",
                  "global_mmlu_full_de_public_relations",
                  "global_mmlu_full_de_human_sexuality",
                  "global_mmlu_full_de_sociology"
                ],
                "global_mmlu_full_de_other": [
                  "global_mmlu_full_de_nutrition",
                  "global_mmlu_full_de_professional_medicine",
                  "global_mmlu_full_de_college_medicine",
                  "global_mmlu_full_de_professional_accounting",
                  "global_mmlu_full_de_marketing",
                  "global_mmlu_full_de_miscellaneous",
                  "global_mmlu_full_de_business_ethics",
                  "global_mmlu_full_de_virology",
                  "global_mmlu_full_de_medical_genetics",
                  "global_mmlu_full_de_human_aging",
                  "global_mmlu_full_de_global_facts",
                  "global_mmlu_full_de_clinical_knowledge",
                  "global_mmlu_full_de_management"
                ],
                "global_mmlu_full_de_stem": [
                  "global_mmlu_full_de_computer_security",
                  "global_mmlu_full_de_college_biology",
                  "global_mmlu_full_de_college_mathematics",
                  "global_mmlu_full_de_conceptual_physics",
                  "global_mmlu_full_de_elementary_mathematics",
                  "global_mmlu_full_de_astronomy",
                  "global_mmlu_full_de_high_school_physics",
                  "global_mmlu_full_de_machine_learning",
                  "global_mmlu_full_de_college_chemistry",
                  "global_mmlu_full_de_high_school_chemistry",
                  "global_mmlu_full_de_college_computer_science",
                  "global_mmlu_full_de_high_school_computer_science",
                  "global_mmlu_full_de_high_school_mathematics",
                  "global_mmlu_full_de_abstract_algebra",
                  "global_mmlu_full_de_high_school_biology",
                  "global_mmlu_full_de_electrical_engineering",
                  "global_mmlu_full_de_college_physics",
                  "global_mmlu_full_de_anatomy",
                  "global_mmlu_full_de_high_school_statistics"
                ],
                "global_mmlu_full_de": [
                  "global_mmlu_full_de_stem",
                  "global_mmlu_full_de_other",
                  "global_mmlu_full_de_social_sciences",
                  "global_mmlu_full_de_humanities"
                ]
              },
              "configs": {
                "global_mmlu_full_de_abstract_algebra": {
                  "task": "global_mmlu_full_de_abstract_algebra",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b984808f70>, subject='abstract_algebra')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_anatomy": {
                  "task": "global_mmlu_full_de_anatomy",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9848083a0>, subject='anatomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_astronomy": {
                  "task": "global_mmlu_full_de_astronomy",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b984809120>, subject='astronomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_business_ethics": {
                  "task": "global_mmlu_full_de_business_ethics",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041f640>, subject='business_ethics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "task": "global_mmlu_full_de_clinical_knowledge",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041d510>, subject='clinical_knowledge')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_college_biology": {
                  "task": "global_mmlu_full_de_college_biology",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b98480b400>, subject='college_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_college_chemistry": {
                  "task": "global_mmlu_full_de_college_chemistry",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b98480be20>, subject='college_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_college_computer_science": {
                  "task": "global_mmlu_full_de_college_computer_science",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b984808040>, subject='college_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_college_mathematics": {
                  "task": "global_mmlu_full_de_college_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f0628430>, subject='college_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_college_medicine": {
                  "task": "global_mmlu_full_de_college_medicine",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b984808310>, subject='college_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_college_physics": {
                  "task": "global_mmlu_full_de_college_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b984808700>, subject='college_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_computer_security": {
                  "task": "global_mmlu_full_de_computer_security",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b98480b7f0>, subject='computer_security')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "task": "global_mmlu_full_de_conceptual_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f06297e0>, subject='conceptual_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_econometrics": {
                  "task": "global_mmlu_full_de_econometrics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041d090>, subject='econometrics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "task": "global_mmlu_full_de_electrical_engineering",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b98480a320>, subject='electrical_engineering')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "task": "global_mmlu_full_de_elementary_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f0629240>, subject='elementary_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_formal_logic": {
                  "task": "global_mmlu_full_de_formal_logic",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f0954dc0>, subject='formal_logic')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_global_facts": {
                  "task": "global_mmlu_full_de_global_facts",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041e710>, subject='global_facts')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_high_school_biology": {
                  "task": "global_mmlu_full_de_high_school_biology",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b98480b1c0>, subject='high_school_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "task": "global_mmlu_full_de_high_school_chemistry",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b98480bac0>, subject='high_school_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "task": "global_mmlu_full_de_high_school_computer_science",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b984808b80>, subject='high_school_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "task": "global_mmlu_full_de_high_school_european_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f0954a60>, subject='high_school_european_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_high_school_geography": {
                  "task": "global_mmlu_full_de_high_school_geography",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f0956cb0>, subject='high_school_geography')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "task": "global_mmlu_full_de_high_school_government_and_politics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041ea70>, subject='high_school_government_and_politics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "task": "global_mmlu_full_de_high_school_macroeconomics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041c160>, subject='high_school_macroeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "task": "global_mmlu_full_de_high_school_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b984809510>, subject='high_school_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "task": "global_mmlu_full_de_high_school_microeconomics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041d7e0>, subject='high_school_microeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_high_school_physics": {
                  "task": "global_mmlu_full_de_high_school_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b98480b0a0>, subject='high_school_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "task": "global_mmlu_full_de_high_school_psychology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041f2e0>, subject='high_school_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "task": "global_mmlu_full_de_high_school_statistics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041c040>, subject='high_school_statistics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "task": "global_mmlu_full_de_high_school_us_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f09549d0>, subject='high_school_us_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "task": "global_mmlu_full_de_high_school_world_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f0954e50>, subject='high_school_world_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_human_aging": {
                  "task": "global_mmlu_full_de_human_aging",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041f5b0>, subject='human_aging')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_human_sexuality": {
                  "task": "global_mmlu_full_de_human_sexuality",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f0956dd0>, subject='human_sexuality')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_international_law": {
                  "task": "global_mmlu_full_de_international_law",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f0955480>, subject='international_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_jurisprudence": {
                  "task": "global_mmlu_full_de_jurisprudence",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f09563b0>, subject='jurisprudence')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "task": "global_mmlu_full_de_logical_fallacies",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f0956950>, subject='logical_fallacies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_machine_learning": {
                  "task": "global_mmlu_full_de_machine_learning",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b98480bbe0>, subject='machine_learning')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_management": {
                  "task": "global_mmlu_full_de_management",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041ecb0>, subject='management')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_marketing": {
                  "task": "global_mmlu_full_de_marketing",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041df30>, subject='marketing')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_medical_genetics": {
                  "task": "global_mmlu_full_de_medical_genetics",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041d2d0>, subject='medical_genetics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_miscellaneous": {
                  "task": "global_mmlu_full_de_miscellaneous",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041c310>, subject='miscellaneous')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_moral_disputes": {
                  "task": "global_mmlu_full_de_moral_disputes",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79bc180dc3a0>, subject='moral_disputes')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "task": "global_mmlu_full_de_moral_scenarios",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f09551b0>, subject='moral_scenarios')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_nutrition": {
                  "task": "global_mmlu_full_de_nutrition",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b984809480>, subject='nutrition')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_philosophy": {
                  "task": "global_mmlu_full_de_philosophy",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79bc1d079870>, subject='philosophy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_prehistory": {
                  "task": "global_mmlu_full_de_prehistory",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f0955870>, subject='prehistory')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_professional_accounting": {
                  "task": "global_mmlu_full_de_professional_accounting",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041fa30>, subject='professional_accounting')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_professional_law": {
                  "task": "global_mmlu_full_de_professional_law",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f0957d90>, subject='professional_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_professional_medicine": {
                  "task": "global_mmlu_full_de_professional_medicine",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b984808430>, subject='professional_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_professional_psychology": {
                  "task": "global_mmlu_full_de_professional_psychology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041c4c0>, subject='professional_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_public_relations": {
                  "task": "global_mmlu_full_de_public_relations",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f0954d30>, subject='public_relations')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_security_studies": {
                  "task": "global_mmlu_full_de_security_studies",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f0956200>, subject='security_studies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_sociology": {
                  "task": "global_mmlu_full_de_sociology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f0957eb0>, subject='sociology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "task": "global_mmlu_full_de_us_foreign_policy",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041c1f0>, subject='us_foreign_policy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_virology": {
                  "task": "global_mmlu_full_de_virology",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f041f400>, subject='virology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                },
                "global_mmlu_full_de_world_religions": {
                  "task": "global_mmlu_full_de_world_religions",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79b9f0954670>, subject='world_religions')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
                    "trust_remote_code": true,
                    "local_files_only": true,
                    "tokenizer": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer"
                  }
                }
              },
              "versions": {
                "global_mmlu_full_de": 0.0,
                "global_mmlu_full_de_abstract_algebra": 0.0,
                "global_mmlu_full_de_anatomy": 0.0,
                "global_mmlu_full_de_astronomy": 0.0,
                "global_mmlu_full_de_business_ethics": 0.0,
                "global_mmlu_full_de_clinical_knowledge": 0.0,
                "global_mmlu_full_de_college_biology": 0.0,
                "global_mmlu_full_de_college_chemistry": 0.0,
                "global_mmlu_full_de_college_computer_science": 0.0,
                "global_mmlu_full_de_college_mathematics": 0.0,
                "global_mmlu_full_de_college_medicine": 0.0,
                "global_mmlu_full_de_college_physics": 0.0,
                "global_mmlu_full_de_computer_security": 0.0,
                "global_mmlu_full_de_conceptual_physics": 0.0,
                "global_mmlu_full_de_econometrics": 0.0,
                "global_mmlu_full_de_electrical_engineering": 0.0,
                "global_mmlu_full_de_elementary_mathematics": 0.0,
                "global_mmlu_full_de_formal_logic": 0.0,
                "global_mmlu_full_de_global_facts": 0.0,
                "global_mmlu_full_de_high_school_biology": 0.0,
                "global_mmlu_full_de_high_school_chemistry": 0.0,
                "global_mmlu_full_de_high_school_computer_science": 0.0,
                "global_mmlu_full_de_high_school_european_history": 0.0,
                "global_mmlu_full_de_high_school_geography": 0.0,
                "global_mmlu_full_de_high_school_government_and_politics": 0.0,
                "global_mmlu_full_de_high_school_macroeconomics": 0.0,
                "global_mmlu_full_de_high_school_mathematics": 0.0,
                "global_mmlu_full_de_high_school_microeconomics": 0.0,
                "global_mmlu_full_de_high_school_physics": 0.0,
                "global_mmlu_full_de_high_school_psychology": 0.0,
                "global_mmlu_full_de_high_school_statistics": 0.0,
                "global_mmlu_full_de_high_school_us_history": 0.0,
                "global_mmlu_full_de_high_school_world_history": 0.0,
                "global_mmlu_full_de_human_aging": 0.0,
                "global_mmlu_full_de_human_sexuality": 0.0,
                "global_mmlu_full_de_humanities": 0.0,
                "global_mmlu_full_de_international_law": 0.0,
                "global_mmlu_full_de_jurisprudence": 0.0,
                "global_mmlu_full_de_logical_fallacies": 0.0,
                "global_mmlu_full_de_machine_learning": 0.0,
                "global_mmlu_full_de_management": 0.0,
                "global_mmlu_full_de_marketing": 0.0,
                "global_mmlu_full_de_medical_genetics": 0.0,
                "global_mmlu_full_de_miscellaneous": 0.0,
                "global_mmlu_full_de_moral_disputes": 0.0,
                "global_mmlu_full_de_moral_scenarios": 0.0,
                "global_mmlu_full_de_nutrition": 0.0,
                "global_mmlu_full_de_other": 0.0,
                "global_mmlu_full_de_philosophy": 0.0,
                "global_mmlu_full_de_prehistory": 0.0,
                "global_mmlu_full_de_professional_accounting": 0.0,
                "global_mmlu_full_de_professional_law": 0.0,
                "global_mmlu_full_de_professional_medicine": 0.0,
                "global_mmlu_full_de_professional_psychology": 0.0,
                "global_mmlu_full_de_public_relations": 0.0,
                "global_mmlu_full_de_security_studies": 0.0,
                "global_mmlu_full_de_social_sciences": 0.0,
                "global_mmlu_full_de_sociology": 0.0,
                "global_mmlu_full_de_stem": 0.0,
                "global_mmlu_full_de_us_foreign_policy": 0.0,
                "global_mmlu_full_de_virology": 0.0,
                "global_mmlu_full_de_world_religions": 0.0
              },
              "n-shot": {
                "global_mmlu_full_de_abstract_algebra": 0,
                "global_mmlu_full_de_anatomy": 0,
                "global_mmlu_full_de_astronomy": 0,
                "global_mmlu_full_de_business_ethics": 0,
                "global_mmlu_full_de_clinical_knowledge": 0,
                "global_mmlu_full_de_college_biology": 0,
                "global_mmlu_full_de_college_chemistry": 0,
                "global_mmlu_full_de_college_computer_science": 0,
                "global_mmlu_full_de_college_mathematics": 0,
                "global_mmlu_full_de_college_medicine": 0,
                "global_mmlu_full_de_college_physics": 0,
                "global_mmlu_full_de_computer_security": 0,
                "global_mmlu_full_de_conceptual_physics": 0,
                "global_mmlu_full_de_econometrics": 0,
                "global_mmlu_full_de_electrical_engineering": 0,
                "global_mmlu_full_de_elementary_mathematics": 0,
                "global_mmlu_full_de_formal_logic": 0,
                "global_mmlu_full_de_global_facts": 0,
                "global_mmlu_full_de_high_school_biology": 0,
                "global_mmlu_full_de_high_school_chemistry": 0,
                "global_mmlu_full_de_high_school_computer_science": 0,
                "global_mmlu_full_de_high_school_european_history": 0,
                "global_mmlu_full_de_high_school_geography": 0,
                "global_mmlu_full_de_high_school_government_and_politics": 0,
                "global_mmlu_full_de_high_school_macroeconomics": 0,
                "global_mmlu_full_de_high_school_mathematics": 0,
                "global_mmlu_full_de_high_school_microeconomics": 0,
                "global_mmlu_full_de_high_school_physics": 0,
                "global_mmlu_full_de_high_school_psychology": 0,
                "global_mmlu_full_de_high_school_statistics": 0,
                "global_mmlu_full_de_high_school_us_history": 0,
                "global_mmlu_full_de_high_school_world_history": 0,
                "global_mmlu_full_de_human_aging": 0,
                "global_mmlu_full_de_human_sexuality": 0,
                "global_mmlu_full_de_international_law": 0,
                "global_mmlu_full_de_jurisprudence": 0,
                "global_mmlu_full_de_logical_fallacies": 0,
                "global_mmlu_full_de_machine_learning": 0,
                "global_mmlu_full_de_management": 0,
                "global_mmlu_full_de_marketing": 0,
                "global_mmlu_full_de_medical_genetics": 0,
                "global_mmlu_full_de_miscellaneous": 0,
                "global_mmlu_full_de_moral_disputes": 0,
                "global_mmlu_full_de_moral_scenarios": 0,
                "global_mmlu_full_de_nutrition": 0,
                "global_mmlu_full_de_philosophy": 0,
                "global_mmlu_full_de_prehistory": 0,
                "global_mmlu_full_de_professional_accounting": 0,
                "global_mmlu_full_de_professional_law": 0,
                "global_mmlu_full_de_professional_medicine": 0,
                "global_mmlu_full_de_professional_psychology": 0,
                "global_mmlu_full_de_public_relations": 0,
                "global_mmlu_full_de_security_studies": 0,
                "global_mmlu_full_de_sociology": 0,
                "global_mmlu_full_de_us_foreign_policy": 0,
                "global_mmlu_full_de_virology": 0,
                "global_mmlu_full_de_world_religions": 0
              },
              "higher_is_better": {
                "global_mmlu_full_de": {
                  "acc": true
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "acc": true
                },
                "global_mmlu_full_de_anatomy": {
                  "acc": true
                },
                "global_mmlu_full_de_astronomy": {
                  "acc": true
                },
                "global_mmlu_full_de_business_ethics": {
                  "acc": true
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "acc": true
                },
                "global_mmlu_full_de_college_biology": {
                  "acc": true
                },
                "global_mmlu_full_de_college_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_de_college_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_de_college_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_college_medicine": {
                  "acc": true
                },
                "global_mmlu_full_de_college_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_computer_security": {
                  "acc": true
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_econometrics": {
                  "acc": true
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "acc": true
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_formal_logic": {
                  "acc": true
                },
                "global_mmlu_full_de_global_facts": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_biology": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_geography": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "acc": true
                },
                "global_mmlu_full_de_human_aging": {
                  "acc": true
                },
                "global_mmlu_full_de_human_sexuality": {
                  "acc": true
                },
                "global_mmlu_full_de_humanities": {
                  "acc": true
                },
                "global_mmlu_full_de_international_law": {
                  "acc": true
                },
                "global_mmlu_full_de_jurisprudence": {
                  "acc": true
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "acc": true
                },
                "global_mmlu_full_de_machine_learning": {
                  "acc": true
                },
                "global_mmlu_full_de_management": {
                  "acc": true
                },
                "global_mmlu_full_de_marketing": {
                  "acc": true
                },
                "global_mmlu_full_de_medical_genetics": {
                  "acc": true
                },
                "global_mmlu_full_de_miscellaneous": {
                  "acc": true
                },
                "global_mmlu_full_de_moral_disputes": {
                  "acc": true
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "acc": true
                },
                "global_mmlu_full_de_nutrition": {
                  "acc": true
                },
                "global_mmlu_full_de_other": {
                  "acc": true
                },
                "global_mmlu_full_de_philosophy": {
                  "acc": true
                },
                "global_mmlu_full_de_prehistory": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_accounting": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_law": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_medicine": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_psychology": {
                  "acc": true
                },
                "global_mmlu_full_de_public_relations": {
                  "acc": true
                },
                "global_mmlu_full_de_security_studies": {
                  "acc": true
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc": true
                },
                "global_mmlu_full_de_sociology": {
                  "acc": true
                },
                "global_mmlu_full_de_stem": {
                  "acc": true
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "acc": true
                },
                "global_mmlu_full_de_virology": {
                  "acc": true
                },
                "global_mmlu_full_de_world_religions": {
                  "acc": true
                }
              },
              "n-samples": {
                "global_mmlu_full_de_computer_security": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_college_biology": {
                  "original": 144,
                  "effective": 144
                },
                "global_mmlu_full_de_college_mathematics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "original": 235,
                  "effective": 235
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "original": 378,
                  "effective": 378
                },
                "global_mmlu_full_de_astronomy": {
                  "original": 152,
                  "effective": 152
                },
                "global_mmlu_full_de_high_school_physics": {
                  "original": 151,
                  "effective": 151
                },
                "global_mmlu_full_de_machine_learning": {
                  "original": 112,
                  "effective": 112
                },
                "global_mmlu_full_de_college_chemistry": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "original": 203,
                  "effective": 203
                },
                "global_mmlu_full_de_college_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "original": 270,
                  "effective": 270
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_biology": {
                  "original": 310,
                  "effective": 310
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "original": 145,
                  "effective": 145
                },
                "global_mmlu_full_de_college_physics": {
                  "original": 102,
                  "effective": 102
                },
                "global_mmlu_full_de_anatomy": {
                  "original": 135,
                  "effective": 135
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "original": 216,
                  "effective": 216
                },
                "global_mmlu_full_de_nutrition": {
                  "original": 306,
                  "effective": 306
                },
                "global_mmlu_full_de_professional_medicine": {
                  "original": 272,
                  "effective": 272
                },
                "global_mmlu_full_de_college_medicine": {
                  "original": 173,
                  "effective": 173
                },
                "global_mmlu_full_de_professional_accounting": {
                  "original": 282,
                  "effective": 282
                },
                "global_mmlu_full_de_marketing": {
                  "original": 234,
                  "effective": 234
                },
                "global_mmlu_full_de_miscellaneous": {
                  "original": 783,
                  "effective": 783
                },
                "global_mmlu_full_de_business_ethics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_virology": {
                  "original": 166,
                  "effective": 166
                },
                "global_mmlu_full_de_medical_genetics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_human_aging": {
                  "original": 223,
                  "effective": 223
                },
                "global_mmlu_full_de_global_facts": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "original": 265,
                  "effective": 265
                },
                "global_mmlu_full_de_management": {
                  "original": 103,
                  "effective": 103
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "original": 545,
                  "effective": 545
                },
                "global_mmlu_full_de_professional_psychology": {
                  "original": 612,
                  "effective": 612
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "original": 193,
                  "effective": 193
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "original": 390,
                  "effective": 390
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "original": 238,
                  "effective": 238
                },
                "global_mmlu_full_de_econometrics": {
                  "original": 114,
                  "effective": 114
                },
                "global_mmlu_full_de_high_school_geography": {
                  "original": 198,
                  "effective": 198
                },
                "global_mmlu_full_de_security_studies": {
                  "original": 245,
                  "effective": 245
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_public_relations": {
                  "original": 110,
                  "effective": 110
                },
                "global_mmlu_full_de_human_sexuality": {
                  "original": 131,
                  "effective": 131
                },
                "global_mmlu_full_de_sociology": {
                  "original": 201,
                  "effective": 201
                },
                "global_mmlu_full_de_professional_law": {
                  "original": 1534,
                  "effective": 1534
                },
                "global_mmlu_full_de_international_law": {
                  "original": 121,
                  "effective": 121
                },
                "global_mmlu_full_de_prehistory": {
                  "original": 324,
                  "effective": 324
                },
                "global_mmlu_full_de_world_religions": {
                  "original": 171,
                  "effective": 171
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "original": 237,
                  "effective": 237
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "original": 204,
                  "effective": 204
                },
                "global_mmlu_full_de_jurisprudence": {
                  "original": 108,
                  "effective": 108
                },
                "global_mmlu_full_de_formal_logic": {
                  "original": 126,
                  "effective": 126
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "original": 163,
                  "effective": 163
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "original": 165,
                  "effective": 165
                },
                "global_mmlu_full_de_philosophy": {
                  "original": 311,
                  "effective": 311
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "original": 895,
                  "effective": 895
                },
                "global_mmlu_full_de_moral_disputes": {
                  "original": 346,
                  "effective": 346
                }
              },
              "config": {
                "model": "hf",
                "model_args": "pretrained=cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model,trust_remote_code=True,local_files_only=True,tokenizer=cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer,trust_remote_code=True",
                "model_num_parameters": 222882048,
                "model_dtype": "torch.float32",
                "model_revision": "main",
                "model_sha": "",
                "batch_size": "auto",
                "batch_sizes": [
                  64
                ],
                "device": "cuda",
                "use_cache": null,
                "limit": null,
                "bootstrap_iters": 100000,
                "gen_kwargs": null,
                "random_seed": 42,
                "numpy_seed": 42,
                "torch_seed": 42,
                "fewshot_seed": 42
              },
              "git_hash": "a4103c7",
              "date": 1758647686.317813,
              "pretty_env_info": "PyTorch version: 2.7.1+cu126\nIs debug build: False\nCUDA used to build PyTorch: 12.6\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.2 LTS (x86_64)\nGCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\nClang version: Could not collect\nCMake version: version 3.24.1\nLibc version: glibc-2.35\n\nPython version: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] (64-bit runtime)\nPython platform: Linux-6.8.0-1036-nvidia-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H200\nNvidia driver version: 580.82.07\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               224\nOn-line CPU(s) list:                  0-223\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480C\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   56\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          3800.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            5.3 MiB (112 instances)\nL1i cache:                            3.5 MiB (112 instances)\nL2 cache:                             224 MiB (112 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-55,112-167\nNUMA node1 CPU(s):                    56-111,168-223\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pytorch-lightning==2.5.2\n[pip3] torch==2.7.1\n[pip3] torchmetrics==1.8.0\n[pip3] triton==3.3.1\n[conda] Could not collect",
              "transformers_version": "4.54.1",
              "lm_eval_version": "0.4.9",
              "upper_git_hash": null,
              "tokenizer_pad_token": [
                "<pad>",
                "0"
              ],
              "tokenizer_eos_token": [
                "</s>",
                "1"
              ],
              "tokenizer_bos_token": [
                null,
                "None"
              ],
              "eot_token_id": 1,
              "max_length": 512,
              "task_hashes": {},
              "model_source": "hf",
              "model_name": "cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model",
              "model_name_sanitized": "cross_lingual_transfer__models__german_T5_Optimized_50Olap_clean_restart_487k__model",
              "system_instruction": null,
              "system_instruction_sha": null,
              "fewshot_as_multiturn": false,
              "chat_template": null,
              "chat_template_sha": null,
              "start_time": 633829.719953824,
              "end_time": 634463.807388305,
              "total_evaluation_time_seconds": "634.0874344810145"
            },
            "duration_seconds": 680.2713005542755,
            "duration_minutes": 11.337855009237925,
            "command": "python -m lm_eval --model hf --model_args pretrained=cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/model,trust_remote_code=True,local_files_only=True,tokenizer=cross_lingual_transfer/models/german_T5_Optimized_50Olap_clean_restart_487k/tokenizer --tasks global_mmlu_full_de --device cuda --batch_size auto --output_path /netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/wechsel-transfer-en-487k_global_mmlu_de_0shot_20250923_191313 --num_fewshot 0 --trust_remote_code --seed 42 --wandb_args project=BA-T5-CrossLingual,entity=nikolas-rauscher-dfki,group=crosslingual_transfer_eval_full_15k-final,name=wechsel-transfer-en-487k_global_mmlu_de_0shot",
            "output_dir": "/netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/wechsel-transfer-en-487k_global_mmlu_de_0shot_20250923_191313",
            "status": "success"
          }
        }
      }
    },
    "hf-germanT5-continued-pretraind-on-german-15k": {
      "model_config": {
        "source_path": "cross_lingual_transfer/logs/native_baseline/train/runs/2025-09-18_23-54-00/checkpoints/steps/step-step=015000.ckpt",
        "name": "hf-germanT5-continued-pretraind-on-german-15k"
      },
      "model_path": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
      "metadata": {
        "training_steps": 15000,
        "run_date": "2025-09-18",
        "run_time": "23:54:00",
        "run_directory": "2025-09-18_23-54-00",
        "learning_rate": null,
        "batch_size": null,
        "model_name": null,
        "actual_global_step": 15000,
        "epoch": 0,
        "lightning_version": "2.5.1.post0",
        "source_type": "checkpoint",
        "original_checkpoint": "/netscratch/nrauscher/projects/BA-hydra/cross_lingual_transfer/logs/native_baseline/train/runs/2025-09-18_23-54-00/checkpoints/steps/step-step=015000.ckpt"
      },
      "benchmarks": {
        "global_mmlu_en": {
          "0_shot": {
            "parse_status": "partial",
            "error": "Could not find primary metric",
            "raw_results": {
              "results": {
                "global_mmlu_full_en": {
                  "acc,none": 0.22945449366187154,
                  "acc_stderr,none": 0.0035426913692823605,
                  "alias": "global_mmlu_full_en"
                },
                "global_mmlu_full_en_humanities": {
                  "acc,none": 0.24208289054197663,
                  "acc_stderr,none": 0.0062426684031394305,
                  "alias": " - global_mmlu_full_en_humanities"
                },
                "global_mmlu_full_en_formal_logic": {
                  "alias": "  - global_mmlu_full_en_formal_logic",
                  "acc,none": 0.2857142857142857,
                  "acc_stderr,none": 0.04040610178208841
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "alias": "  - global_mmlu_full_en_high_school_european_history",
                  "acc,none": 0.21818181818181817,
                  "acc_stderr,none": 0.03225078108306289
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "alias": "  - global_mmlu_full_en_high_school_us_history",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.03039153369274154
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "alias": "  - global_mmlu_full_en_high_school_world_history",
                  "acc,none": 0.270042194092827,
                  "acc_stderr,none": 0.028900721906293426
                },
                "global_mmlu_full_en_international_law": {
                  "alias": "  - global_mmlu_full_en_international_law",
                  "acc,none": 0.2396694214876033,
                  "acc_stderr,none": 0.03896878985070417
                },
                "global_mmlu_full_en_jurisprudence": {
                  "alias": "  - global_mmlu_full_en_jurisprudence",
                  "acc,none": 0.25925925925925924,
                  "acc_stderr,none": 0.04236511258094634
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "alias": "  - global_mmlu_full_en_logical_fallacies",
                  "acc,none": 0.22085889570552147,
                  "acc_stderr,none": 0.032591773927421776
                },
                "global_mmlu_full_en_moral_disputes": {
                  "alias": "  - global_mmlu_full_en_moral_disputes",
                  "acc,none": 0.24855491329479767,
                  "acc_stderr,none": 0.023267528432100174
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "alias": "  - global_mmlu_full_en_moral_scenarios",
                  "acc,none": 0.23798882681564246,
                  "acc_stderr,none": 0.014242630070574885
                },
                "global_mmlu_full_en_philosophy": {
                  "alias": "  - global_mmlu_full_en_philosophy",
                  "acc,none": 0.1864951768488746,
                  "acc_stderr,none": 0.02212243977248077
                },
                "global_mmlu_full_en_prehistory": {
                  "alias": "  - global_mmlu_full_en_prehistory",
                  "acc,none": 0.21604938271604937,
                  "acc_stderr,none": 0.022899162918445813
                },
                "global_mmlu_full_en_professional_law": {
                  "alias": "  - global_mmlu_full_en_professional_law",
                  "acc,none": 0.2457627118644068,
                  "acc_stderr,none": 0.01099615663514269
                },
                "global_mmlu_full_en_world_religions": {
                  "alias": "  - global_mmlu_full_en_world_religions",
                  "acc,none": 0.3216374269005848,
                  "acc_stderr,none": 0.03582529442573122
                },
                "global_mmlu_full_en_other": {
                  "acc,none": 0.23978113936272932,
                  "acc_stderr,none": 0.00764225029165751,
                  "alias": " - global_mmlu_full_en_other"
                },
                "global_mmlu_full_en_business_ethics": {
                  "alias": "  - global_mmlu_full_en_business_ethics",
                  "acc,none": 0.3,
                  "acc_stderr,none": 0.046056618647183814
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "alias": "  - global_mmlu_full_en_clinical_knowledge",
                  "acc,none": 0.21509433962264152,
                  "acc_stderr,none": 0.025288394502891377
                },
                "global_mmlu_full_en_college_medicine": {
                  "alias": "  - global_mmlu_full_en_college_medicine",
                  "acc,none": 0.20809248554913296,
                  "acc_stderr,none": 0.030952890217749884
                },
                "global_mmlu_full_en_global_facts": {
                  "alias": "  - global_mmlu_full_en_global_facts",
                  "acc,none": 0.18,
                  "acc_stderr,none": 0.038612291966536955
                },
                "global_mmlu_full_en_human_aging": {
                  "alias": "  - global_mmlu_full_en_human_aging",
                  "acc,none": 0.31390134529147984,
                  "acc_stderr,none": 0.03114679648297246
                },
                "global_mmlu_full_en_management": {
                  "alias": "  - global_mmlu_full_en_management",
                  "acc,none": 0.17475728155339806,
                  "acc_stderr,none": 0.03760178006026621
                },
                "global_mmlu_full_en_marketing": {
                  "alias": "  - global_mmlu_full_en_marketing",
                  "acc,none": 0.2905982905982906,
                  "acc_stderr,none": 0.029745048572674057
                },
                "global_mmlu_full_en_medical_genetics": {
                  "alias": "  - global_mmlu_full_en_medical_genetics",
                  "acc,none": 0.3,
                  "acc_stderr,none": 0.046056618647183814
                },
                "global_mmlu_full_en_miscellaneous": {
                  "alias": "  - global_mmlu_full_en_miscellaneous",
                  "acc,none": 0.23754789272030652,
                  "acc_stderr,none": 0.015218733046150195
                },
                "global_mmlu_full_en_nutrition": {
                  "alias": "  - global_mmlu_full_en_nutrition",
                  "acc,none": 0.22549019607843138,
                  "acc_stderr,none": 0.023929155517351284
                },
                "global_mmlu_full_en_professional_accounting": {
                  "alias": "  - global_mmlu_full_en_professional_accounting",
                  "acc,none": 0.23404255319148937,
                  "acc_stderr,none": 0.025257861359432407
                },
                "global_mmlu_full_en_professional_medicine": {
                  "alias": "  - global_mmlu_full_en_professional_medicine",
                  "acc,none": 0.18382352941176472,
                  "acc_stderr,none": 0.02352924218519311
                },
                "global_mmlu_full_en_virology": {
                  "alias": "  - global_mmlu_full_en_virology",
                  "acc,none": 0.28313253012048195,
                  "acc_stderr,none": 0.03507295431370518
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc,none": 0.2170945726356841,
                  "acc_stderr,none": 0.007428786285788534,
                  "alias": " - global_mmlu_full_en_social_sciences"
                },
                "global_mmlu_full_en_econometrics": {
                  "alias": "  - global_mmlu_full_en_econometrics",
                  "acc,none": 0.23684210526315788,
                  "acc_stderr,none": 0.039994238792813386
                },
                "global_mmlu_full_en_high_school_geography": {
                  "alias": "  - global_mmlu_full_en_high_school_geography",
                  "acc,none": 0.17676767676767677,
                  "acc_stderr,none": 0.027178752639044915
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "alias": "  - global_mmlu_full_en_high_school_government_and_politics",
                  "acc,none": 0.19689119170984457,
                  "acc_stderr,none": 0.02869787397186069
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "alias": "  - global_mmlu_full_en_high_school_macroeconomics",
                  "acc,none": 0.20256410256410257,
                  "acc_stderr,none": 0.020377660970371397
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "alias": "  - global_mmlu_full_en_high_school_microeconomics",
                  "acc,none": 0.21008403361344538,
                  "acc_stderr,none": 0.026461398717471874
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "alias": "  - global_mmlu_full_en_high_school_psychology",
                  "acc,none": 0.1926605504587156,
                  "acc_stderr,none": 0.016909276884936073
                },
                "global_mmlu_full_en_human_sexuality": {
                  "alias": "  - global_mmlu_full_en_human_sexuality",
                  "acc,none": 0.2595419847328244,
                  "acc_stderr,none": 0.03844876139785271
                },
                "global_mmlu_full_en_professional_psychology": {
                  "alias": "  - global_mmlu_full_en_professional_psychology",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.01751781884501444
                },
                "global_mmlu_full_en_public_relations": {
                  "alias": "  - global_mmlu_full_en_public_relations",
                  "acc,none": 0.21818181818181817,
                  "acc_stderr,none": 0.03955932861795833
                },
                "global_mmlu_full_en_security_studies": {
                  "alias": "  - global_mmlu_full_en_security_studies",
                  "acc,none": 0.18775510204081633,
                  "acc_stderr,none": 0.02500025603954622
                },
                "global_mmlu_full_en_sociology": {
                  "alias": "  - global_mmlu_full_en_sociology",
                  "acc,none": 0.24378109452736318,
                  "acc_stderr,none": 0.030360490154014652
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "alias": "  - global_mmlu_full_en_us_foreign_policy",
                  "acc,none": 0.28,
                  "acc_stderr,none": 0.045126085985421276
                },
                "global_mmlu_full_en_stem": {
                  "acc,none": 0.21249603552172533,
                  "acc_stderr,none": 0.007271218700485502,
                  "alias": " - global_mmlu_full_en_stem"
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "alias": "  - global_mmlu_full_en_abstract_algebra",
                  "acc,none": 0.22,
                  "acc_stderr,none": 0.04163331998932269
                },
                "global_mmlu_full_en_anatomy": {
                  "alias": "  - global_mmlu_full_en_anatomy",
                  "acc,none": 0.18518518518518517,
                  "acc_stderr,none": 0.03355677216313142
                },
                "global_mmlu_full_en_astronomy": {
                  "alias": "  - global_mmlu_full_en_astronomy",
                  "acc,none": 0.17763157894736842,
                  "acc_stderr,none": 0.031103182383123398
                },
                "global_mmlu_full_en_college_biology": {
                  "alias": "  - global_mmlu_full_en_college_biology",
                  "acc,none": 0.2569444444444444,
                  "acc_stderr,none": 0.03653946969442099
                },
                "global_mmlu_full_en_college_chemistry": {
                  "alias": "  - global_mmlu_full_en_college_chemistry",
                  "acc,none": 0.2,
                  "acc_stderr,none": 0.040201512610368445
                },
                "global_mmlu_full_en_college_computer_science": {
                  "alias": "  - global_mmlu_full_en_college_computer_science",
                  "acc,none": 0.26,
                  "acc_stderr,none": 0.044084400227680794
                },
                "global_mmlu_full_en_college_mathematics": {
                  "alias": "  - global_mmlu_full_en_college_mathematics",
                  "acc,none": 0.21,
                  "acc_stderr,none": 0.040936018074033256
                },
                "global_mmlu_full_en_college_physics": {
                  "alias": "  - global_mmlu_full_en_college_physics",
                  "acc,none": 0.21568627450980393,
                  "acc_stderr,none": 0.040925639582376556
                },
                "global_mmlu_full_en_computer_security": {
                  "alias": "  - global_mmlu_full_en_computer_security",
                  "acc,none": 0.28,
                  "acc_stderr,none": 0.045126085985421276
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "alias": "  - global_mmlu_full_en_conceptual_physics",
                  "acc,none": 0.26382978723404255,
                  "acc_stderr,none": 0.02880998985410298
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "alias": "  - global_mmlu_full_en_electrical_engineering",
                  "acc,none": 0.2413793103448276,
                  "acc_stderr,none": 0.03565998174135302
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "alias": "  - global_mmlu_full_en_elementary_mathematics",
                  "acc,none": 0.20899470899470898,
                  "acc_stderr,none": 0.020940481565334835
                },
                "global_mmlu_full_en_high_school_biology": {
                  "alias": "  - global_mmlu_full_en_high_school_biology",
                  "acc,none": 0.1774193548387097,
                  "acc_stderr,none": 0.021732540689329265
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "alias": "  - global_mmlu_full_en_high_school_chemistry",
                  "acc,none": 0.15270935960591134,
                  "acc_stderr,none": 0.025308904539380624
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "alias": "  - global_mmlu_full_en_high_school_computer_science",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.04351941398892446
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "alias": "  - global_mmlu_full_en_high_school_mathematics",
                  "acc,none": 0.2111111111111111,
                  "acc_stderr,none": 0.02488211685765508
                },
                "global_mmlu_full_en_high_school_physics": {
                  "alias": "  - global_mmlu_full_en_high_school_physics",
                  "acc,none": 0.1986754966887417,
                  "acc_stderr,none": 0.032578473844367746
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "alias": "  - global_mmlu_full_en_high_school_statistics",
                  "acc,none": 0.1527777777777778,
                  "acc_stderr,none": 0.02453632602613422
                },
                "global_mmlu_full_en_machine_learning": {
                  "alias": "  - global_mmlu_full_en_machine_learning",
                  "acc,none": 0.3125,
                  "acc_stderr,none": 0.043994650575715215
                }
              },
              "groups": {
                "global_mmlu_full_en": {
                  "acc,none": 0.22945449366187154,
                  "acc_stderr,none": 0.0035426913692823605,
                  "alias": "global_mmlu_full_en"
                },
                "global_mmlu_full_en_humanities": {
                  "acc,none": 0.24208289054197663,
                  "acc_stderr,none": 0.0062426684031394305,
                  "alias": " - global_mmlu_full_en_humanities"
                },
                "global_mmlu_full_en_other": {
                  "acc,none": 0.23978113936272932,
                  "acc_stderr,none": 0.00764225029165751,
                  "alias": " - global_mmlu_full_en_other"
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc,none": 0.2170945726356841,
                  "acc_stderr,none": 0.007428786285788534,
                  "alias": " - global_mmlu_full_en_social_sciences"
                },
                "global_mmlu_full_en_stem": {
                  "acc,none": 0.21249603552172533,
                  "acc_stderr,none": 0.007271218700485502,
                  "alias": " - global_mmlu_full_en_stem"
                }
              },
              "group_subtasks": {
                "global_mmlu_full_en_humanities": [
                  "global_mmlu_full_en_world_religions",
                  "global_mmlu_full_en_international_law",
                  "global_mmlu_full_en_moral_scenarios",
                  "global_mmlu_full_en_high_school_us_history",
                  "global_mmlu_full_en_jurisprudence",
                  "global_mmlu_full_en_high_school_world_history",
                  "global_mmlu_full_en_logical_fallacies",
                  "global_mmlu_full_en_philosophy",
                  "global_mmlu_full_en_professional_law",
                  "global_mmlu_full_en_high_school_european_history",
                  "global_mmlu_full_en_formal_logic",
                  "global_mmlu_full_en_moral_disputes",
                  "global_mmlu_full_en_prehistory"
                ],
                "global_mmlu_full_en_social_sciences": [
                  "global_mmlu_full_en_us_foreign_policy",
                  "global_mmlu_full_en_public_relations",
                  "global_mmlu_full_en_econometrics",
                  "global_mmlu_full_en_high_school_microeconomics",
                  "global_mmlu_full_en_human_sexuality",
                  "global_mmlu_full_en_security_studies",
                  "global_mmlu_full_en_professional_psychology",
                  "global_mmlu_full_en_high_school_geography",
                  "global_mmlu_full_en_high_school_government_and_politics",
                  "global_mmlu_full_en_high_school_psychology",
                  "global_mmlu_full_en_high_school_macroeconomics",
                  "global_mmlu_full_en_sociology"
                ],
                "global_mmlu_full_en_other": [
                  "global_mmlu_full_en_clinical_knowledge",
                  "global_mmlu_full_en_medical_genetics",
                  "global_mmlu_full_en_professional_medicine",
                  "global_mmlu_full_en_business_ethics",
                  "global_mmlu_full_en_marketing",
                  "global_mmlu_full_en_global_facts",
                  "global_mmlu_full_en_college_medicine",
                  "global_mmlu_full_en_human_aging",
                  "global_mmlu_full_en_management",
                  "global_mmlu_full_en_nutrition",
                  "global_mmlu_full_en_virology",
                  "global_mmlu_full_en_professional_accounting",
                  "global_mmlu_full_en_miscellaneous"
                ],
                "global_mmlu_full_en_stem": [
                  "global_mmlu_full_en_high_school_statistics",
                  "global_mmlu_full_en_computer_security",
                  "global_mmlu_full_en_machine_learning",
                  "global_mmlu_full_en_abstract_algebra",
                  "global_mmlu_full_en_astronomy",
                  "global_mmlu_full_en_college_computer_science",
                  "global_mmlu_full_en_high_school_biology",
                  "global_mmlu_full_en_high_school_computer_science",
                  "global_mmlu_full_en_high_school_chemistry",
                  "global_mmlu_full_en_elementary_mathematics",
                  "global_mmlu_full_en_college_physics",
                  "global_mmlu_full_en_high_school_physics",
                  "global_mmlu_full_en_college_chemistry",
                  "global_mmlu_full_en_college_biology",
                  "global_mmlu_full_en_college_mathematics",
                  "global_mmlu_full_en_conceptual_physics",
                  "global_mmlu_full_en_anatomy",
                  "global_mmlu_full_en_high_school_mathematics",
                  "global_mmlu_full_en_electrical_engineering"
                ],
                "global_mmlu_full_en": [
                  "global_mmlu_full_en_stem",
                  "global_mmlu_full_en_other",
                  "global_mmlu_full_en_social_sciences",
                  "global_mmlu_full_en_humanities"
                ]
              },
              "configs": {
                "global_mmlu_full_en_abstract_algebra": {
                  "task": "global_mmlu_full_en_abstract_algebra",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a2cb0>, subject='abstract_algebra')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_anatomy": {
                  "task": "global_mmlu_full_en_anatomy",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a0f70>, subject='anatomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_astronomy": {
                  "task": "global_mmlu_full_en_astronomy",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a05e0>, subject='astronomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_business_ethics": {
                  "task": "global_mmlu_full_en_business_ethics",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b7490>, subject='business_ethics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "task": "global_mmlu_full_en_clinical_knowledge",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a08b0>, subject='clinical_knowledge')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_biology": {
                  "task": "global_mmlu_full_en_college_biology",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a3010>, subject='college_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_chemistry": {
                  "task": "global_mmlu_full_en_college_chemistry",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a11b0>, subject='college_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_computer_science": {
                  "task": "global_mmlu_full_en_college_computer_science",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a1630>, subject='college_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_mathematics": {
                  "task": "global_mmlu_full_en_college_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a01f0>, subject='college_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_medicine": {
                  "task": "global_mmlu_full_en_college_medicine",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b7520>, subject='college_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_physics": {
                  "task": "global_mmlu_full_en_college_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a12d0>, subject='college_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_computer_security": {
                  "task": "global_mmlu_full_en_computer_security",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a1d80>, subject='computer_security')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "task": "global_mmlu_full_en_conceptual_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a04c0>, subject='conceptual_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_econometrics": {
                  "task": "global_mmlu_full_en_econometrics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b5cf0>, subject='econometrics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "task": "global_mmlu_full_en_electrical_engineering",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a1480>, subject='electrical_engineering')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "task": "global_mmlu_full_en_elementary_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a32e0>, subject='elementary_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_formal_logic": {
                  "task": "global_mmlu_full_en_formal_logic",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796dc5b5d990>, subject='formal_logic')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_global_facts": {
                  "task": "global_mmlu_full_en_global_facts",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b6b90>, subject='global_facts')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_biology": {
                  "task": "global_mmlu_full_en_high_school_biology",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a2d40>, subject='high_school_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "task": "global_mmlu_full_en_high_school_chemistry",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a2c20>, subject='high_school_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "task": "global_mmlu_full_en_high_school_computer_science",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a2b00>, subject='high_school_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "task": "global_mmlu_full_en_high_school_european_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796dc022c4c0>, subject='high_school_european_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_geography": {
                  "task": "global_mmlu_full_en_high_school_geography",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796dc022c430>, subject='high_school_geography')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "task": "global_mmlu_full_en_high_school_government_and_politics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b5fc0>, subject='high_school_government_and_politics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "task": "global_mmlu_full_en_high_school_macroeconomics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b5120>, subject='high_school_macroeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "task": "global_mmlu_full_en_high_school_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a1ea0>, subject='high_school_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "task": "global_mmlu_full_en_high_school_microeconomics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b6200>, subject='high_school_microeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_physics": {
                  "task": "global_mmlu_full_en_high_school_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a1120>, subject='high_school_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "task": "global_mmlu_full_en_high_school_psychology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b4670>, subject='high_school_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "task": "global_mmlu_full_en_high_school_statistics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a2e60>, subject='high_school_statistics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "task": "global_mmlu_full_en_high_school_us_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796dc022de10>, subject='high_school_us_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "task": "global_mmlu_full_en_high_school_world_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796dc022fac0>, subject='high_school_world_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_human_aging": {
                  "task": "global_mmlu_full_en_human_aging",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b6e60>, subject='human_aging')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_human_sexuality": {
                  "task": "global_mmlu_full_en_human_sexuality",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b5e10>, subject='human_sexuality')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_international_law": {
                  "task": "global_mmlu_full_en_international_law",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796dc022d870>, subject='international_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_jurisprudence": {
                  "task": "global_mmlu_full_en_jurisprudence",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796dc022d360>, subject='jurisprudence')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "task": "global_mmlu_full_en_logical_fallacies",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796dc022eef0>, subject='logical_fallacies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_machine_learning": {
                  "task": "global_mmlu_full_en_machine_learning",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a0280>, subject='machine_learning')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_management": {
                  "task": "global_mmlu_full_en_management",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b6ef0>, subject='management')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_marketing": {
                  "task": "global_mmlu_full_en_marketing",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b71c0>, subject='marketing')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_medical_genetics": {
                  "task": "global_mmlu_full_en_medical_genetics",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a0940>, subject='medical_genetics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_miscellaneous": {
                  "task": "global_mmlu_full_en_miscellaneous",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b7250>, subject='miscellaneous')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_moral_disputes": {
                  "task": "global_mmlu_full_en_moral_disputes",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796dc022c8b0>, subject='moral_disputes')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "task": "global_mmlu_full_en_moral_scenarios",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796dc022feb0>, subject='moral_scenarios')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_nutrition": {
                  "task": "global_mmlu_full_en_nutrition",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b77f0>, subject='nutrition')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_philosophy": {
                  "task": "global_mmlu_full_en_philosophy",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796dc022d5a0>, subject='philosophy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_prehistory": {
                  "task": "global_mmlu_full_en_prehistory",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db0cd0160>, subject='prehistory')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_professional_accounting": {
                  "task": "global_mmlu_full_en_professional_accounting",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b6c20>, subject='professional_accounting')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_professional_law": {
                  "task": "global_mmlu_full_en_professional_law",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796dc022cca0>, subject='professional_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_professional_medicine": {
                  "task": "global_mmlu_full_en_professional_medicine",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796b6e0a0700>, subject='professional_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_professional_psychology": {
                  "task": "global_mmlu_full_en_professional_psychology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796dc022e440>, subject='professional_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_public_relations": {
                  "task": "global_mmlu_full_en_public_relations",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b7400>, subject='public_relations')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_security_studies": {
                  "task": "global_mmlu_full_en_security_studies",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b4430>, subject='security_studies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_sociology": {
                  "task": "global_mmlu_full_en_sociology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b4ca0>, subject='sociology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "task": "global_mmlu_full_en_us_foreign_policy",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b4f70>, subject='us_foreign_policy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_virology": {
                  "task": "global_mmlu_full_en_virology",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796db01b4ee0>, subject='virology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_world_religions": {
                  "task": "global_mmlu_full_en_world_religions",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x796dc022f490>, subject='world_religions')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                }
              },
              "versions": {
                "global_mmlu_full_en": 0.0,
                "global_mmlu_full_en_abstract_algebra": 0.0,
                "global_mmlu_full_en_anatomy": 0.0,
                "global_mmlu_full_en_astronomy": 0.0,
                "global_mmlu_full_en_business_ethics": 0.0,
                "global_mmlu_full_en_clinical_knowledge": 0.0,
                "global_mmlu_full_en_college_biology": 0.0,
                "global_mmlu_full_en_college_chemistry": 0.0,
                "global_mmlu_full_en_college_computer_science": 0.0,
                "global_mmlu_full_en_college_mathematics": 0.0,
                "global_mmlu_full_en_college_medicine": 0.0,
                "global_mmlu_full_en_college_physics": 0.0,
                "global_mmlu_full_en_computer_security": 0.0,
                "global_mmlu_full_en_conceptual_physics": 0.0,
                "global_mmlu_full_en_econometrics": 0.0,
                "global_mmlu_full_en_electrical_engineering": 0.0,
                "global_mmlu_full_en_elementary_mathematics": 0.0,
                "global_mmlu_full_en_formal_logic": 0.0,
                "global_mmlu_full_en_global_facts": 0.0,
                "global_mmlu_full_en_high_school_biology": 0.0,
                "global_mmlu_full_en_high_school_chemistry": 0.0,
                "global_mmlu_full_en_high_school_computer_science": 0.0,
                "global_mmlu_full_en_high_school_european_history": 0.0,
                "global_mmlu_full_en_high_school_geography": 0.0,
                "global_mmlu_full_en_high_school_government_and_politics": 0.0,
                "global_mmlu_full_en_high_school_macroeconomics": 0.0,
                "global_mmlu_full_en_high_school_mathematics": 0.0,
                "global_mmlu_full_en_high_school_microeconomics": 0.0,
                "global_mmlu_full_en_high_school_physics": 0.0,
                "global_mmlu_full_en_high_school_psychology": 0.0,
                "global_mmlu_full_en_high_school_statistics": 0.0,
                "global_mmlu_full_en_high_school_us_history": 0.0,
                "global_mmlu_full_en_high_school_world_history": 0.0,
                "global_mmlu_full_en_human_aging": 0.0,
                "global_mmlu_full_en_human_sexuality": 0.0,
                "global_mmlu_full_en_humanities": 0.0,
                "global_mmlu_full_en_international_law": 0.0,
                "global_mmlu_full_en_jurisprudence": 0.0,
                "global_mmlu_full_en_logical_fallacies": 0.0,
                "global_mmlu_full_en_machine_learning": 0.0,
                "global_mmlu_full_en_management": 0.0,
                "global_mmlu_full_en_marketing": 0.0,
                "global_mmlu_full_en_medical_genetics": 0.0,
                "global_mmlu_full_en_miscellaneous": 0.0,
                "global_mmlu_full_en_moral_disputes": 0.0,
                "global_mmlu_full_en_moral_scenarios": 0.0,
                "global_mmlu_full_en_nutrition": 0.0,
                "global_mmlu_full_en_other": 0.0,
                "global_mmlu_full_en_philosophy": 0.0,
                "global_mmlu_full_en_prehistory": 0.0,
                "global_mmlu_full_en_professional_accounting": 0.0,
                "global_mmlu_full_en_professional_law": 0.0,
                "global_mmlu_full_en_professional_medicine": 0.0,
                "global_mmlu_full_en_professional_psychology": 0.0,
                "global_mmlu_full_en_public_relations": 0.0,
                "global_mmlu_full_en_security_studies": 0.0,
                "global_mmlu_full_en_social_sciences": 0.0,
                "global_mmlu_full_en_sociology": 0.0,
                "global_mmlu_full_en_stem": 0.0,
                "global_mmlu_full_en_us_foreign_policy": 0.0,
                "global_mmlu_full_en_virology": 0.0,
                "global_mmlu_full_en_world_religions": 0.0
              },
              "n-shot": {
                "global_mmlu_full_en_abstract_algebra": 0,
                "global_mmlu_full_en_anatomy": 0,
                "global_mmlu_full_en_astronomy": 0,
                "global_mmlu_full_en_business_ethics": 0,
                "global_mmlu_full_en_clinical_knowledge": 0,
                "global_mmlu_full_en_college_biology": 0,
                "global_mmlu_full_en_college_chemistry": 0,
                "global_mmlu_full_en_college_computer_science": 0,
                "global_mmlu_full_en_college_mathematics": 0,
                "global_mmlu_full_en_college_medicine": 0,
                "global_mmlu_full_en_college_physics": 0,
                "global_mmlu_full_en_computer_security": 0,
                "global_mmlu_full_en_conceptual_physics": 0,
                "global_mmlu_full_en_econometrics": 0,
                "global_mmlu_full_en_electrical_engineering": 0,
                "global_mmlu_full_en_elementary_mathematics": 0,
                "global_mmlu_full_en_formal_logic": 0,
                "global_mmlu_full_en_global_facts": 0,
                "global_mmlu_full_en_high_school_biology": 0,
                "global_mmlu_full_en_high_school_chemistry": 0,
                "global_mmlu_full_en_high_school_computer_science": 0,
                "global_mmlu_full_en_high_school_european_history": 0,
                "global_mmlu_full_en_high_school_geography": 0,
                "global_mmlu_full_en_high_school_government_and_politics": 0,
                "global_mmlu_full_en_high_school_macroeconomics": 0,
                "global_mmlu_full_en_high_school_mathematics": 0,
                "global_mmlu_full_en_high_school_microeconomics": 0,
                "global_mmlu_full_en_high_school_physics": 0,
                "global_mmlu_full_en_high_school_psychology": 0,
                "global_mmlu_full_en_high_school_statistics": 0,
                "global_mmlu_full_en_high_school_us_history": 0,
                "global_mmlu_full_en_high_school_world_history": 0,
                "global_mmlu_full_en_human_aging": 0,
                "global_mmlu_full_en_human_sexuality": 0,
                "global_mmlu_full_en_international_law": 0,
                "global_mmlu_full_en_jurisprudence": 0,
                "global_mmlu_full_en_logical_fallacies": 0,
                "global_mmlu_full_en_machine_learning": 0,
                "global_mmlu_full_en_management": 0,
                "global_mmlu_full_en_marketing": 0,
                "global_mmlu_full_en_medical_genetics": 0,
                "global_mmlu_full_en_miscellaneous": 0,
                "global_mmlu_full_en_moral_disputes": 0,
                "global_mmlu_full_en_moral_scenarios": 0,
                "global_mmlu_full_en_nutrition": 0,
                "global_mmlu_full_en_philosophy": 0,
                "global_mmlu_full_en_prehistory": 0,
                "global_mmlu_full_en_professional_accounting": 0,
                "global_mmlu_full_en_professional_law": 0,
                "global_mmlu_full_en_professional_medicine": 0,
                "global_mmlu_full_en_professional_psychology": 0,
                "global_mmlu_full_en_public_relations": 0,
                "global_mmlu_full_en_security_studies": 0,
                "global_mmlu_full_en_sociology": 0,
                "global_mmlu_full_en_us_foreign_policy": 0,
                "global_mmlu_full_en_virology": 0,
                "global_mmlu_full_en_world_religions": 0
              },
              "higher_is_better": {
                "global_mmlu_full_en": {
                  "acc": true
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "acc": true
                },
                "global_mmlu_full_en_anatomy": {
                  "acc": true
                },
                "global_mmlu_full_en_astronomy": {
                  "acc": true
                },
                "global_mmlu_full_en_business_ethics": {
                  "acc": true
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "acc": true
                },
                "global_mmlu_full_en_college_biology": {
                  "acc": true
                },
                "global_mmlu_full_en_college_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_en_college_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_en_college_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_college_medicine": {
                  "acc": true
                },
                "global_mmlu_full_en_college_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_computer_security": {
                  "acc": true
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_econometrics": {
                  "acc": true
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "acc": true
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_formal_logic": {
                  "acc": true
                },
                "global_mmlu_full_en_global_facts": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_biology": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_geography": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "acc": true
                },
                "global_mmlu_full_en_human_aging": {
                  "acc": true
                },
                "global_mmlu_full_en_human_sexuality": {
                  "acc": true
                },
                "global_mmlu_full_en_humanities": {
                  "acc": true
                },
                "global_mmlu_full_en_international_law": {
                  "acc": true
                },
                "global_mmlu_full_en_jurisprudence": {
                  "acc": true
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "acc": true
                },
                "global_mmlu_full_en_machine_learning": {
                  "acc": true
                },
                "global_mmlu_full_en_management": {
                  "acc": true
                },
                "global_mmlu_full_en_marketing": {
                  "acc": true
                },
                "global_mmlu_full_en_medical_genetics": {
                  "acc": true
                },
                "global_mmlu_full_en_miscellaneous": {
                  "acc": true
                },
                "global_mmlu_full_en_moral_disputes": {
                  "acc": true
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "acc": true
                },
                "global_mmlu_full_en_nutrition": {
                  "acc": true
                },
                "global_mmlu_full_en_other": {
                  "acc": true
                },
                "global_mmlu_full_en_philosophy": {
                  "acc": true
                },
                "global_mmlu_full_en_prehistory": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_accounting": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_law": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_medicine": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_psychology": {
                  "acc": true
                },
                "global_mmlu_full_en_public_relations": {
                  "acc": true
                },
                "global_mmlu_full_en_security_studies": {
                  "acc": true
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc": true
                },
                "global_mmlu_full_en_sociology": {
                  "acc": true
                },
                "global_mmlu_full_en_stem": {
                  "acc": true
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "acc": true
                },
                "global_mmlu_full_en_virology": {
                  "acc": true
                },
                "global_mmlu_full_en_world_religions": {
                  "acc": true
                }
              },
              "n-samples": {
                "global_mmlu_full_en_high_school_statistics": {
                  "original": 216,
                  "effective": 216
                },
                "global_mmlu_full_en_computer_security": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_machine_learning": {
                  "original": 112,
                  "effective": 112
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_astronomy": {
                  "original": 152,
                  "effective": 152
                },
                "global_mmlu_full_en_college_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_high_school_biology": {
                  "original": 310,
                  "effective": 310
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "original": 203,
                  "effective": 203
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "original": 378,
                  "effective": 378
                },
                "global_mmlu_full_en_college_physics": {
                  "original": 102,
                  "effective": 102
                },
                "global_mmlu_full_en_high_school_physics": {
                  "original": 151,
                  "effective": 151
                },
                "global_mmlu_full_en_college_chemistry": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_college_biology": {
                  "original": 144,
                  "effective": 144
                },
                "global_mmlu_full_en_college_mathematics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "original": 235,
                  "effective": 235
                },
                "global_mmlu_full_en_anatomy": {
                  "original": 135,
                  "effective": 135
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "original": 270,
                  "effective": 270
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "original": 145,
                  "effective": 145
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "original": 265,
                  "effective": 265
                },
                "global_mmlu_full_en_medical_genetics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_professional_medicine": {
                  "original": 272,
                  "effective": 272
                },
                "global_mmlu_full_en_business_ethics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_marketing": {
                  "original": 234,
                  "effective": 234
                },
                "global_mmlu_full_en_global_facts": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_college_medicine": {
                  "original": 173,
                  "effective": 173
                },
                "global_mmlu_full_en_human_aging": {
                  "original": 223,
                  "effective": 223
                },
                "global_mmlu_full_en_management": {
                  "original": 103,
                  "effective": 103
                },
                "global_mmlu_full_en_nutrition": {
                  "original": 306,
                  "effective": 306
                },
                "global_mmlu_full_en_virology": {
                  "original": 166,
                  "effective": 166
                },
                "global_mmlu_full_en_professional_accounting": {
                  "original": 282,
                  "effective": 282
                },
                "global_mmlu_full_en_miscellaneous": {
                  "original": 783,
                  "effective": 783
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_public_relations": {
                  "original": 110,
                  "effective": 110
                },
                "global_mmlu_full_en_econometrics": {
                  "original": 114,
                  "effective": 114
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "original": 238,
                  "effective": 238
                },
                "global_mmlu_full_en_human_sexuality": {
                  "original": 131,
                  "effective": 131
                },
                "global_mmlu_full_en_security_studies": {
                  "original": 245,
                  "effective": 245
                },
                "global_mmlu_full_en_professional_psychology": {
                  "original": 612,
                  "effective": 612
                },
                "global_mmlu_full_en_high_school_geography": {
                  "original": 198,
                  "effective": 198
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "original": 193,
                  "effective": 193
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "original": 545,
                  "effective": 545
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "original": 390,
                  "effective": 390
                },
                "global_mmlu_full_en_sociology": {
                  "original": 201,
                  "effective": 201
                },
                "global_mmlu_full_en_world_religions": {
                  "original": 171,
                  "effective": 171
                },
                "global_mmlu_full_en_international_law": {
                  "original": 121,
                  "effective": 121
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "original": 895,
                  "effective": 895
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "original": 204,
                  "effective": 204
                },
                "global_mmlu_full_en_jurisprudence": {
                  "original": 108,
                  "effective": 108
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "original": 237,
                  "effective": 237
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "original": 163,
                  "effective": 163
                },
                "global_mmlu_full_en_philosophy": {
                  "original": 311,
                  "effective": 311
                },
                "global_mmlu_full_en_professional_law": {
                  "original": 1534,
                  "effective": 1534
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "original": 165,
                  "effective": 165
                },
                "global_mmlu_full_en_formal_logic": {
                  "original": 126,
                  "effective": 126
                },
                "global_mmlu_full_en_moral_disputes": {
                  "original": 346,
                  "effective": 346
                },
                "global_mmlu_full_en_prehistory": {
                  "original": 324,
                  "effective": 324
                }
              },
              "config": {
                "model": "hf",
                "model_args": "pretrained=/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k,trust_remote_code=True,local_files_only=True,trust_remote_code=True",
                "model_num_parameters": 222903552,
                "model_dtype": "torch.float32",
                "model_revision": "main",
                "model_sha": "",
                "batch_size": "auto",
                "batch_sizes": [
                  64
                ],
                "device": "cuda",
                "use_cache": null,
                "limit": null,
                "bootstrap_iters": 100000,
                "gen_kwargs": null,
                "random_seed": 42,
                "numpy_seed": 42,
                "torch_seed": 42,
                "fewshot_seed": 42
              },
              "git_hash": "a4103c7",
              "date": 1758646967.7892098,
              "pretty_env_info": "PyTorch version: 2.7.1+cu126\nIs debug build: False\nCUDA used to build PyTorch: 12.6\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.2 LTS (x86_64)\nGCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\nClang version: Could not collect\nCMake version: version 3.24.1\nLibc version: glibc-2.35\n\nPython version: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] (64-bit runtime)\nPython platform: Linux-6.8.0-1036-nvidia-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H200\nNvidia driver version: 580.82.07\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               224\nOn-line CPU(s) list:                  0-223\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480C\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   56\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          3800.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            5.3 MiB (112 instances)\nL1i cache:                            3.5 MiB (112 instances)\nL2 cache:                             224 MiB (112 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-55,112-167\nNUMA node1 CPU(s):                    56-111,168-223\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pytorch-lightning==2.5.2\n[pip3] torch==2.7.1\n[pip3] torchmetrics==1.8.0\n[pip3] triton==3.3.1\n[conda] Could not collect",
              "transformers_version": "4.54.1",
              "lm_eval_version": "0.4.9",
              "upper_git_hash": null,
              "tokenizer_pad_token": [
                "<pad>",
                "0"
              ],
              "tokenizer_eos_token": [
                "</s>",
                "1"
              ],
              "tokenizer_bos_token": [
                null,
                "None"
              ],
              "eot_token_id": 1,
              "max_length": 512,
              "task_hashes": {},
              "model_source": "hf",
              "model_name": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
              "model_name_sanitized": "__netscratch__nrauscher__projects__BA-hydra__evaluation__models__temp__hf-germanT5-continued-pretraind-on-german-15k",
              "system_instruction": null,
              "system_instruction_sha": null,
              "fewshot_as_multiturn": false,
              "chat_template": null,
              "chat_template_sha": null,
              "start_time": 633111.786801929,
              "end_time": 633781.133485997,
              "total_evaluation_time_seconds": "669.3466840679757"
            },
            "duration_seconds": 716.0257775783539,
            "duration_minutes": 11.933762959639232,
            "command": "python -m lm_eval --model hf --model_args pretrained=/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k,trust_remote_code=True,local_files_only=True --tasks global_mmlu_full_en --device cuda --batch_size auto --output_path /netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/hf-germanT5-continued-pretraind-on-german-15k_global_mmlu_en_0shot_20250923_190115 --num_fewshot 0 --trust_remote_code --seed 42 --wandb_args project=BA-T5-CrossLingual,entity=nikolas-rauscher-dfki,group=crosslingual_transfer_eval_full_15k-final,name=hf-germanT5-continued-pretraind-on-german-15k_global_mmlu_en_0shot",
            "output_dir": "/netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/hf-germanT5-continued-pretraind-on-german-15k_global_mmlu_en_0shot_20250923_190115",
            "status": "success"
          }
        },
        "global_mmlu_de": {
          "0_shot": {
            "parse_status": "partial",
            "error": "Could not find primary metric",
            "raw_results": {
              "results": {
                "global_mmlu_full_de": {
                  "acc,none": 0.22945449366187154,
                  "acc_stderr,none": 0.0035426913692823605,
                  "alias": "global_mmlu_full_de"
                },
                "global_mmlu_full_de_humanities": {
                  "acc,none": 0.24208289054197663,
                  "acc_stderr,none": 0.0062426684031394305,
                  "alias": " - global_mmlu_full_de_humanities"
                },
                "global_mmlu_full_de_formal_logic": {
                  "alias": "  - global_mmlu_full_de_formal_logic",
                  "acc,none": 0.2857142857142857,
                  "acc_stderr,none": 0.04040610178208841
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "alias": "  - global_mmlu_full_de_high_school_european_history",
                  "acc,none": 0.21818181818181817,
                  "acc_stderr,none": 0.03225078108306289
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "alias": "  - global_mmlu_full_de_high_school_us_history",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.03039153369274154
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "alias": "  - global_mmlu_full_de_high_school_world_history",
                  "acc,none": 0.270042194092827,
                  "acc_stderr,none": 0.028900721906293426
                },
                "global_mmlu_full_de_international_law": {
                  "alias": "  - global_mmlu_full_de_international_law",
                  "acc,none": 0.2396694214876033,
                  "acc_stderr,none": 0.03896878985070417
                },
                "global_mmlu_full_de_jurisprudence": {
                  "alias": "  - global_mmlu_full_de_jurisprudence",
                  "acc,none": 0.25925925925925924,
                  "acc_stderr,none": 0.04236511258094634
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "alias": "  - global_mmlu_full_de_logical_fallacies",
                  "acc,none": 0.22085889570552147,
                  "acc_stderr,none": 0.032591773927421776
                },
                "global_mmlu_full_de_moral_disputes": {
                  "alias": "  - global_mmlu_full_de_moral_disputes",
                  "acc,none": 0.24855491329479767,
                  "acc_stderr,none": 0.023267528432100174
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "alias": "  - global_mmlu_full_de_moral_scenarios",
                  "acc,none": 0.23798882681564246,
                  "acc_stderr,none": 0.014242630070574885
                },
                "global_mmlu_full_de_philosophy": {
                  "alias": "  - global_mmlu_full_de_philosophy",
                  "acc,none": 0.1864951768488746,
                  "acc_stderr,none": 0.02212243977248077
                },
                "global_mmlu_full_de_prehistory": {
                  "alias": "  - global_mmlu_full_de_prehistory",
                  "acc,none": 0.21604938271604937,
                  "acc_stderr,none": 0.022899162918445813
                },
                "global_mmlu_full_de_professional_law": {
                  "alias": "  - global_mmlu_full_de_professional_law",
                  "acc,none": 0.2457627118644068,
                  "acc_stderr,none": 0.01099615663514269
                },
                "global_mmlu_full_de_world_religions": {
                  "alias": "  - global_mmlu_full_de_world_religions",
                  "acc,none": 0.3216374269005848,
                  "acc_stderr,none": 0.03582529442573122
                },
                "global_mmlu_full_de_other": {
                  "acc,none": 0.23978113936272932,
                  "acc_stderr,none": 0.00764225029165751,
                  "alias": " - global_mmlu_full_de_other"
                },
                "global_mmlu_full_de_business_ethics": {
                  "alias": "  - global_mmlu_full_de_business_ethics",
                  "acc,none": 0.3,
                  "acc_stderr,none": 0.046056618647183814
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "alias": "  - global_mmlu_full_de_clinical_knowledge",
                  "acc,none": 0.21509433962264152,
                  "acc_stderr,none": 0.025288394502891377
                },
                "global_mmlu_full_de_college_medicine": {
                  "alias": "  - global_mmlu_full_de_college_medicine",
                  "acc,none": 0.20809248554913296,
                  "acc_stderr,none": 0.030952890217749884
                },
                "global_mmlu_full_de_global_facts": {
                  "alias": "  - global_mmlu_full_de_global_facts",
                  "acc,none": 0.18,
                  "acc_stderr,none": 0.038612291966536955
                },
                "global_mmlu_full_de_human_aging": {
                  "alias": "  - global_mmlu_full_de_human_aging",
                  "acc,none": 0.31390134529147984,
                  "acc_stderr,none": 0.03114679648297246
                },
                "global_mmlu_full_de_management": {
                  "alias": "  - global_mmlu_full_de_management",
                  "acc,none": 0.17475728155339806,
                  "acc_stderr,none": 0.03760178006026621
                },
                "global_mmlu_full_de_marketing": {
                  "alias": "  - global_mmlu_full_de_marketing",
                  "acc,none": 0.2905982905982906,
                  "acc_stderr,none": 0.029745048572674057
                },
                "global_mmlu_full_de_medical_genetics": {
                  "alias": "  - global_mmlu_full_de_medical_genetics",
                  "acc,none": 0.3,
                  "acc_stderr,none": 0.046056618647183814
                },
                "global_mmlu_full_de_miscellaneous": {
                  "alias": "  - global_mmlu_full_de_miscellaneous",
                  "acc,none": 0.23754789272030652,
                  "acc_stderr,none": 0.015218733046150195
                },
                "global_mmlu_full_de_nutrition": {
                  "alias": "  - global_mmlu_full_de_nutrition",
                  "acc,none": 0.22549019607843138,
                  "acc_stderr,none": 0.023929155517351284
                },
                "global_mmlu_full_de_professional_accounting": {
                  "alias": "  - global_mmlu_full_de_professional_accounting",
                  "acc,none": 0.23404255319148937,
                  "acc_stderr,none": 0.025257861359432407
                },
                "global_mmlu_full_de_professional_medicine": {
                  "alias": "  - global_mmlu_full_de_professional_medicine",
                  "acc,none": 0.18382352941176472,
                  "acc_stderr,none": 0.02352924218519311
                },
                "global_mmlu_full_de_virology": {
                  "alias": "  - global_mmlu_full_de_virology",
                  "acc,none": 0.28313253012048195,
                  "acc_stderr,none": 0.03507295431370518
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc,none": 0.2170945726356841,
                  "acc_stderr,none": 0.007428786285788536,
                  "alias": " - global_mmlu_full_de_social_sciences"
                },
                "global_mmlu_full_de_econometrics": {
                  "alias": "  - global_mmlu_full_de_econometrics",
                  "acc,none": 0.23684210526315788,
                  "acc_stderr,none": 0.039994238792813386
                },
                "global_mmlu_full_de_high_school_geography": {
                  "alias": "  - global_mmlu_full_de_high_school_geography",
                  "acc,none": 0.17676767676767677,
                  "acc_stderr,none": 0.027178752639044915
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "alias": "  - global_mmlu_full_de_high_school_government_and_politics",
                  "acc,none": 0.19689119170984457,
                  "acc_stderr,none": 0.02869787397186069
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "alias": "  - global_mmlu_full_de_high_school_macroeconomics",
                  "acc,none": 0.20256410256410257,
                  "acc_stderr,none": 0.020377660970371397
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "alias": "  - global_mmlu_full_de_high_school_microeconomics",
                  "acc,none": 0.21008403361344538,
                  "acc_stderr,none": 0.026461398717471874
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "alias": "  - global_mmlu_full_de_high_school_psychology",
                  "acc,none": 0.1926605504587156,
                  "acc_stderr,none": 0.016909276884936073
                },
                "global_mmlu_full_de_human_sexuality": {
                  "alias": "  - global_mmlu_full_de_human_sexuality",
                  "acc,none": 0.2595419847328244,
                  "acc_stderr,none": 0.03844876139785271
                },
                "global_mmlu_full_de_professional_psychology": {
                  "alias": "  - global_mmlu_full_de_professional_psychology",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.01751781884501444
                },
                "global_mmlu_full_de_public_relations": {
                  "alias": "  - global_mmlu_full_de_public_relations",
                  "acc,none": 0.21818181818181817,
                  "acc_stderr,none": 0.03955932861795833
                },
                "global_mmlu_full_de_security_studies": {
                  "alias": "  - global_mmlu_full_de_security_studies",
                  "acc,none": 0.18775510204081633,
                  "acc_stderr,none": 0.02500025603954622
                },
                "global_mmlu_full_de_sociology": {
                  "alias": "  - global_mmlu_full_de_sociology",
                  "acc,none": 0.24378109452736318,
                  "acc_stderr,none": 0.030360490154014652
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "alias": "  - global_mmlu_full_de_us_foreign_policy",
                  "acc,none": 0.28,
                  "acc_stderr,none": 0.045126085985421276
                },
                "global_mmlu_full_de_stem": {
                  "acc,none": 0.21249603552172533,
                  "acc_stderr,none": 0.007271218700485502,
                  "alias": " - global_mmlu_full_de_stem"
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "alias": "  - global_mmlu_full_de_abstract_algebra",
                  "acc,none": 0.22,
                  "acc_stderr,none": 0.04163331998932269
                },
                "global_mmlu_full_de_anatomy": {
                  "alias": "  - global_mmlu_full_de_anatomy",
                  "acc,none": 0.18518518518518517,
                  "acc_stderr,none": 0.03355677216313142
                },
                "global_mmlu_full_de_astronomy": {
                  "alias": "  - global_mmlu_full_de_astronomy",
                  "acc,none": 0.17763157894736842,
                  "acc_stderr,none": 0.031103182383123398
                },
                "global_mmlu_full_de_college_biology": {
                  "alias": "  - global_mmlu_full_de_college_biology",
                  "acc,none": 0.2569444444444444,
                  "acc_stderr,none": 0.03653946969442099
                },
                "global_mmlu_full_de_college_chemistry": {
                  "alias": "  - global_mmlu_full_de_college_chemistry",
                  "acc,none": 0.2,
                  "acc_stderr,none": 0.040201512610368445
                },
                "global_mmlu_full_de_college_computer_science": {
                  "alias": "  - global_mmlu_full_de_college_computer_science",
                  "acc,none": 0.26,
                  "acc_stderr,none": 0.044084400227680794
                },
                "global_mmlu_full_de_college_mathematics": {
                  "alias": "  - global_mmlu_full_de_college_mathematics",
                  "acc,none": 0.21,
                  "acc_stderr,none": 0.040936018074033256
                },
                "global_mmlu_full_de_college_physics": {
                  "alias": "  - global_mmlu_full_de_college_physics",
                  "acc,none": 0.21568627450980393,
                  "acc_stderr,none": 0.040925639582376556
                },
                "global_mmlu_full_de_computer_security": {
                  "alias": "  - global_mmlu_full_de_computer_security",
                  "acc,none": 0.28,
                  "acc_stderr,none": 0.045126085985421276
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "alias": "  - global_mmlu_full_de_conceptual_physics",
                  "acc,none": 0.26382978723404255,
                  "acc_stderr,none": 0.02880998985410298
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "alias": "  - global_mmlu_full_de_electrical_engineering",
                  "acc,none": 0.2413793103448276,
                  "acc_stderr,none": 0.03565998174135302
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "alias": "  - global_mmlu_full_de_elementary_mathematics",
                  "acc,none": 0.20899470899470898,
                  "acc_stderr,none": 0.020940481565334835
                },
                "global_mmlu_full_de_high_school_biology": {
                  "alias": "  - global_mmlu_full_de_high_school_biology",
                  "acc,none": 0.1774193548387097,
                  "acc_stderr,none": 0.021732540689329265
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "alias": "  - global_mmlu_full_de_high_school_chemistry",
                  "acc,none": 0.15270935960591134,
                  "acc_stderr,none": 0.025308904539380624
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "alias": "  - global_mmlu_full_de_high_school_computer_science",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.04351941398892446
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "alias": "  - global_mmlu_full_de_high_school_mathematics",
                  "acc,none": 0.2111111111111111,
                  "acc_stderr,none": 0.02488211685765508
                },
                "global_mmlu_full_de_high_school_physics": {
                  "alias": "  - global_mmlu_full_de_high_school_physics",
                  "acc,none": 0.1986754966887417,
                  "acc_stderr,none": 0.032578473844367746
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "alias": "  - global_mmlu_full_de_high_school_statistics",
                  "acc,none": 0.1527777777777778,
                  "acc_stderr,none": 0.02453632602613422
                },
                "global_mmlu_full_de_machine_learning": {
                  "alias": "  - global_mmlu_full_de_machine_learning",
                  "acc,none": 0.3125,
                  "acc_stderr,none": 0.043994650575715215
                }
              },
              "groups": {
                "global_mmlu_full_de": {
                  "acc,none": 0.22945449366187154,
                  "acc_stderr,none": 0.0035426913692823605,
                  "alias": "global_mmlu_full_de"
                },
                "global_mmlu_full_de_humanities": {
                  "acc,none": 0.24208289054197663,
                  "acc_stderr,none": 0.0062426684031394305,
                  "alias": " - global_mmlu_full_de_humanities"
                },
                "global_mmlu_full_de_other": {
                  "acc,none": 0.23978113936272932,
                  "acc_stderr,none": 0.00764225029165751,
                  "alias": " - global_mmlu_full_de_other"
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc,none": 0.2170945726356841,
                  "acc_stderr,none": 0.007428786285788536,
                  "alias": " - global_mmlu_full_de_social_sciences"
                },
                "global_mmlu_full_de_stem": {
                  "acc,none": 0.21249603552172533,
                  "acc_stderr,none": 0.007271218700485502,
                  "alias": " - global_mmlu_full_de_stem"
                }
              },
              "group_subtasks": {
                "global_mmlu_full_de_humanities": [
                  "global_mmlu_full_de_professional_law",
                  "global_mmlu_full_de_international_law",
                  "global_mmlu_full_de_prehistory",
                  "global_mmlu_full_de_world_religions",
                  "global_mmlu_full_de_high_school_world_history",
                  "global_mmlu_full_de_high_school_us_history",
                  "global_mmlu_full_de_jurisprudence",
                  "global_mmlu_full_de_formal_logic",
                  "global_mmlu_full_de_logical_fallacies",
                  "global_mmlu_full_de_high_school_european_history",
                  "global_mmlu_full_de_philosophy",
                  "global_mmlu_full_de_moral_scenarios",
                  "global_mmlu_full_de_moral_disputes"
                ],
                "global_mmlu_full_de_social_sciences": [
                  "global_mmlu_full_de_high_school_psychology",
                  "global_mmlu_full_de_professional_psychology",
                  "global_mmlu_full_de_high_school_government_and_politics",
                  "global_mmlu_full_de_high_school_macroeconomics",
                  "global_mmlu_full_de_high_school_microeconomics",
                  "global_mmlu_full_de_econometrics",
                  "global_mmlu_full_de_high_school_geography",
                  "global_mmlu_full_de_security_studies",
                  "global_mmlu_full_de_us_foreign_policy",
                  "global_mmlu_full_de_public_relations",
                  "global_mmlu_full_de_human_sexuality",
                  "global_mmlu_full_de_sociology"
                ],
                "global_mmlu_full_de_other": [
                  "global_mmlu_full_de_nutrition",
                  "global_mmlu_full_de_professional_medicine",
                  "global_mmlu_full_de_college_medicine",
                  "global_mmlu_full_de_professional_accounting",
                  "global_mmlu_full_de_marketing",
                  "global_mmlu_full_de_miscellaneous",
                  "global_mmlu_full_de_business_ethics",
                  "global_mmlu_full_de_virology",
                  "global_mmlu_full_de_medical_genetics",
                  "global_mmlu_full_de_human_aging",
                  "global_mmlu_full_de_global_facts",
                  "global_mmlu_full_de_clinical_knowledge",
                  "global_mmlu_full_de_management"
                ],
                "global_mmlu_full_de_stem": [
                  "global_mmlu_full_de_computer_security",
                  "global_mmlu_full_de_college_biology",
                  "global_mmlu_full_de_college_mathematics",
                  "global_mmlu_full_de_conceptual_physics",
                  "global_mmlu_full_de_elementary_mathematics",
                  "global_mmlu_full_de_astronomy",
                  "global_mmlu_full_de_high_school_physics",
                  "global_mmlu_full_de_machine_learning",
                  "global_mmlu_full_de_college_chemistry",
                  "global_mmlu_full_de_high_school_chemistry",
                  "global_mmlu_full_de_college_computer_science",
                  "global_mmlu_full_de_high_school_computer_science",
                  "global_mmlu_full_de_high_school_mathematics",
                  "global_mmlu_full_de_abstract_algebra",
                  "global_mmlu_full_de_high_school_biology",
                  "global_mmlu_full_de_electrical_engineering",
                  "global_mmlu_full_de_college_physics",
                  "global_mmlu_full_de_anatomy",
                  "global_mmlu_full_de_high_school_statistics"
                ],
                "global_mmlu_full_de": [
                  "global_mmlu_full_de_stem",
                  "global_mmlu_full_de_other",
                  "global_mmlu_full_de_social_sciences",
                  "global_mmlu_full_de_humanities"
                ]
              },
              "configs": {
                "global_mmlu_full_de_abstract_algebra": {
                  "task": "global_mmlu_full_de_abstract_algebra",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179f490>, subject='abstract_algebra')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_anatomy": {
                  "task": "global_mmlu_full_de_anatomy",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179c310>, subject='anatomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_astronomy": {
                  "task": "global_mmlu_full_de_astronomy",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179dea0>, subject='astronomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_business_ethics": {
                  "task": "global_mmlu_full_de_business_ethics",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b0526d40>, subject='business_ethics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "task": "global_mmlu_full_de_clinical_knowledge",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b0525240>, subject='clinical_knowledge')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_biology": {
                  "task": "global_mmlu_full_de_college_biology",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179eb00>, subject='college_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_chemistry": {
                  "task": "global_mmlu_full_de_college_chemistry",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179c5e0>, subject='college_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_computer_science": {
                  "task": "global_mmlu_full_de_college_computer_science",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179c820>, subject='college_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_mathematics": {
                  "task": "global_mmlu_full_de_college_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef250a95a0>, subject='college_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_medicine": {
                  "task": "global_mmlu_full_de_college_medicine",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b0525360>, subject='college_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_physics": {
                  "task": "global_mmlu_full_de_college_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179ed40>, subject='college_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_computer_security": {
                  "task": "global_mmlu_full_de_computer_security",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179f9a0>, subject='computer_security')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "task": "global_mmlu_full_de_conceptual_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179f0a0>, subject='conceptual_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_econometrics": {
                  "task": "global_mmlu_full_de_econometrics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b05252d0>, subject='econometrics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "task": "global_mmlu_full_de_electrical_engineering",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179e440>, subject='electrical_engineering')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "task": "global_mmlu_full_de_elementary_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179f2e0>, subject='elementary_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_formal_logic": {
                  "task": "global_mmlu_full_de_formal_logic",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1c1284670>, subject='formal_logic')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_global_facts": {
                  "task": "global_mmlu_full_de_global_facts",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b0525630>, subject='global_facts')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_biology": {
                  "task": "global_mmlu_full_de_high_school_biology",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179ea70>, subject='high_school_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "task": "global_mmlu_full_de_high_school_chemistry",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179f1c0>, subject='high_school_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "task": "global_mmlu_full_de_high_school_computer_science",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179e680>, subject='high_school_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "task": "global_mmlu_full_de_high_school_european_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1c1285630>, subject='high_school_european_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_geography": {
                  "task": "global_mmlu_full_de_high_school_geography",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1c1285120>, subject='high_school_geography')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "task": "global_mmlu_full_de_high_school_government_and_politics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b05257e0>, subject='high_school_government_and_politics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "task": "global_mmlu_full_de_high_school_macroeconomics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b05265f0>, subject='high_school_macroeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "task": "global_mmlu_full_de_high_school_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179d2d0>, subject='high_school_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "task": "global_mmlu_full_de_high_school_microeconomics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b0524e50>, subject='high_school_microeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_physics": {
                  "task": "global_mmlu_full_de_high_school_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179fe20>, subject='high_school_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "task": "global_mmlu_full_de_high_school_psychology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b0526f80>, subject='high_school_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "task": "global_mmlu_full_de_high_school_statistics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179d480>, subject='high_school_statistics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "task": "global_mmlu_full_de_high_school_us_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1c12869e0>, subject='high_school_us_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "task": "global_mmlu_full_de_high_school_world_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1c12849d0>, subject='high_school_world_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_human_aging": {
                  "task": "global_mmlu_full_de_human_aging",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b0525ab0>, subject='human_aging')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_human_sexuality": {
                  "task": "global_mmlu_full_de_human_sexuality",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1c12853f0>, subject='human_sexuality')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_international_law": {
                  "task": "global_mmlu_full_de_international_law",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1c1285d80>, subject='international_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_jurisprudence": {
                  "task": "global_mmlu_full_de_jurisprudence",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1c12870a0>, subject='jurisprudence')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "task": "global_mmlu_full_de_logical_fallacies",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1c1284ca0>, subject='logical_fallacies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_machine_learning": {
                  "task": "global_mmlu_full_de_machine_learning",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179f130>, subject='machine_learning')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_management": {
                  "task": "global_mmlu_full_de_management",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b05270a0>, subject='management')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_marketing": {
                  "task": "global_mmlu_full_de_marketing",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b0524040>, subject='marketing')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_medical_genetics": {
                  "task": "global_mmlu_full_de_medical_genetics",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b0524ca0>, subject='medical_genetics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_miscellaneous": {
                  "task": "global_mmlu_full_de_miscellaneous",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b0527880>, subject='miscellaneous')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_moral_disputes": {
                  "task": "global_mmlu_full_de_moral_disputes",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b084c160>, subject='moral_disputes')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "task": "global_mmlu_full_de_moral_scenarios",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1c1284f70>, subject='moral_scenarios')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_nutrition": {
                  "task": "global_mmlu_full_de_nutrition",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179d510>, subject='nutrition')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_philosophy": {
                  "task": "global_mmlu_full_de_philosophy",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1c5615990>, subject='philosophy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_prehistory": {
                  "task": "global_mmlu_full_de_prehistory",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1c12843a0>, subject='prehistory')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_professional_accounting": {
                  "task": "global_mmlu_full_de_professional_accounting",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b0525e10>, subject='professional_accounting')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_professional_law": {
                  "task": "global_mmlu_full_de_professional_law",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1c1284af0>, subject='professional_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_professional_medicine": {
                  "task": "global_mmlu_full_de_professional_medicine",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cef2179c3a0>, subject='professional_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_professional_psychology": {
                  "task": "global_mmlu_full_de_professional_psychology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b0525000>, subject='professional_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_public_relations": {
                  "task": "global_mmlu_full_de_public_relations",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1c1286200>, subject='public_relations')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_security_studies": {
                  "task": "global_mmlu_full_de_security_studies",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1c1287010>, subject='security_studies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_sociology": {
                  "task": "global_mmlu_full_de_sociology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1c1287640>, subject='sociology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "task": "global_mmlu_full_de_us_foreign_policy",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b05243a0>, subject='us_foreign_policy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_virology": {
                  "task": "global_mmlu_full_de_virology",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1b05271c0>, subject='virology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_world_religions": {
                  "task": "global_mmlu_full_de_world_religions",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7cf1c1286ef0>, subject='world_religions')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                }
              },
              "versions": {
                "global_mmlu_full_de": 0.0,
                "global_mmlu_full_de_abstract_algebra": 0.0,
                "global_mmlu_full_de_anatomy": 0.0,
                "global_mmlu_full_de_astronomy": 0.0,
                "global_mmlu_full_de_business_ethics": 0.0,
                "global_mmlu_full_de_clinical_knowledge": 0.0,
                "global_mmlu_full_de_college_biology": 0.0,
                "global_mmlu_full_de_college_chemistry": 0.0,
                "global_mmlu_full_de_college_computer_science": 0.0,
                "global_mmlu_full_de_college_mathematics": 0.0,
                "global_mmlu_full_de_college_medicine": 0.0,
                "global_mmlu_full_de_college_physics": 0.0,
                "global_mmlu_full_de_computer_security": 0.0,
                "global_mmlu_full_de_conceptual_physics": 0.0,
                "global_mmlu_full_de_econometrics": 0.0,
                "global_mmlu_full_de_electrical_engineering": 0.0,
                "global_mmlu_full_de_elementary_mathematics": 0.0,
                "global_mmlu_full_de_formal_logic": 0.0,
                "global_mmlu_full_de_global_facts": 0.0,
                "global_mmlu_full_de_high_school_biology": 0.0,
                "global_mmlu_full_de_high_school_chemistry": 0.0,
                "global_mmlu_full_de_high_school_computer_science": 0.0,
                "global_mmlu_full_de_high_school_european_history": 0.0,
                "global_mmlu_full_de_high_school_geography": 0.0,
                "global_mmlu_full_de_high_school_government_and_politics": 0.0,
                "global_mmlu_full_de_high_school_macroeconomics": 0.0,
                "global_mmlu_full_de_high_school_mathematics": 0.0,
                "global_mmlu_full_de_high_school_microeconomics": 0.0,
                "global_mmlu_full_de_high_school_physics": 0.0,
                "global_mmlu_full_de_high_school_psychology": 0.0,
                "global_mmlu_full_de_high_school_statistics": 0.0,
                "global_mmlu_full_de_high_school_us_history": 0.0,
                "global_mmlu_full_de_high_school_world_history": 0.0,
                "global_mmlu_full_de_human_aging": 0.0,
                "global_mmlu_full_de_human_sexuality": 0.0,
                "global_mmlu_full_de_humanities": 0.0,
                "global_mmlu_full_de_international_law": 0.0,
                "global_mmlu_full_de_jurisprudence": 0.0,
                "global_mmlu_full_de_logical_fallacies": 0.0,
                "global_mmlu_full_de_machine_learning": 0.0,
                "global_mmlu_full_de_management": 0.0,
                "global_mmlu_full_de_marketing": 0.0,
                "global_mmlu_full_de_medical_genetics": 0.0,
                "global_mmlu_full_de_miscellaneous": 0.0,
                "global_mmlu_full_de_moral_disputes": 0.0,
                "global_mmlu_full_de_moral_scenarios": 0.0,
                "global_mmlu_full_de_nutrition": 0.0,
                "global_mmlu_full_de_other": 0.0,
                "global_mmlu_full_de_philosophy": 0.0,
                "global_mmlu_full_de_prehistory": 0.0,
                "global_mmlu_full_de_professional_accounting": 0.0,
                "global_mmlu_full_de_professional_law": 0.0,
                "global_mmlu_full_de_professional_medicine": 0.0,
                "global_mmlu_full_de_professional_psychology": 0.0,
                "global_mmlu_full_de_public_relations": 0.0,
                "global_mmlu_full_de_security_studies": 0.0,
                "global_mmlu_full_de_social_sciences": 0.0,
                "global_mmlu_full_de_sociology": 0.0,
                "global_mmlu_full_de_stem": 0.0,
                "global_mmlu_full_de_us_foreign_policy": 0.0,
                "global_mmlu_full_de_virology": 0.0,
                "global_mmlu_full_de_world_religions": 0.0
              },
              "n-shot": {
                "global_mmlu_full_de_abstract_algebra": 0,
                "global_mmlu_full_de_anatomy": 0,
                "global_mmlu_full_de_astronomy": 0,
                "global_mmlu_full_de_business_ethics": 0,
                "global_mmlu_full_de_clinical_knowledge": 0,
                "global_mmlu_full_de_college_biology": 0,
                "global_mmlu_full_de_college_chemistry": 0,
                "global_mmlu_full_de_college_computer_science": 0,
                "global_mmlu_full_de_college_mathematics": 0,
                "global_mmlu_full_de_college_medicine": 0,
                "global_mmlu_full_de_college_physics": 0,
                "global_mmlu_full_de_computer_security": 0,
                "global_mmlu_full_de_conceptual_physics": 0,
                "global_mmlu_full_de_econometrics": 0,
                "global_mmlu_full_de_electrical_engineering": 0,
                "global_mmlu_full_de_elementary_mathematics": 0,
                "global_mmlu_full_de_formal_logic": 0,
                "global_mmlu_full_de_global_facts": 0,
                "global_mmlu_full_de_high_school_biology": 0,
                "global_mmlu_full_de_high_school_chemistry": 0,
                "global_mmlu_full_de_high_school_computer_science": 0,
                "global_mmlu_full_de_high_school_european_history": 0,
                "global_mmlu_full_de_high_school_geography": 0,
                "global_mmlu_full_de_high_school_government_and_politics": 0,
                "global_mmlu_full_de_high_school_macroeconomics": 0,
                "global_mmlu_full_de_high_school_mathematics": 0,
                "global_mmlu_full_de_high_school_microeconomics": 0,
                "global_mmlu_full_de_high_school_physics": 0,
                "global_mmlu_full_de_high_school_psychology": 0,
                "global_mmlu_full_de_high_school_statistics": 0,
                "global_mmlu_full_de_high_school_us_history": 0,
                "global_mmlu_full_de_high_school_world_history": 0,
                "global_mmlu_full_de_human_aging": 0,
                "global_mmlu_full_de_human_sexuality": 0,
                "global_mmlu_full_de_international_law": 0,
                "global_mmlu_full_de_jurisprudence": 0,
                "global_mmlu_full_de_logical_fallacies": 0,
                "global_mmlu_full_de_machine_learning": 0,
                "global_mmlu_full_de_management": 0,
                "global_mmlu_full_de_marketing": 0,
                "global_mmlu_full_de_medical_genetics": 0,
                "global_mmlu_full_de_miscellaneous": 0,
                "global_mmlu_full_de_moral_disputes": 0,
                "global_mmlu_full_de_moral_scenarios": 0,
                "global_mmlu_full_de_nutrition": 0,
                "global_mmlu_full_de_philosophy": 0,
                "global_mmlu_full_de_prehistory": 0,
                "global_mmlu_full_de_professional_accounting": 0,
                "global_mmlu_full_de_professional_law": 0,
                "global_mmlu_full_de_professional_medicine": 0,
                "global_mmlu_full_de_professional_psychology": 0,
                "global_mmlu_full_de_public_relations": 0,
                "global_mmlu_full_de_security_studies": 0,
                "global_mmlu_full_de_sociology": 0,
                "global_mmlu_full_de_us_foreign_policy": 0,
                "global_mmlu_full_de_virology": 0,
                "global_mmlu_full_de_world_religions": 0
              },
              "higher_is_better": {
                "global_mmlu_full_de": {
                  "acc": true
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "acc": true
                },
                "global_mmlu_full_de_anatomy": {
                  "acc": true
                },
                "global_mmlu_full_de_astronomy": {
                  "acc": true
                },
                "global_mmlu_full_de_business_ethics": {
                  "acc": true
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "acc": true
                },
                "global_mmlu_full_de_college_biology": {
                  "acc": true
                },
                "global_mmlu_full_de_college_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_de_college_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_de_college_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_college_medicine": {
                  "acc": true
                },
                "global_mmlu_full_de_college_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_computer_security": {
                  "acc": true
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_econometrics": {
                  "acc": true
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "acc": true
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_formal_logic": {
                  "acc": true
                },
                "global_mmlu_full_de_global_facts": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_biology": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_geography": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "acc": true
                },
                "global_mmlu_full_de_human_aging": {
                  "acc": true
                },
                "global_mmlu_full_de_human_sexuality": {
                  "acc": true
                },
                "global_mmlu_full_de_humanities": {
                  "acc": true
                },
                "global_mmlu_full_de_international_law": {
                  "acc": true
                },
                "global_mmlu_full_de_jurisprudence": {
                  "acc": true
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "acc": true
                },
                "global_mmlu_full_de_machine_learning": {
                  "acc": true
                },
                "global_mmlu_full_de_management": {
                  "acc": true
                },
                "global_mmlu_full_de_marketing": {
                  "acc": true
                },
                "global_mmlu_full_de_medical_genetics": {
                  "acc": true
                },
                "global_mmlu_full_de_miscellaneous": {
                  "acc": true
                },
                "global_mmlu_full_de_moral_disputes": {
                  "acc": true
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "acc": true
                },
                "global_mmlu_full_de_nutrition": {
                  "acc": true
                },
                "global_mmlu_full_de_other": {
                  "acc": true
                },
                "global_mmlu_full_de_philosophy": {
                  "acc": true
                },
                "global_mmlu_full_de_prehistory": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_accounting": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_law": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_medicine": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_psychology": {
                  "acc": true
                },
                "global_mmlu_full_de_public_relations": {
                  "acc": true
                },
                "global_mmlu_full_de_security_studies": {
                  "acc": true
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc": true
                },
                "global_mmlu_full_de_sociology": {
                  "acc": true
                },
                "global_mmlu_full_de_stem": {
                  "acc": true
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "acc": true
                },
                "global_mmlu_full_de_virology": {
                  "acc": true
                },
                "global_mmlu_full_de_world_religions": {
                  "acc": true
                }
              },
              "n-samples": {
                "global_mmlu_full_de_computer_security": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_college_biology": {
                  "original": 144,
                  "effective": 144
                },
                "global_mmlu_full_de_college_mathematics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "original": 235,
                  "effective": 235
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "original": 378,
                  "effective": 378
                },
                "global_mmlu_full_de_astronomy": {
                  "original": 152,
                  "effective": 152
                },
                "global_mmlu_full_de_high_school_physics": {
                  "original": 151,
                  "effective": 151
                },
                "global_mmlu_full_de_machine_learning": {
                  "original": 112,
                  "effective": 112
                },
                "global_mmlu_full_de_college_chemistry": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "original": 203,
                  "effective": 203
                },
                "global_mmlu_full_de_college_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "original": 270,
                  "effective": 270
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_biology": {
                  "original": 310,
                  "effective": 310
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "original": 145,
                  "effective": 145
                },
                "global_mmlu_full_de_college_physics": {
                  "original": 102,
                  "effective": 102
                },
                "global_mmlu_full_de_anatomy": {
                  "original": 135,
                  "effective": 135
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "original": 216,
                  "effective": 216
                },
                "global_mmlu_full_de_nutrition": {
                  "original": 306,
                  "effective": 306
                },
                "global_mmlu_full_de_professional_medicine": {
                  "original": 272,
                  "effective": 272
                },
                "global_mmlu_full_de_college_medicine": {
                  "original": 173,
                  "effective": 173
                },
                "global_mmlu_full_de_professional_accounting": {
                  "original": 282,
                  "effective": 282
                },
                "global_mmlu_full_de_marketing": {
                  "original": 234,
                  "effective": 234
                },
                "global_mmlu_full_de_miscellaneous": {
                  "original": 783,
                  "effective": 783
                },
                "global_mmlu_full_de_business_ethics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_virology": {
                  "original": 166,
                  "effective": 166
                },
                "global_mmlu_full_de_medical_genetics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_human_aging": {
                  "original": 223,
                  "effective": 223
                },
                "global_mmlu_full_de_global_facts": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "original": 265,
                  "effective": 265
                },
                "global_mmlu_full_de_management": {
                  "original": 103,
                  "effective": 103
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "original": 545,
                  "effective": 545
                },
                "global_mmlu_full_de_professional_psychology": {
                  "original": 612,
                  "effective": 612
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "original": 193,
                  "effective": 193
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "original": 390,
                  "effective": 390
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "original": 238,
                  "effective": 238
                },
                "global_mmlu_full_de_econometrics": {
                  "original": 114,
                  "effective": 114
                },
                "global_mmlu_full_de_high_school_geography": {
                  "original": 198,
                  "effective": 198
                },
                "global_mmlu_full_de_security_studies": {
                  "original": 245,
                  "effective": 245
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_public_relations": {
                  "original": 110,
                  "effective": 110
                },
                "global_mmlu_full_de_human_sexuality": {
                  "original": 131,
                  "effective": 131
                },
                "global_mmlu_full_de_sociology": {
                  "original": 201,
                  "effective": 201
                },
                "global_mmlu_full_de_professional_law": {
                  "original": 1534,
                  "effective": 1534
                },
                "global_mmlu_full_de_international_law": {
                  "original": 121,
                  "effective": 121
                },
                "global_mmlu_full_de_prehistory": {
                  "original": 324,
                  "effective": 324
                },
                "global_mmlu_full_de_world_religions": {
                  "original": 171,
                  "effective": 171
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "original": 237,
                  "effective": 237
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "original": 204,
                  "effective": 204
                },
                "global_mmlu_full_de_jurisprudence": {
                  "original": 108,
                  "effective": 108
                },
                "global_mmlu_full_de_formal_logic": {
                  "original": 126,
                  "effective": 126
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "original": 163,
                  "effective": 163
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "original": 165,
                  "effective": 165
                },
                "global_mmlu_full_de_philosophy": {
                  "original": 311,
                  "effective": 311
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "original": 895,
                  "effective": 895
                },
                "global_mmlu_full_de_moral_disputes": {
                  "original": 346,
                  "effective": 346
                }
              },
              "config": {
                "model": "hf",
                "model_args": "pretrained=/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k,trust_remote_code=True,local_files_only=True,trust_remote_code=True",
                "model_num_parameters": 222903552,
                "model_dtype": "torch.float32",
                "model_revision": "main",
                "model_sha": "",
                "batch_size": "auto",
                "batch_sizes": [
                  64
                ],
                "device": "cuda",
                "use_cache": null,
                "limit": null,
                "bootstrap_iters": 100000,
                "gen_kwargs": null,
                "random_seed": 42,
                "numpy_seed": 42,
                "torch_seed": 42,
                "fewshot_seed": 42
              },
              "git_hash": "a4103c7",
              "date": 1758647686.3370848,
              "pretty_env_info": "PyTorch version: 2.7.1+cu126\nIs debug build: False\nCUDA used to build PyTorch: 12.6\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.2 LTS (x86_64)\nGCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\nClang version: Could not collect\nCMake version: version 3.24.1\nLibc version: glibc-2.35\n\nPython version: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] (64-bit runtime)\nPython platform: Linux-6.8.0-1036-nvidia-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H200\nNvidia driver version: 580.82.07\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               224\nOn-line CPU(s) list:                  0-223\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480C\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   56\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          3800.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            5.3 MiB (112 instances)\nL1i cache:                            3.5 MiB (112 instances)\nL2 cache:                             224 MiB (112 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-55,112-167\nNUMA node1 CPU(s):                    56-111,168-223\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pytorch-lightning==2.5.2\n[pip3] torch==2.7.1\n[pip3] torchmetrics==1.8.0\n[pip3] triton==3.3.1\n[conda] Could not collect",
              "transformers_version": "4.54.1",
              "lm_eval_version": "0.4.9",
              "upper_git_hash": null,
              "tokenizer_pad_token": [
                "<pad>",
                "0"
              ],
              "tokenizer_eos_token": [
                "</s>",
                "1"
              ],
              "tokenizer_bos_token": [
                null,
                "None"
              ],
              "eot_token_id": 1,
              "max_length": 512,
              "task_hashes": {},
              "model_source": "hf",
              "model_name": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k",
              "model_name_sanitized": "__netscratch__nrauscher__projects__BA-hydra__evaluation__models__temp__hf-germanT5-continued-pretraind-on-german-15k",
              "system_instruction": null,
              "system_instruction_sha": null,
              "fewshot_as_multiturn": false,
              "chat_template": null,
              "chat_template_sha": null,
              "start_time": 633829.642194156,
              "end_time": 634467.005231098,
              "total_evaluation_time_seconds": "637.3630369420862"
            },
            "duration_seconds": 684.9367001056671,
            "duration_minutes": 11.415611668427784,
            "command": "python -m lm_eval --model hf --model_args pretrained=/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/hf-germanT5-continued-pretraind-on-german-15k,trust_remote_code=True,local_files_only=True --tasks global_mmlu_full_de --device cuda --batch_size auto --output_path /netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/hf-germanT5-continued-pretraind-on-german-15k_global_mmlu_de_0shot_20250923_191311 --num_fewshot 0 --trust_remote_code --seed 42 --wandb_args project=BA-T5-CrossLingual,entity=nikolas-rauscher-dfki,group=crosslingual_transfer_eval_full_15k-final,name=hf-germanT5-continued-pretraind-on-german-15k_global_mmlu_de_0shot",
            "output_dir": "/netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/hf-germanT5-continued-pretraind-on-german-15k_global_mmlu_de_0shot_20250923_191311",
            "status": "success"
          }
        }
      }
    },
    "hf-germanT5-base": {
      "model_config": {
        "source_path": "GermanT5/t5-efficient-gc4-german-base-nl36",
        "name": "hf-germanT5-base"
      },
      "model_path": "GermanT5/t5-efficient-gc4-german-base-nl36",
      "metadata": {
        "source_type": "huggingface",
        "original_source": "GermanT5/t5-efficient-gc4-german-base-nl36"
      },
      "benchmarks": {
        "global_mmlu_en": {
          "0_shot": {
            "parse_status": "partial",
            "error": "Could not find primary metric",
            "raw_results": {
              "results": {
                "global_mmlu_full_en": {
                  "acc,none": 0.22945449366187154,
                  "acc_stderr,none": 0.0035426913692823605,
                  "alias": "global_mmlu_full_en"
                },
                "global_mmlu_full_en_humanities": {
                  "acc,none": 0.24208289054197663,
                  "acc_stderr,none": 0.0062426684031394305,
                  "alias": " - global_mmlu_full_en_humanities"
                },
                "global_mmlu_full_en_formal_logic": {
                  "alias": "  - global_mmlu_full_en_formal_logic",
                  "acc,none": 0.2857142857142857,
                  "acc_stderr,none": 0.04040610178208841
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "alias": "  - global_mmlu_full_en_high_school_european_history",
                  "acc,none": 0.21818181818181817,
                  "acc_stderr,none": 0.03225078108306289
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "alias": "  - global_mmlu_full_en_high_school_us_history",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.03039153369274154
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "alias": "  - global_mmlu_full_en_high_school_world_history",
                  "acc,none": 0.270042194092827,
                  "acc_stderr,none": 0.028900721906293426
                },
                "global_mmlu_full_en_international_law": {
                  "alias": "  - global_mmlu_full_en_international_law",
                  "acc,none": 0.2396694214876033,
                  "acc_stderr,none": 0.03896878985070417
                },
                "global_mmlu_full_en_jurisprudence": {
                  "alias": "  - global_mmlu_full_en_jurisprudence",
                  "acc,none": 0.25925925925925924,
                  "acc_stderr,none": 0.04236511258094634
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "alias": "  - global_mmlu_full_en_logical_fallacies",
                  "acc,none": 0.22085889570552147,
                  "acc_stderr,none": 0.032591773927421776
                },
                "global_mmlu_full_en_moral_disputes": {
                  "alias": "  - global_mmlu_full_en_moral_disputes",
                  "acc,none": 0.24855491329479767,
                  "acc_stderr,none": 0.023267528432100174
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "alias": "  - global_mmlu_full_en_moral_scenarios",
                  "acc,none": 0.23798882681564246,
                  "acc_stderr,none": 0.014242630070574885
                },
                "global_mmlu_full_en_philosophy": {
                  "alias": "  - global_mmlu_full_en_philosophy",
                  "acc,none": 0.1864951768488746,
                  "acc_stderr,none": 0.02212243977248077
                },
                "global_mmlu_full_en_prehistory": {
                  "alias": "  - global_mmlu_full_en_prehistory",
                  "acc,none": 0.21604938271604937,
                  "acc_stderr,none": 0.022899162918445813
                },
                "global_mmlu_full_en_professional_law": {
                  "alias": "  - global_mmlu_full_en_professional_law",
                  "acc,none": 0.2457627118644068,
                  "acc_stderr,none": 0.01099615663514269
                },
                "global_mmlu_full_en_world_religions": {
                  "alias": "  - global_mmlu_full_en_world_religions",
                  "acc,none": 0.3216374269005848,
                  "acc_stderr,none": 0.03582529442573122
                },
                "global_mmlu_full_en_other": {
                  "acc,none": 0.23978113936272932,
                  "acc_stderr,none": 0.00764225029165751,
                  "alias": " - global_mmlu_full_en_other"
                },
                "global_mmlu_full_en_business_ethics": {
                  "alias": "  - global_mmlu_full_en_business_ethics",
                  "acc,none": 0.3,
                  "acc_stderr,none": 0.046056618647183814
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "alias": "  - global_mmlu_full_en_clinical_knowledge",
                  "acc,none": 0.21509433962264152,
                  "acc_stderr,none": 0.025288394502891377
                },
                "global_mmlu_full_en_college_medicine": {
                  "alias": "  - global_mmlu_full_en_college_medicine",
                  "acc,none": 0.20809248554913296,
                  "acc_stderr,none": 0.030952890217749884
                },
                "global_mmlu_full_en_global_facts": {
                  "alias": "  - global_mmlu_full_en_global_facts",
                  "acc,none": 0.18,
                  "acc_stderr,none": 0.038612291966536955
                },
                "global_mmlu_full_en_human_aging": {
                  "alias": "  - global_mmlu_full_en_human_aging",
                  "acc,none": 0.31390134529147984,
                  "acc_stderr,none": 0.03114679648297246
                },
                "global_mmlu_full_en_management": {
                  "alias": "  - global_mmlu_full_en_management",
                  "acc,none": 0.17475728155339806,
                  "acc_stderr,none": 0.03760178006026621
                },
                "global_mmlu_full_en_marketing": {
                  "alias": "  - global_mmlu_full_en_marketing",
                  "acc,none": 0.2905982905982906,
                  "acc_stderr,none": 0.029745048572674057
                },
                "global_mmlu_full_en_medical_genetics": {
                  "alias": "  - global_mmlu_full_en_medical_genetics",
                  "acc,none": 0.3,
                  "acc_stderr,none": 0.046056618647183814
                },
                "global_mmlu_full_en_miscellaneous": {
                  "alias": "  - global_mmlu_full_en_miscellaneous",
                  "acc,none": 0.23754789272030652,
                  "acc_stderr,none": 0.015218733046150195
                },
                "global_mmlu_full_en_nutrition": {
                  "alias": "  - global_mmlu_full_en_nutrition",
                  "acc,none": 0.22549019607843138,
                  "acc_stderr,none": 0.023929155517351284
                },
                "global_mmlu_full_en_professional_accounting": {
                  "alias": "  - global_mmlu_full_en_professional_accounting",
                  "acc,none": 0.23404255319148937,
                  "acc_stderr,none": 0.025257861359432407
                },
                "global_mmlu_full_en_professional_medicine": {
                  "alias": "  - global_mmlu_full_en_professional_medicine",
                  "acc,none": 0.18382352941176472,
                  "acc_stderr,none": 0.02352924218519311
                },
                "global_mmlu_full_en_virology": {
                  "alias": "  - global_mmlu_full_en_virology",
                  "acc,none": 0.28313253012048195,
                  "acc_stderr,none": 0.03507295431370518
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc,none": 0.2170945726356841,
                  "acc_stderr,none": 0.007428786285788534,
                  "alias": " - global_mmlu_full_en_social_sciences"
                },
                "global_mmlu_full_en_econometrics": {
                  "alias": "  - global_mmlu_full_en_econometrics",
                  "acc,none": 0.23684210526315788,
                  "acc_stderr,none": 0.039994238792813386
                },
                "global_mmlu_full_en_high_school_geography": {
                  "alias": "  - global_mmlu_full_en_high_school_geography",
                  "acc,none": 0.17676767676767677,
                  "acc_stderr,none": 0.027178752639044915
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "alias": "  - global_mmlu_full_en_high_school_government_and_politics",
                  "acc,none": 0.19689119170984457,
                  "acc_stderr,none": 0.02869787397186069
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "alias": "  - global_mmlu_full_en_high_school_macroeconomics",
                  "acc,none": 0.20256410256410257,
                  "acc_stderr,none": 0.020377660970371397
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "alias": "  - global_mmlu_full_en_high_school_microeconomics",
                  "acc,none": 0.21008403361344538,
                  "acc_stderr,none": 0.026461398717471874
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "alias": "  - global_mmlu_full_en_high_school_psychology",
                  "acc,none": 0.1926605504587156,
                  "acc_stderr,none": 0.016909276884936073
                },
                "global_mmlu_full_en_human_sexuality": {
                  "alias": "  - global_mmlu_full_en_human_sexuality",
                  "acc,none": 0.2595419847328244,
                  "acc_stderr,none": 0.03844876139785271
                },
                "global_mmlu_full_en_professional_psychology": {
                  "alias": "  - global_mmlu_full_en_professional_psychology",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.01751781884501444
                },
                "global_mmlu_full_en_public_relations": {
                  "alias": "  - global_mmlu_full_en_public_relations",
                  "acc,none": 0.21818181818181817,
                  "acc_stderr,none": 0.03955932861795833
                },
                "global_mmlu_full_en_security_studies": {
                  "alias": "  - global_mmlu_full_en_security_studies",
                  "acc,none": 0.18775510204081633,
                  "acc_stderr,none": 0.02500025603954622
                },
                "global_mmlu_full_en_sociology": {
                  "alias": "  - global_mmlu_full_en_sociology",
                  "acc,none": 0.24378109452736318,
                  "acc_stderr,none": 0.030360490154014652
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "alias": "  - global_mmlu_full_en_us_foreign_policy",
                  "acc,none": 0.28,
                  "acc_stderr,none": 0.045126085985421276
                },
                "global_mmlu_full_en_stem": {
                  "acc,none": 0.21249603552172533,
                  "acc_stderr,none": 0.007271218700485502,
                  "alias": " - global_mmlu_full_en_stem"
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "alias": "  - global_mmlu_full_en_abstract_algebra",
                  "acc,none": 0.22,
                  "acc_stderr,none": 0.04163331998932269
                },
                "global_mmlu_full_en_anatomy": {
                  "alias": "  - global_mmlu_full_en_anatomy",
                  "acc,none": 0.18518518518518517,
                  "acc_stderr,none": 0.03355677216313142
                },
                "global_mmlu_full_en_astronomy": {
                  "alias": "  - global_mmlu_full_en_astronomy",
                  "acc,none": 0.17763157894736842,
                  "acc_stderr,none": 0.031103182383123398
                },
                "global_mmlu_full_en_college_biology": {
                  "alias": "  - global_mmlu_full_en_college_biology",
                  "acc,none": 0.2569444444444444,
                  "acc_stderr,none": 0.03653946969442099
                },
                "global_mmlu_full_en_college_chemistry": {
                  "alias": "  - global_mmlu_full_en_college_chemistry",
                  "acc,none": 0.2,
                  "acc_stderr,none": 0.040201512610368445
                },
                "global_mmlu_full_en_college_computer_science": {
                  "alias": "  - global_mmlu_full_en_college_computer_science",
                  "acc,none": 0.26,
                  "acc_stderr,none": 0.044084400227680794
                },
                "global_mmlu_full_en_college_mathematics": {
                  "alias": "  - global_mmlu_full_en_college_mathematics",
                  "acc,none": 0.21,
                  "acc_stderr,none": 0.040936018074033256
                },
                "global_mmlu_full_en_college_physics": {
                  "alias": "  - global_mmlu_full_en_college_physics",
                  "acc,none": 0.21568627450980393,
                  "acc_stderr,none": 0.040925639582376556
                },
                "global_mmlu_full_en_computer_security": {
                  "alias": "  - global_mmlu_full_en_computer_security",
                  "acc,none": 0.28,
                  "acc_stderr,none": 0.045126085985421276
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "alias": "  - global_mmlu_full_en_conceptual_physics",
                  "acc,none": 0.26382978723404255,
                  "acc_stderr,none": 0.02880998985410298
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "alias": "  - global_mmlu_full_en_electrical_engineering",
                  "acc,none": 0.2413793103448276,
                  "acc_stderr,none": 0.03565998174135302
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "alias": "  - global_mmlu_full_en_elementary_mathematics",
                  "acc,none": 0.20899470899470898,
                  "acc_stderr,none": 0.020940481565334835
                },
                "global_mmlu_full_en_high_school_biology": {
                  "alias": "  - global_mmlu_full_en_high_school_biology",
                  "acc,none": 0.1774193548387097,
                  "acc_stderr,none": 0.021732540689329265
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "alias": "  - global_mmlu_full_en_high_school_chemistry",
                  "acc,none": 0.15270935960591134,
                  "acc_stderr,none": 0.025308904539380624
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "alias": "  - global_mmlu_full_en_high_school_computer_science",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.04351941398892446
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "alias": "  - global_mmlu_full_en_high_school_mathematics",
                  "acc,none": 0.2111111111111111,
                  "acc_stderr,none": 0.02488211685765508
                },
                "global_mmlu_full_en_high_school_physics": {
                  "alias": "  - global_mmlu_full_en_high_school_physics",
                  "acc,none": 0.1986754966887417,
                  "acc_stderr,none": 0.032578473844367746
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "alias": "  - global_mmlu_full_en_high_school_statistics",
                  "acc,none": 0.1527777777777778,
                  "acc_stderr,none": 0.02453632602613422
                },
                "global_mmlu_full_en_machine_learning": {
                  "alias": "  - global_mmlu_full_en_machine_learning",
                  "acc,none": 0.3125,
                  "acc_stderr,none": 0.043994650575715215
                }
              },
              "groups": {
                "global_mmlu_full_en": {
                  "acc,none": 0.22945449366187154,
                  "acc_stderr,none": 0.0035426913692823605,
                  "alias": "global_mmlu_full_en"
                },
                "global_mmlu_full_en_humanities": {
                  "acc,none": 0.24208289054197663,
                  "acc_stderr,none": 0.0062426684031394305,
                  "alias": " - global_mmlu_full_en_humanities"
                },
                "global_mmlu_full_en_other": {
                  "acc,none": 0.23978113936272932,
                  "acc_stderr,none": 0.00764225029165751,
                  "alias": " - global_mmlu_full_en_other"
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc,none": 0.2170945726356841,
                  "acc_stderr,none": 0.007428786285788534,
                  "alias": " - global_mmlu_full_en_social_sciences"
                },
                "global_mmlu_full_en_stem": {
                  "acc,none": 0.21249603552172533,
                  "acc_stderr,none": 0.007271218700485502,
                  "alias": " - global_mmlu_full_en_stem"
                }
              },
              "group_subtasks": {
                "global_mmlu_full_en_humanities": [
                  "global_mmlu_full_en_world_religions",
                  "global_mmlu_full_en_international_law",
                  "global_mmlu_full_en_moral_scenarios",
                  "global_mmlu_full_en_high_school_us_history",
                  "global_mmlu_full_en_jurisprudence",
                  "global_mmlu_full_en_high_school_world_history",
                  "global_mmlu_full_en_logical_fallacies",
                  "global_mmlu_full_en_philosophy",
                  "global_mmlu_full_en_professional_law",
                  "global_mmlu_full_en_high_school_european_history",
                  "global_mmlu_full_en_formal_logic",
                  "global_mmlu_full_en_moral_disputes",
                  "global_mmlu_full_en_prehistory"
                ],
                "global_mmlu_full_en_social_sciences": [
                  "global_mmlu_full_en_us_foreign_policy",
                  "global_mmlu_full_en_public_relations",
                  "global_mmlu_full_en_econometrics",
                  "global_mmlu_full_en_high_school_microeconomics",
                  "global_mmlu_full_en_human_sexuality",
                  "global_mmlu_full_en_security_studies",
                  "global_mmlu_full_en_professional_psychology",
                  "global_mmlu_full_en_high_school_geography",
                  "global_mmlu_full_en_high_school_government_and_politics",
                  "global_mmlu_full_en_high_school_psychology",
                  "global_mmlu_full_en_high_school_macroeconomics",
                  "global_mmlu_full_en_sociology"
                ],
                "global_mmlu_full_en_other": [
                  "global_mmlu_full_en_clinical_knowledge",
                  "global_mmlu_full_en_medical_genetics",
                  "global_mmlu_full_en_professional_medicine",
                  "global_mmlu_full_en_business_ethics",
                  "global_mmlu_full_en_marketing",
                  "global_mmlu_full_en_global_facts",
                  "global_mmlu_full_en_college_medicine",
                  "global_mmlu_full_en_human_aging",
                  "global_mmlu_full_en_management",
                  "global_mmlu_full_en_nutrition",
                  "global_mmlu_full_en_virology",
                  "global_mmlu_full_en_professional_accounting",
                  "global_mmlu_full_en_miscellaneous"
                ],
                "global_mmlu_full_en_stem": [
                  "global_mmlu_full_en_high_school_statistics",
                  "global_mmlu_full_en_computer_security",
                  "global_mmlu_full_en_machine_learning",
                  "global_mmlu_full_en_abstract_algebra",
                  "global_mmlu_full_en_astronomy",
                  "global_mmlu_full_en_college_computer_science",
                  "global_mmlu_full_en_high_school_biology",
                  "global_mmlu_full_en_high_school_computer_science",
                  "global_mmlu_full_en_high_school_chemistry",
                  "global_mmlu_full_en_elementary_mathematics",
                  "global_mmlu_full_en_college_physics",
                  "global_mmlu_full_en_high_school_physics",
                  "global_mmlu_full_en_college_chemistry",
                  "global_mmlu_full_en_college_biology",
                  "global_mmlu_full_en_college_mathematics",
                  "global_mmlu_full_en_conceptual_physics",
                  "global_mmlu_full_en_anatomy",
                  "global_mmlu_full_en_high_school_mathematics",
                  "global_mmlu_full_en_electrical_engineering"
                ],
                "global_mmlu_full_en": [
                  "global_mmlu_full_en_stem",
                  "global_mmlu_full_en_other",
                  "global_mmlu_full_en_social_sciences",
                  "global_mmlu_full_en_humanities"
                ]
              },
              "configs": {
                "global_mmlu_full_en_abstract_algebra": {
                  "task": "global_mmlu_full_en_abstract_algebra",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f08b0>, subject='abstract_algebra')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_anatomy": {
                  "task": "global_mmlu_full_en_anatomy",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f00d0>, subject='anatomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_astronomy": {
                  "task": "global_mmlu_full_en_astronomy",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f3130>, subject='astronomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_business_ethics": {
                  "task": "global_mmlu_full_en_business_ethics",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f1360>, subject='business_ethics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "task": "global_mmlu_full_en_clinical_knowledge",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f1870>, subject='clinical_knowledge')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_college_biology": {
                  "task": "global_mmlu_full_en_college_biology",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f2b90>, subject='college_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_college_chemistry": {
                  "task": "global_mmlu_full_en_college_chemistry",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d6257e20>, subject='college_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_college_computer_science": {
                  "task": "global_mmlu_full_en_college_computer_science",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f12d0>, subject='college_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_college_mathematics": {
                  "task": "global_mmlu_full_en_college_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f0ca0>, subject='college_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_college_medicine": {
                  "task": "global_mmlu_full_en_college_medicine",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d6257640>, subject='college_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_college_physics": {
                  "task": "global_mmlu_full_en_college_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f2710>, subject='college_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_computer_security": {
                  "task": "global_mmlu_full_en_computer_security",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f09d0>, subject='computer_security')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "task": "global_mmlu_full_en_conceptual_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f0430>, subject='conceptual_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_econometrics": {
                  "task": "global_mmlu_full_en_econometrics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d6256680>, subject='econometrics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "task": "global_mmlu_full_en_electrical_engineering",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d6254670>, subject='electrical_engineering')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "task": "global_mmlu_full_en_elementary_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f1fc0>, subject='elementary_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_formal_logic": {
                  "task": "global_mmlu_full_en_formal_logic",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed3e820c700>, subject='formal_logic')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_global_facts": {
                  "task": "global_mmlu_full_en_global_facts",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d6256ef0>, subject='global_facts')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_high_school_biology": {
                  "task": "global_mmlu_full_en_high_school_biology",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f2830>, subject='high_school_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "task": "global_mmlu_full_en_high_school_chemistry",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f3370>, subject='high_school_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "task": "global_mmlu_full_en_high_school_computer_science",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f3520>, subject='high_school_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "task": "global_mmlu_full_en_high_school_european_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed3e8125120>, subject='high_school_european_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_high_school_geography": {
                  "task": "global_mmlu_full_en_high_school_geography",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed3e81257e0>, subject='high_school_geography')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "task": "global_mmlu_full_en_high_school_government_and_politics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed3e81269e0>, subject='high_school_government_and_politics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "task": "global_mmlu_full_en_high_school_macroeconomics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d62555a0>, subject='high_school_macroeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "task": "global_mmlu_full_en_high_school_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f0820>, subject='high_school_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "task": "global_mmlu_full_en_high_school_microeconomics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d62569e0>, subject='high_school_microeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_high_school_physics": {
                  "task": "global_mmlu_full_en_high_school_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f2dd0>, subject='high_school_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "task": "global_mmlu_full_en_high_school_psychology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d6254700>, subject='high_school_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "task": "global_mmlu_full_en_high_school_statistics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f0280>, subject='high_school_statistics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "task": "global_mmlu_full_en_high_school_us_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed3e8126290>, subject='high_school_us_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "task": "global_mmlu_full_en_high_school_world_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed3e81249d0>, subject='high_school_world_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_human_aging": {
                  "task": "global_mmlu_full_en_human_aging",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d6255ab0>, subject='human_aging')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_human_sexuality": {
                  "task": "global_mmlu_full_en_human_sexuality",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d6255d80>, subject='human_sexuality')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_international_law": {
                  "task": "global_mmlu_full_en_international_law",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed3e81255a0>, subject='international_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_jurisprudence": {
                  "task": "global_mmlu_full_en_jurisprudence",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed3e8127400>, subject='jurisprudence')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "task": "global_mmlu_full_en_logical_fallacies",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed3e8124790>, subject='logical_fallacies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_machine_learning": {
                  "task": "global_mmlu_full_en_machine_learning",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f39a0>, subject='machine_learning')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_management": {
                  "task": "global_mmlu_full_en_management",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d6257eb0>, subject='management')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_marketing": {
                  "task": "global_mmlu_full_en_marketing",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d6254940>, subject='marketing')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_medical_genetics": {
                  "task": "global_mmlu_full_en_medical_genetics",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f0e50>, subject='medical_genetics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_miscellaneous": {
                  "task": "global_mmlu_full_en_miscellaneous",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d62549d0>, subject='miscellaneous')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_moral_disputes": {
                  "task": "global_mmlu_full_en_moral_disputes",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed3e8124dc0>, subject='moral_disputes')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "task": "global_mmlu_full_en_moral_scenarios",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed3e81279a0>, subject='moral_scenarios')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_nutrition": {
                  "task": "global_mmlu_full_en_nutrition",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d6254310>, subject='nutrition')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_philosophy": {
                  "task": "global_mmlu_full_en_philosophy",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed3eb2c4ca0>, subject='philosophy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_prehistory": {
                  "task": "global_mmlu_full_en_prehistory",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed3eb2c7f40>, subject='prehistory')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_professional_accounting": {
                  "task": "global_mmlu_full_en_professional_accounting",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d62567a0>, subject='professional_accounting')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_professional_law": {
                  "task": "global_mmlu_full_en_professional_law",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed3e8125900>, subject='professional_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_professional_medicine": {
                  "task": "global_mmlu_full_en_professional_medicine",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1a41f0670>, subject='professional_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_professional_psychology": {
                  "task": "global_mmlu_full_en_professional_psychology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d6254ee0>, subject='professional_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_public_relations": {
                  "task": "global_mmlu_full_en_public_relations",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d62540d0>, subject='public_relations')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_security_studies": {
                  "task": "global_mmlu_full_en_security_studies",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d6255e10>, subject='security_studies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_sociology": {
                  "task": "global_mmlu_full_en_sociology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d62541f0>, subject='sociology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "task": "global_mmlu_full_en_us_foreign_policy",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d6256830>, subject='us_foreign_policy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_virology": {
                  "task": "global_mmlu_full_en_virology",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed1d6257250>, subject='virology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_en_world_religions": {
                  "task": "global_mmlu_full_en_world_religions",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7ed3e8127d90>, subject='world_religions')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                }
              },
              "versions": {
                "global_mmlu_full_en": 0.0,
                "global_mmlu_full_en_abstract_algebra": 0.0,
                "global_mmlu_full_en_anatomy": 0.0,
                "global_mmlu_full_en_astronomy": 0.0,
                "global_mmlu_full_en_business_ethics": 0.0,
                "global_mmlu_full_en_clinical_knowledge": 0.0,
                "global_mmlu_full_en_college_biology": 0.0,
                "global_mmlu_full_en_college_chemistry": 0.0,
                "global_mmlu_full_en_college_computer_science": 0.0,
                "global_mmlu_full_en_college_mathematics": 0.0,
                "global_mmlu_full_en_college_medicine": 0.0,
                "global_mmlu_full_en_college_physics": 0.0,
                "global_mmlu_full_en_computer_security": 0.0,
                "global_mmlu_full_en_conceptual_physics": 0.0,
                "global_mmlu_full_en_econometrics": 0.0,
                "global_mmlu_full_en_electrical_engineering": 0.0,
                "global_mmlu_full_en_elementary_mathematics": 0.0,
                "global_mmlu_full_en_formal_logic": 0.0,
                "global_mmlu_full_en_global_facts": 0.0,
                "global_mmlu_full_en_high_school_biology": 0.0,
                "global_mmlu_full_en_high_school_chemistry": 0.0,
                "global_mmlu_full_en_high_school_computer_science": 0.0,
                "global_mmlu_full_en_high_school_european_history": 0.0,
                "global_mmlu_full_en_high_school_geography": 0.0,
                "global_mmlu_full_en_high_school_government_and_politics": 0.0,
                "global_mmlu_full_en_high_school_macroeconomics": 0.0,
                "global_mmlu_full_en_high_school_mathematics": 0.0,
                "global_mmlu_full_en_high_school_microeconomics": 0.0,
                "global_mmlu_full_en_high_school_physics": 0.0,
                "global_mmlu_full_en_high_school_psychology": 0.0,
                "global_mmlu_full_en_high_school_statistics": 0.0,
                "global_mmlu_full_en_high_school_us_history": 0.0,
                "global_mmlu_full_en_high_school_world_history": 0.0,
                "global_mmlu_full_en_human_aging": 0.0,
                "global_mmlu_full_en_human_sexuality": 0.0,
                "global_mmlu_full_en_humanities": 0.0,
                "global_mmlu_full_en_international_law": 0.0,
                "global_mmlu_full_en_jurisprudence": 0.0,
                "global_mmlu_full_en_logical_fallacies": 0.0,
                "global_mmlu_full_en_machine_learning": 0.0,
                "global_mmlu_full_en_management": 0.0,
                "global_mmlu_full_en_marketing": 0.0,
                "global_mmlu_full_en_medical_genetics": 0.0,
                "global_mmlu_full_en_miscellaneous": 0.0,
                "global_mmlu_full_en_moral_disputes": 0.0,
                "global_mmlu_full_en_moral_scenarios": 0.0,
                "global_mmlu_full_en_nutrition": 0.0,
                "global_mmlu_full_en_other": 0.0,
                "global_mmlu_full_en_philosophy": 0.0,
                "global_mmlu_full_en_prehistory": 0.0,
                "global_mmlu_full_en_professional_accounting": 0.0,
                "global_mmlu_full_en_professional_law": 0.0,
                "global_mmlu_full_en_professional_medicine": 0.0,
                "global_mmlu_full_en_professional_psychology": 0.0,
                "global_mmlu_full_en_public_relations": 0.0,
                "global_mmlu_full_en_security_studies": 0.0,
                "global_mmlu_full_en_social_sciences": 0.0,
                "global_mmlu_full_en_sociology": 0.0,
                "global_mmlu_full_en_stem": 0.0,
                "global_mmlu_full_en_us_foreign_policy": 0.0,
                "global_mmlu_full_en_virology": 0.0,
                "global_mmlu_full_en_world_religions": 0.0
              },
              "n-shot": {
                "global_mmlu_full_en_abstract_algebra": 0,
                "global_mmlu_full_en_anatomy": 0,
                "global_mmlu_full_en_astronomy": 0,
                "global_mmlu_full_en_business_ethics": 0,
                "global_mmlu_full_en_clinical_knowledge": 0,
                "global_mmlu_full_en_college_biology": 0,
                "global_mmlu_full_en_college_chemistry": 0,
                "global_mmlu_full_en_college_computer_science": 0,
                "global_mmlu_full_en_college_mathematics": 0,
                "global_mmlu_full_en_college_medicine": 0,
                "global_mmlu_full_en_college_physics": 0,
                "global_mmlu_full_en_computer_security": 0,
                "global_mmlu_full_en_conceptual_physics": 0,
                "global_mmlu_full_en_econometrics": 0,
                "global_mmlu_full_en_electrical_engineering": 0,
                "global_mmlu_full_en_elementary_mathematics": 0,
                "global_mmlu_full_en_formal_logic": 0,
                "global_mmlu_full_en_global_facts": 0,
                "global_mmlu_full_en_high_school_biology": 0,
                "global_mmlu_full_en_high_school_chemistry": 0,
                "global_mmlu_full_en_high_school_computer_science": 0,
                "global_mmlu_full_en_high_school_european_history": 0,
                "global_mmlu_full_en_high_school_geography": 0,
                "global_mmlu_full_en_high_school_government_and_politics": 0,
                "global_mmlu_full_en_high_school_macroeconomics": 0,
                "global_mmlu_full_en_high_school_mathematics": 0,
                "global_mmlu_full_en_high_school_microeconomics": 0,
                "global_mmlu_full_en_high_school_physics": 0,
                "global_mmlu_full_en_high_school_psychology": 0,
                "global_mmlu_full_en_high_school_statistics": 0,
                "global_mmlu_full_en_high_school_us_history": 0,
                "global_mmlu_full_en_high_school_world_history": 0,
                "global_mmlu_full_en_human_aging": 0,
                "global_mmlu_full_en_human_sexuality": 0,
                "global_mmlu_full_en_international_law": 0,
                "global_mmlu_full_en_jurisprudence": 0,
                "global_mmlu_full_en_logical_fallacies": 0,
                "global_mmlu_full_en_machine_learning": 0,
                "global_mmlu_full_en_management": 0,
                "global_mmlu_full_en_marketing": 0,
                "global_mmlu_full_en_medical_genetics": 0,
                "global_mmlu_full_en_miscellaneous": 0,
                "global_mmlu_full_en_moral_disputes": 0,
                "global_mmlu_full_en_moral_scenarios": 0,
                "global_mmlu_full_en_nutrition": 0,
                "global_mmlu_full_en_philosophy": 0,
                "global_mmlu_full_en_prehistory": 0,
                "global_mmlu_full_en_professional_accounting": 0,
                "global_mmlu_full_en_professional_law": 0,
                "global_mmlu_full_en_professional_medicine": 0,
                "global_mmlu_full_en_professional_psychology": 0,
                "global_mmlu_full_en_public_relations": 0,
                "global_mmlu_full_en_security_studies": 0,
                "global_mmlu_full_en_sociology": 0,
                "global_mmlu_full_en_us_foreign_policy": 0,
                "global_mmlu_full_en_virology": 0,
                "global_mmlu_full_en_world_religions": 0
              },
              "higher_is_better": {
                "global_mmlu_full_en": {
                  "acc": true
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "acc": true
                },
                "global_mmlu_full_en_anatomy": {
                  "acc": true
                },
                "global_mmlu_full_en_astronomy": {
                  "acc": true
                },
                "global_mmlu_full_en_business_ethics": {
                  "acc": true
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "acc": true
                },
                "global_mmlu_full_en_college_biology": {
                  "acc": true
                },
                "global_mmlu_full_en_college_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_en_college_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_en_college_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_college_medicine": {
                  "acc": true
                },
                "global_mmlu_full_en_college_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_computer_security": {
                  "acc": true
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_econometrics": {
                  "acc": true
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "acc": true
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_formal_logic": {
                  "acc": true
                },
                "global_mmlu_full_en_global_facts": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_biology": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_geography": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "acc": true
                },
                "global_mmlu_full_en_human_aging": {
                  "acc": true
                },
                "global_mmlu_full_en_human_sexuality": {
                  "acc": true
                },
                "global_mmlu_full_en_humanities": {
                  "acc": true
                },
                "global_mmlu_full_en_international_law": {
                  "acc": true
                },
                "global_mmlu_full_en_jurisprudence": {
                  "acc": true
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "acc": true
                },
                "global_mmlu_full_en_machine_learning": {
                  "acc": true
                },
                "global_mmlu_full_en_management": {
                  "acc": true
                },
                "global_mmlu_full_en_marketing": {
                  "acc": true
                },
                "global_mmlu_full_en_medical_genetics": {
                  "acc": true
                },
                "global_mmlu_full_en_miscellaneous": {
                  "acc": true
                },
                "global_mmlu_full_en_moral_disputes": {
                  "acc": true
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "acc": true
                },
                "global_mmlu_full_en_nutrition": {
                  "acc": true
                },
                "global_mmlu_full_en_other": {
                  "acc": true
                },
                "global_mmlu_full_en_philosophy": {
                  "acc": true
                },
                "global_mmlu_full_en_prehistory": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_accounting": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_law": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_medicine": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_psychology": {
                  "acc": true
                },
                "global_mmlu_full_en_public_relations": {
                  "acc": true
                },
                "global_mmlu_full_en_security_studies": {
                  "acc": true
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc": true
                },
                "global_mmlu_full_en_sociology": {
                  "acc": true
                },
                "global_mmlu_full_en_stem": {
                  "acc": true
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "acc": true
                },
                "global_mmlu_full_en_virology": {
                  "acc": true
                },
                "global_mmlu_full_en_world_religions": {
                  "acc": true
                }
              },
              "n-samples": {
                "global_mmlu_full_en_high_school_statistics": {
                  "original": 216,
                  "effective": 216
                },
                "global_mmlu_full_en_computer_security": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_machine_learning": {
                  "original": 112,
                  "effective": 112
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_astronomy": {
                  "original": 152,
                  "effective": 152
                },
                "global_mmlu_full_en_college_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_high_school_biology": {
                  "original": 310,
                  "effective": 310
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "original": 203,
                  "effective": 203
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "original": 378,
                  "effective": 378
                },
                "global_mmlu_full_en_college_physics": {
                  "original": 102,
                  "effective": 102
                },
                "global_mmlu_full_en_high_school_physics": {
                  "original": 151,
                  "effective": 151
                },
                "global_mmlu_full_en_college_chemistry": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_college_biology": {
                  "original": 144,
                  "effective": 144
                },
                "global_mmlu_full_en_college_mathematics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "original": 235,
                  "effective": 235
                },
                "global_mmlu_full_en_anatomy": {
                  "original": 135,
                  "effective": 135
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "original": 270,
                  "effective": 270
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "original": 145,
                  "effective": 145
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "original": 265,
                  "effective": 265
                },
                "global_mmlu_full_en_medical_genetics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_professional_medicine": {
                  "original": 272,
                  "effective": 272
                },
                "global_mmlu_full_en_business_ethics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_marketing": {
                  "original": 234,
                  "effective": 234
                },
                "global_mmlu_full_en_global_facts": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_college_medicine": {
                  "original": 173,
                  "effective": 173
                },
                "global_mmlu_full_en_human_aging": {
                  "original": 223,
                  "effective": 223
                },
                "global_mmlu_full_en_management": {
                  "original": 103,
                  "effective": 103
                },
                "global_mmlu_full_en_nutrition": {
                  "original": 306,
                  "effective": 306
                },
                "global_mmlu_full_en_virology": {
                  "original": 166,
                  "effective": 166
                },
                "global_mmlu_full_en_professional_accounting": {
                  "original": 282,
                  "effective": 282
                },
                "global_mmlu_full_en_miscellaneous": {
                  "original": 783,
                  "effective": 783
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_public_relations": {
                  "original": 110,
                  "effective": 110
                },
                "global_mmlu_full_en_econometrics": {
                  "original": 114,
                  "effective": 114
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "original": 238,
                  "effective": 238
                },
                "global_mmlu_full_en_human_sexuality": {
                  "original": 131,
                  "effective": 131
                },
                "global_mmlu_full_en_security_studies": {
                  "original": 245,
                  "effective": 245
                },
                "global_mmlu_full_en_professional_psychology": {
                  "original": 612,
                  "effective": 612
                },
                "global_mmlu_full_en_high_school_geography": {
                  "original": 198,
                  "effective": 198
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "original": 193,
                  "effective": 193
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "original": 545,
                  "effective": 545
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "original": 390,
                  "effective": 390
                },
                "global_mmlu_full_en_sociology": {
                  "original": 201,
                  "effective": 201
                },
                "global_mmlu_full_en_world_religions": {
                  "original": 171,
                  "effective": 171
                },
                "global_mmlu_full_en_international_law": {
                  "original": 121,
                  "effective": 121
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "original": 895,
                  "effective": 895
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "original": 204,
                  "effective": 204
                },
                "global_mmlu_full_en_jurisprudence": {
                  "original": 108,
                  "effective": 108
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "original": 237,
                  "effective": 237
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "original": 163,
                  "effective": 163
                },
                "global_mmlu_full_en_philosophy": {
                  "original": 311,
                  "effective": 311
                },
                "global_mmlu_full_en_professional_law": {
                  "original": 1534,
                  "effective": 1534
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "original": 165,
                  "effective": 165
                },
                "global_mmlu_full_en_formal_logic": {
                  "original": 126,
                  "effective": 126
                },
                "global_mmlu_full_en_moral_disputes": {
                  "original": 346,
                  "effective": 346
                },
                "global_mmlu_full_en_prehistory": {
                  "original": 324,
                  "effective": 324
                }
              },
              "config": {
                "model": "hf",
                "model_args": "pretrained=GermanT5/t5-efficient-gc4-german-base-nl36,trust_remote_code=True",
                "model_num_parameters": 619357440,
                "model_dtype": "torch.float32",
                "model_revision": "main",
                "model_sha": "471c7616ac14cb86f0c177716315a92c26b57504",
                "batch_size": "auto",
                "batch_sizes": [
                  64
                ],
                "device": "cuda",
                "use_cache": null,
                "limit": null,
                "bootstrap_iters": 100000,
                "gen_kwargs": null,
                "random_seed": 42,
                "numpy_seed": 42,
                "torch_seed": 42,
                "fewshot_seed": 42
              },
              "git_hash": "a4103c7",
              "date": 1758646945.3604379,
              "pretty_env_info": "PyTorch version: 2.7.1+cu126\nIs debug build: False\nCUDA used to build PyTorch: 12.6\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.2 LTS (x86_64)\nGCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\nClang version: Could not collect\nCMake version: version 3.24.1\nLibc version: glibc-2.35\n\nPython version: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] (64-bit runtime)\nPython platform: Linux-6.8.0-1036-nvidia-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H200\nNvidia driver version: 580.82.07\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               224\nOn-line CPU(s) list:                  0-223\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480C\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   56\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          3800.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            5.3 MiB (112 instances)\nL1i cache:                            3.5 MiB (112 instances)\nL2 cache:                             224 MiB (112 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-55,112-167\nNUMA node1 CPU(s):                    56-111,168-223\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pytorch-lightning==2.5.2\n[pip3] torch==2.7.1\n[pip3] torchmetrics==1.8.0\n[pip3] triton==3.3.1\n[conda] Could not collect",
              "transformers_version": "4.54.1",
              "lm_eval_version": "0.4.9",
              "upper_git_hash": null,
              "tokenizer_pad_token": [
                "<pad>",
                "0"
              ],
              "tokenizer_eos_token": [
                "</s>",
                "1"
              ],
              "tokenizer_bos_token": [
                null,
                "None"
              ],
              "eot_token_id": 1,
              "max_length": 512,
              "task_hashes": {},
              "model_source": "hf",
              "model_name": "GermanT5/t5-efficient-gc4-german-base-nl36",
              "model_name_sanitized": "GermanT5__t5-efficient-gc4-german-base-nl36",
              "system_instruction": null,
              "system_instruction_sha": null,
              "fewshot_as_multiturn": false,
              "chat_template": null,
              "chat_template_sha": null,
              "start_time": 633091.745828648,
              "end_time": 633807.229614301,
              "total_evaluation_time_seconds": "715.4837856530212"
            },
            "duration_seconds": 763.8060276508331,
            "duration_minutes": 12.730100460847218,
            "command": "python -m lm_eval --model hf --model_args pretrained=GermanT5/t5-efficient-gc4-german-base-nl36 --tasks global_mmlu_full_en --device cuda --batch_size auto --output_path /netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/hf-germanT5-base_global_mmlu_en_0shot_20250923_190053 --num_fewshot 0 --trust_remote_code --seed 42 --wandb_args project=BA-T5-CrossLingual,entity=nikolas-rauscher-dfki,group=crosslingual_transfer_eval_full_15k-final,name=hf-germanT5-base_global_mmlu_en_0shot",
            "output_dir": "/netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/hf-germanT5-base_global_mmlu_en_0shot_20250923_190053",
            "status": "success"
          }
        },
        "global_mmlu_de": {
          "0_shot": {
            "parse_status": "partial",
            "error": "Could not find primary metric",
            "raw_results": {
              "results": {
                "global_mmlu_full_de": {
                  "acc,none": 0.22945449366187154,
                  "acc_stderr,none": 0.0035426913692823605,
                  "alias": "global_mmlu_full_de"
                },
                "global_mmlu_full_de_humanities": {
                  "acc,none": 0.24208289054197663,
                  "acc_stderr,none": 0.0062426684031394305,
                  "alias": " - global_mmlu_full_de_humanities"
                },
                "global_mmlu_full_de_formal_logic": {
                  "alias": "  - global_mmlu_full_de_formal_logic",
                  "acc,none": 0.2857142857142857,
                  "acc_stderr,none": 0.04040610178208841
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "alias": "  - global_mmlu_full_de_high_school_european_history",
                  "acc,none": 0.21818181818181817,
                  "acc_stderr,none": 0.03225078108306289
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "alias": "  - global_mmlu_full_de_high_school_us_history",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.03039153369274154
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "alias": "  - global_mmlu_full_de_high_school_world_history",
                  "acc,none": 0.270042194092827,
                  "acc_stderr,none": 0.028900721906293426
                },
                "global_mmlu_full_de_international_law": {
                  "alias": "  - global_mmlu_full_de_international_law",
                  "acc,none": 0.2396694214876033,
                  "acc_stderr,none": 0.03896878985070417
                },
                "global_mmlu_full_de_jurisprudence": {
                  "alias": "  - global_mmlu_full_de_jurisprudence",
                  "acc,none": 0.25925925925925924,
                  "acc_stderr,none": 0.04236511258094634
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "alias": "  - global_mmlu_full_de_logical_fallacies",
                  "acc,none": 0.22085889570552147,
                  "acc_stderr,none": 0.032591773927421776
                },
                "global_mmlu_full_de_moral_disputes": {
                  "alias": "  - global_mmlu_full_de_moral_disputes",
                  "acc,none": 0.24855491329479767,
                  "acc_stderr,none": 0.023267528432100174
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "alias": "  - global_mmlu_full_de_moral_scenarios",
                  "acc,none": 0.23798882681564246,
                  "acc_stderr,none": 0.014242630070574885
                },
                "global_mmlu_full_de_philosophy": {
                  "alias": "  - global_mmlu_full_de_philosophy",
                  "acc,none": 0.1864951768488746,
                  "acc_stderr,none": 0.02212243977248077
                },
                "global_mmlu_full_de_prehistory": {
                  "alias": "  - global_mmlu_full_de_prehistory",
                  "acc,none": 0.21604938271604937,
                  "acc_stderr,none": 0.022899162918445813
                },
                "global_mmlu_full_de_professional_law": {
                  "alias": "  - global_mmlu_full_de_professional_law",
                  "acc,none": 0.2457627118644068,
                  "acc_stderr,none": 0.01099615663514269
                },
                "global_mmlu_full_de_world_religions": {
                  "alias": "  - global_mmlu_full_de_world_religions",
                  "acc,none": 0.3216374269005848,
                  "acc_stderr,none": 0.03582529442573122
                },
                "global_mmlu_full_de_other": {
                  "acc,none": 0.23978113936272932,
                  "acc_stderr,none": 0.00764225029165751,
                  "alias": " - global_mmlu_full_de_other"
                },
                "global_mmlu_full_de_business_ethics": {
                  "alias": "  - global_mmlu_full_de_business_ethics",
                  "acc,none": 0.3,
                  "acc_stderr,none": 0.046056618647183814
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "alias": "  - global_mmlu_full_de_clinical_knowledge",
                  "acc,none": 0.21509433962264152,
                  "acc_stderr,none": 0.025288394502891377
                },
                "global_mmlu_full_de_college_medicine": {
                  "alias": "  - global_mmlu_full_de_college_medicine",
                  "acc,none": 0.20809248554913296,
                  "acc_stderr,none": 0.030952890217749884
                },
                "global_mmlu_full_de_global_facts": {
                  "alias": "  - global_mmlu_full_de_global_facts",
                  "acc,none": 0.18,
                  "acc_stderr,none": 0.038612291966536955
                },
                "global_mmlu_full_de_human_aging": {
                  "alias": "  - global_mmlu_full_de_human_aging",
                  "acc,none": 0.31390134529147984,
                  "acc_stderr,none": 0.03114679648297246
                },
                "global_mmlu_full_de_management": {
                  "alias": "  - global_mmlu_full_de_management",
                  "acc,none": 0.17475728155339806,
                  "acc_stderr,none": 0.03760178006026621
                },
                "global_mmlu_full_de_marketing": {
                  "alias": "  - global_mmlu_full_de_marketing",
                  "acc,none": 0.2905982905982906,
                  "acc_stderr,none": 0.029745048572674057
                },
                "global_mmlu_full_de_medical_genetics": {
                  "alias": "  - global_mmlu_full_de_medical_genetics",
                  "acc,none": 0.3,
                  "acc_stderr,none": 0.046056618647183814
                },
                "global_mmlu_full_de_miscellaneous": {
                  "alias": "  - global_mmlu_full_de_miscellaneous",
                  "acc,none": 0.23754789272030652,
                  "acc_stderr,none": 0.015218733046150195
                },
                "global_mmlu_full_de_nutrition": {
                  "alias": "  - global_mmlu_full_de_nutrition",
                  "acc,none": 0.22549019607843138,
                  "acc_stderr,none": 0.023929155517351284
                },
                "global_mmlu_full_de_professional_accounting": {
                  "alias": "  - global_mmlu_full_de_professional_accounting",
                  "acc,none": 0.23404255319148937,
                  "acc_stderr,none": 0.025257861359432407
                },
                "global_mmlu_full_de_professional_medicine": {
                  "alias": "  - global_mmlu_full_de_professional_medicine",
                  "acc,none": 0.18382352941176472,
                  "acc_stderr,none": 0.02352924218519311
                },
                "global_mmlu_full_de_virology": {
                  "alias": "  - global_mmlu_full_de_virology",
                  "acc,none": 0.28313253012048195,
                  "acc_stderr,none": 0.03507295431370518
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc,none": 0.2170945726356841,
                  "acc_stderr,none": 0.007428786285788536,
                  "alias": " - global_mmlu_full_de_social_sciences"
                },
                "global_mmlu_full_de_econometrics": {
                  "alias": "  - global_mmlu_full_de_econometrics",
                  "acc,none": 0.23684210526315788,
                  "acc_stderr,none": 0.039994238792813386
                },
                "global_mmlu_full_de_high_school_geography": {
                  "alias": "  - global_mmlu_full_de_high_school_geography",
                  "acc,none": 0.17676767676767677,
                  "acc_stderr,none": 0.027178752639044915
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "alias": "  - global_mmlu_full_de_high_school_government_and_politics",
                  "acc,none": 0.19689119170984457,
                  "acc_stderr,none": 0.02869787397186069
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "alias": "  - global_mmlu_full_de_high_school_macroeconomics",
                  "acc,none": 0.20256410256410257,
                  "acc_stderr,none": 0.020377660970371397
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "alias": "  - global_mmlu_full_de_high_school_microeconomics",
                  "acc,none": 0.21008403361344538,
                  "acc_stderr,none": 0.026461398717471874
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "alias": "  - global_mmlu_full_de_high_school_psychology",
                  "acc,none": 0.1926605504587156,
                  "acc_stderr,none": 0.016909276884936073
                },
                "global_mmlu_full_de_human_sexuality": {
                  "alias": "  - global_mmlu_full_de_human_sexuality",
                  "acc,none": 0.2595419847328244,
                  "acc_stderr,none": 0.03844876139785271
                },
                "global_mmlu_full_de_professional_psychology": {
                  "alias": "  - global_mmlu_full_de_professional_psychology",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.01751781884501444
                },
                "global_mmlu_full_de_public_relations": {
                  "alias": "  - global_mmlu_full_de_public_relations",
                  "acc,none": 0.21818181818181817,
                  "acc_stderr,none": 0.03955932861795833
                },
                "global_mmlu_full_de_security_studies": {
                  "alias": "  - global_mmlu_full_de_security_studies",
                  "acc,none": 0.18775510204081633,
                  "acc_stderr,none": 0.02500025603954622
                },
                "global_mmlu_full_de_sociology": {
                  "alias": "  - global_mmlu_full_de_sociology",
                  "acc,none": 0.24378109452736318,
                  "acc_stderr,none": 0.030360490154014652
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "alias": "  - global_mmlu_full_de_us_foreign_policy",
                  "acc,none": 0.28,
                  "acc_stderr,none": 0.045126085985421276
                },
                "global_mmlu_full_de_stem": {
                  "acc,none": 0.21249603552172533,
                  "acc_stderr,none": 0.007271218700485502,
                  "alias": " - global_mmlu_full_de_stem"
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "alias": "  - global_mmlu_full_de_abstract_algebra",
                  "acc,none": 0.22,
                  "acc_stderr,none": 0.04163331998932269
                },
                "global_mmlu_full_de_anatomy": {
                  "alias": "  - global_mmlu_full_de_anatomy",
                  "acc,none": 0.18518518518518517,
                  "acc_stderr,none": 0.03355677216313142
                },
                "global_mmlu_full_de_astronomy": {
                  "alias": "  - global_mmlu_full_de_astronomy",
                  "acc,none": 0.17763157894736842,
                  "acc_stderr,none": 0.031103182383123398
                },
                "global_mmlu_full_de_college_biology": {
                  "alias": "  - global_mmlu_full_de_college_biology",
                  "acc,none": 0.2569444444444444,
                  "acc_stderr,none": 0.03653946969442099
                },
                "global_mmlu_full_de_college_chemistry": {
                  "alias": "  - global_mmlu_full_de_college_chemistry",
                  "acc,none": 0.2,
                  "acc_stderr,none": 0.040201512610368445
                },
                "global_mmlu_full_de_college_computer_science": {
                  "alias": "  - global_mmlu_full_de_college_computer_science",
                  "acc,none": 0.26,
                  "acc_stderr,none": 0.044084400227680794
                },
                "global_mmlu_full_de_college_mathematics": {
                  "alias": "  - global_mmlu_full_de_college_mathematics",
                  "acc,none": 0.21,
                  "acc_stderr,none": 0.040936018074033256
                },
                "global_mmlu_full_de_college_physics": {
                  "alias": "  - global_mmlu_full_de_college_physics",
                  "acc,none": 0.21568627450980393,
                  "acc_stderr,none": 0.040925639582376556
                },
                "global_mmlu_full_de_computer_security": {
                  "alias": "  - global_mmlu_full_de_computer_security",
                  "acc,none": 0.28,
                  "acc_stderr,none": 0.045126085985421276
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "alias": "  - global_mmlu_full_de_conceptual_physics",
                  "acc,none": 0.26382978723404255,
                  "acc_stderr,none": 0.02880998985410298
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "alias": "  - global_mmlu_full_de_electrical_engineering",
                  "acc,none": 0.2413793103448276,
                  "acc_stderr,none": 0.03565998174135302
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "alias": "  - global_mmlu_full_de_elementary_mathematics",
                  "acc,none": 0.20899470899470898,
                  "acc_stderr,none": 0.020940481565334835
                },
                "global_mmlu_full_de_high_school_biology": {
                  "alias": "  - global_mmlu_full_de_high_school_biology",
                  "acc,none": 0.1774193548387097,
                  "acc_stderr,none": 0.021732540689329265
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "alias": "  - global_mmlu_full_de_high_school_chemistry",
                  "acc,none": 0.15270935960591134,
                  "acc_stderr,none": 0.025308904539380624
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "alias": "  - global_mmlu_full_de_high_school_computer_science",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.04351941398892446
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "alias": "  - global_mmlu_full_de_high_school_mathematics",
                  "acc,none": 0.2111111111111111,
                  "acc_stderr,none": 0.02488211685765508
                },
                "global_mmlu_full_de_high_school_physics": {
                  "alias": "  - global_mmlu_full_de_high_school_physics",
                  "acc,none": 0.1986754966887417,
                  "acc_stderr,none": 0.032578473844367746
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "alias": "  - global_mmlu_full_de_high_school_statistics",
                  "acc,none": 0.1527777777777778,
                  "acc_stderr,none": 0.02453632602613422
                },
                "global_mmlu_full_de_machine_learning": {
                  "alias": "  - global_mmlu_full_de_machine_learning",
                  "acc,none": 0.3125,
                  "acc_stderr,none": 0.043994650575715215
                }
              },
              "groups": {
                "global_mmlu_full_de": {
                  "acc,none": 0.22945449366187154,
                  "acc_stderr,none": 0.0035426913692823605,
                  "alias": "global_mmlu_full_de"
                },
                "global_mmlu_full_de_humanities": {
                  "acc,none": 0.24208289054197663,
                  "acc_stderr,none": 0.0062426684031394305,
                  "alias": " - global_mmlu_full_de_humanities"
                },
                "global_mmlu_full_de_other": {
                  "acc,none": 0.23978113936272932,
                  "acc_stderr,none": 0.00764225029165751,
                  "alias": " - global_mmlu_full_de_other"
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc,none": 0.2170945726356841,
                  "acc_stderr,none": 0.007428786285788536,
                  "alias": " - global_mmlu_full_de_social_sciences"
                },
                "global_mmlu_full_de_stem": {
                  "acc,none": 0.21249603552172533,
                  "acc_stderr,none": 0.007271218700485502,
                  "alias": " - global_mmlu_full_de_stem"
                }
              },
              "group_subtasks": {
                "global_mmlu_full_de_humanities": [
                  "global_mmlu_full_de_professional_law",
                  "global_mmlu_full_de_international_law",
                  "global_mmlu_full_de_prehistory",
                  "global_mmlu_full_de_world_religions",
                  "global_mmlu_full_de_high_school_world_history",
                  "global_mmlu_full_de_high_school_us_history",
                  "global_mmlu_full_de_jurisprudence",
                  "global_mmlu_full_de_formal_logic",
                  "global_mmlu_full_de_logical_fallacies",
                  "global_mmlu_full_de_high_school_european_history",
                  "global_mmlu_full_de_philosophy",
                  "global_mmlu_full_de_moral_scenarios",
                  "global_mmlu_full_de_moral_disputes"
                ],
                "global_mmlu_full_de_social_sciences": [
                  "global_mmlu_full_de_high_school_psychology",
                  "global_mmlu_full_de_professional_psychology",
                  "global_mmlu_full_de_high_school_government_and_politics",
                  "global_mmlu_full_de_high_school_macroeconomics",
                  "global_mmlu_full_de_high_school_microeconomics",
                  "global_mmlu_full_de_econometrics",
                  "global_mmlu_full_de_high_school_geography",
                  "global_mmlu_full_de_security_studies",
                  "global_mmlu_full_de_us_foreign_policy",
                  "global_mmlu_full_de_public_relations",
                  "global_mmlu_full_de_human_sexuality",
                  "global_mmlu_full_de_sociology"
                ],
                "global_mmlu_full_de_other": [
                  "global_mmlu_full_de_nutrition",
                  "global_mmlu_full_de_professional_medicine",
                  "global_mmlu_full_de_college_medicine",
                  "global_mmlu_full_de_professional_accounting",
                  "global_mmlu_full_de_marketing",
                  "global_mmlu_full_de_miscellaneous",
                  "global_mmlu_full_de_business_ethics",
                  "global_mmlu_full_de_virology",
                  "global_mmlu_full_de_medical_genetics",
                  "global_mmlu_full_de_human_aging",
                  "global_mmlu_full_de_global_facts",
                  "global_mmlu_full_de_clinical_knowledge",
                  "global_mmlu_full_de_management"
                ],
                "global_mmlu_full_de_stem": [
                  "global_mmlu_full_de_computer_security",
                  "global_mmlu_full_de_college_biology",
                  "global_mmlu_full_de_college_mathematics",
                  "global_mmlu_full_de_conceptual_physics",
                  "global_mmlu_full_de_elementary_mathematics",
                  "global_mmlu_full_de_astronomy",
                  "global_mmlu_full_de_high_school_physics",
                  "global_mmlu_full_de_machine_learning",
                  "global_mmlu_full_de_college_chemistry",
                  "global_mmlu_full_de_high_school_chemistry",
                  "global_mmlu_full_de_college_computer_science",
                  "global_mmlu_full_de_high_school_computer_science",
                  "global_mmlu_full_de_high_school_mathematics",
                  "global_mmlu_full_de_abstract_algebra",
                  "global_mmlu_full_de_high_school_biology",
                  "global_mmlu_full_de_electrical_engineering",
                  "global_mmlu_full_de_college_physics",
                  "global_mmlu_full_de_anatomy",
                  "global_mmlu_full_de_high_school_statistics"
                ],
                "global_mmlu_full_de": [
                  "global_mmlu_full_de_stem",
                  "global_mmlu_full_de_other",
                  "global_mmlu_full_de_social_sciences",
                  "global_mmlu_full_de_humanities"
                ]
              },
              "configs": {
                "global_mmlu_full_de_abstract_algebra": {
                  "task": "global_mmlu_full_de_abstract_algebra",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901481a9e0>, subject='abstract_algebra')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_anatomy": {
                  "task": "global_mmlu_full_de_anatomy",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901481a0e0>, subject='anatomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_astronomy": {
                  "task": "global_mmlu_full_de_astronomy",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x759014819a20>, subject='astronomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_business_ethics": {
                  "task": "global_mmlu_full_de_business_ethics",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c3bf9a0>, subject='business_ethics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "task": "global_mmlu_full_de_clinical_knowledge",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c3be7a0>, subject='clinical_knowledge')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_college_biology": {
                  "task": "global_mmlu_full_de_college_biology",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901481ac20>, subject='college_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_college_chemistry": {
                  "task": "global_mmlu_full_de_college_chemistry",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7590148181f0>, subject='college_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_college_computer_science": {
                  "task": "global_mmlu_full_de_college_computer_science",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x759014818310>, subject='college_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_college_mathematics": {
                  "task": "global_mmlu_full_de_college_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c195510>, subject='college_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_college_medicine": {
                  "task": "global_mmlu_full_de_college_medicine",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c3bcdc0>, subject='college_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_college_physics": {
                  "task": "global_mmlu_full_de_college_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901481a320>, subject='college_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_computer_security": {
                  "task": "global_mmlu_full_de_computer_security",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901481bb50>, subject='computer_security')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "task": "global_mmlu_full_de_conceptual_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c195360>, subject='conceptual_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_econometrics": {
                  "task": "global_mmlu_full_de_econometrics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c3bd750>, subject='econometrics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "task": "global_mmlu_full_de_electrical_engineering",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x759014818550>, subject='electrical_engineering')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "task": "global_mmlu_full_de_elementary_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901481b760>, subject='elementary_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_formal_logic": {
                  "task": "global_mmlu_full_de_formal_logic",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f419ff40>, subject='formal_logic')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_global_facts": {
                  "task": "global_mmlu_full_de_global_facts",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c3bff40>, subject='global_facts')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_high_school_biology": {
                  "task": "global_mmlu_full_de_high_school_biology",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x759014818790>, subject='high_school_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "task": "global_mmlu_full_de_high_school_chemistry",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901481b7f0>, subject='high_school_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "task": "global_mmlu_full_de_high_school_computer_science",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901481a290>, subject='high_school_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "task": "global_mmlu_full_de_high_school_european_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f4619000>, subject='high_school_european_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_high_school_geography": {
                  "task": "global_mmlu_full_de_high_school_geography",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c3bcc10>, subject='high_school_geography')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "task": "global_mmlu_full_de_high_school_government_and_politics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c3becb0>, subject='high_school_government_and_politics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "task": "global_mmlu_full_de_high_school_macroeconomics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f4619900>, subject='high_school_macroeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "task": "global_mmlu_full_de_high_school_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901481b130>, subject='high_school_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "task": "global_mmlu_full_de_high_school_microeconomics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c3bc790>, subject='high_school_microeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_high_school_physics": {
                  "task": "global_mmlu_full_de_high_school_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x759014818dc0>, subject='high_school_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "task": "global_mmlu_full_de_high_school_psychology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c3bf010>, subject='high_school_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "task": "global_mmlu_full_de_high_school_statistics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c3be290>, subject='high_school_statistics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "task": "global_mmlu_full_de_high_school_us_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f4619480>, subject='high_school_us_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "task": "global_mmlu_full_de_high_school_world_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f461a830>, subject='high_school_world_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_human_aging": {
                  "task": "global_mmlu_full_de_human_aging",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c3bf880>, subject='human_aging')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_human_sexuality": {
                  "task": "global_mmlu_full_de_human_sexuality",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f4618c10>, subject='human_sexuality')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_international_law": {
                  "task": "global_mmlu_full_de_international_law",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f46197e0>, subject='international_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_jurisprudence": {
                  "task": "global_mmlu_full_de_jurisprudence",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f4618d30>, subject='jurisprudence')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "task": "global_mmlu_full_de_logical_fallacies",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f461a3b0>, subject='logical_fallacies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_machine_learning": {
                  "task": "global_mmlu_full_de_machine_learning",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901481bd90>, subject='machine_learning')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_management": {
                  "task": "global_mmlu_full_de_management",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c3bf5b0>, subject='management')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_marketing": {
                  "task": "global_mmlu_full_de_marketing",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c3bde10>, subject='marketing')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_medical_genetics": {
                  "task": "global_mmlu_full_de_medical_genetics",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c3beef0>, subject='medical_genetics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_miscellaneous": {
                  "task": "global_mmlu_full_de_miscellaneous",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c3bef80>, subject='miscellaneous')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_moral_disputes": {
                  "task": "global_mmlu_full_de_moral_disputes",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f419feb0>, subject='moral_disputes')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "task": "global_mmlu_full_de_moral_scenarios",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f4a4c700>, subject='moral_scenarios')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_nutrition": {
                  "task": "global_mmlu_full_de_nutrition",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x759014818040>, subject='nutrition')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_philosophy": {
                  "task": "global_mmlu_full_de_philosophy",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f4650430>, subject='philosophy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_prehistory": {
                  "task": "global_mmlu_full_de_prehistory",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f461bd00>, subject='prehistory')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_professional_accounting": {
                  "task": "global_mmlu_full_de_professional_accounting",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x759014819120>, subject='professional_accounting')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_professional_law": {
                  "task": "global_mmlu_full_de_professional_law",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f461add0>, subject='professional_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_professional_medicine": {
                  "task": "global_mmlu_full_de_professional_medicine",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7590148196c0>, subject='professional_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_professional_psychology": {
                  "task": "global_mmlu_full_de_professional_psychology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c3bc9d0>, subject='professional_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_public_relations": {
                  "task": "global_mmlu_full_de_public_relations",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f46189d0>, subject='public_relations')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_security_studies": {
                  "task": "global_mmlu_full_de_security_studies",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f4619750>, subject='security_studies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_sociology": {
                  "task": "global_mmlu_full_de_sociology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f4618b80>, subject='sociology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "task": "global_mmlu_full_de_us_foreign_policy",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f461bac0>, subject='us_foreign_policy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_virology": {
                  "task": "global_mmlu_full_de_virology",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x75901c3be560>, subject='virology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                },
                "global_mmlu_full_de_world_religions": {
                  "task": "global_mmlu_full_de_world_religions",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7592f461b9a0>, subject='world_religions')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "GermanT5/t5-efficient-gc4-german-base-nl36"
                  }
                }
              },
              "versions": {
                "global_mmlu_full_de": 0.0,
                "global_mmlu_full_de_abstract_algebra": 0.0,
                "global_mmlu_full_de_anatomy": 0.0,
                "global_mmlu_full_de_astronomy": 0.0,
                "global_mmlu_full_de_business_ethics": 0.0,
                "global_mmlu_full_de_clinical_knowledge": 0.0,
                "global_mmlu_full_de_college_biology": 0.0,
                "global_mmlu_full_de_college_chemistry": 0.0,
                "global_mmlu_full_de_college_computer_science": 0.0,
                "global_mmlu_full_de_college_mathematics": 0.0,
                "global_mmlu_full_de_college_medicine": 0.0,
                "global_mmlu_full_de_college_physics": 0.0,
                "global_mmlu_full_de_computer_security": 0.0,
                "global_mmlu_full_de_conceptual_physics": 0.0,
                "global_mmlu_full_de_econometrics": 0.0,
                "global_mmlu_full_de_electrical_engineering": 0.0,
                "global_mmlu_full_de_elementary_mathematics": 0.0,
                "global_mmlu_full_de_formal_logic": 0.0,
                "global_mmlu_full_de_global_facts": 0.0,
                "global_mmlu_full_de_high_school_biology": 0.0,
                "global_mmlu_full_de_high_school_chemistry": 0.0,
                "global_mmlu_full_de_high_school_computer_science": 0.0,
                "global_mmlu_full_de_high_school_european_history": 0.0,
                "global_mmlu_full_de_high_school_geography": 0.0,
                "global_mmlu_full_de_high_school_government_and_politics": 0.0,
                "global_mmlu_full_de_high_school_macroeconomics": 0.0,
                "global_mmlu_full_de_high_school_mathematics": 0.0,
                "global_mmlu_full_de_high_school_microeconomics": 0.0,
                "global_mmlu_full_de_high_school_physics": 0.0,
                "global_mmlu_full_de_high_school_psychology": 0.0,
                "global_mmlu_full_de_high_school_statistics": 0.0,
                "global_mmlu_full_de_high_school_us_history": 0.0,
                "global_mmlu_full_de_high_school_world_history": 0.0,
                "global_mmlu_full_de_human_aging": 0.0,
                "global_mmlu_full_de_human_sexuality": 0.0,
                "global_mmlu_full_de_humanities": 0.0,
                "global_mmlu_full_de_international_law": 0.0,
                "global_mmlu_full_de_jurisprudence": 0.0,
                "global_mmlu_full_de_logical_fallacies": 0.0,
                "global_mmlu_full_de_machine_learning": 0.0,
                "global_mmlu_full_de_management": 0.0,
                "global_mmlu_full_de_marketing": 0.0,
                "global_mmlu_full_de_medical_genetics": 0.0,
                "global_mmlu_full_de_miscellaneous": 0.0,
                "global_mmlu_full_de_moral_disputes": 0.0,
                "global_mmlu_full_de_moral_scenarios": 0.0,
                "global_mmlu_full_de_nutrition": 0.0,
                "global_mmlu_full_de_other": 0.0,
                "global_mmlu_full_de_philosophy": 0.0,
                "global_mmlu_full_de_prehistory": 0.0,
                "global_mmlu_full_de_professional_accounting": 0.0,
                "global_mmlu_full_de_professional_law": 0.0,
                "global_mmlu_full_de_professional_medicine": 0.0,
                "global_mmlu_full_de_professional_psychology": 0.0,
                "global_mmlu_full_de_public_relations": 0.0,
                "global_mmlu_full_de_security_studies": 0.0,
                "global_mmlu_full_de_social_sciences": 0.0,
                "global_mmlu_full_de_sociology": 0.0,
                "global_mmlu_full_de_stem": 0.0,
                "global_mmlu_full_de_us_foreign_policy": 0.0,
                "global_mmlu_full_de_virology": 0.0,
                "global_mmlu_full_de_world_religions": 0.0
              },
              "n-shot": {
                "global_mmlu_full_de_abstract_algebra": 0,
                "global_mmlu_full_de_anatomy": 0,
                "global_mmlu_full_de_astronomy": 0,
                "global_mmlu_full_de_business_ethics": 0,
                "global_mmlu_full_de_clinical_knowledge": 0,
                "global_mmlu_full_de_college_biology": 0,
                "global_mmlu_full_de_college_chemistry": 0,
                "global_mmlu_full_de_college_computer_science": 0,
                "global_mmlu_full_de_college_mathematics": 0,
                "global_mmlu_full_de_college_medicine": 0,
                "global_mmlu_full_de_college_physics": 0,
                "global_mmlu_full_de_computer_security": 0,
                "global_mmlu_full_de_conceptual_physics": 0,
                "global_mmlu_full_de_econometrics": 0,
                "global_mmlu_full_de_electrical_engineering": 0,
                "global_mmlu_full_de_elementary_mathematics": 0,
                "global_mmlu_full_de_formal_logic": 0,
                "global_mmlu_full_de_global_facts": 0,
                "global_mmlu_full_de_high_school_biology": 0,
                "global_mmlu_full_de_high_school_chemistry": 0,
                "global_mmlu_full_de_high_school_computer_science": 0,
                "global_mmlu_full_de_high_school_european_history": 0,
                "global_mmlu_full_de_high_school_geography": 0,
                "global_mmlu_full_de_high_school_government_and_politics": 0,
                "global_mmlu_full_de_high_school_macroeconomics": 0,
                "global_mmlu_full_de_high_school_mathematics": 0,
                "global_mmlu_full_de_high_school_microeconomics": 0,
                "global_mmlu_full_de_high_school_physics": 0,
                "global_mmlu_full_de_high_school_psychology": 0,
                "global_mmlu_full_de_high_school_statistics": 0,
                "global_mmlu_full_de_high_school_us_history": 0,
                "global_mmlu_full_de_high_school_world_history": 0,
                "global_mmlu_full_de_human_aging": 0,
                "global_mmlu_full_de_human_sexuality": 0,
                "global_mmlu_full_de_international_law": 0,
                "global_mmlu_full_de_jurisprudence": 0,
                "global_mmlu_full_de_logical_fallacies": 0,
                "global_mmlu_full_de_machine_learning": 0,
                "global_mmlu_full_de_management": 0,
                "global_mmlu_full_de_marketing": 0,
                "global_mmlu_full_de_medical_genetics": 0,
                "global_mmlu_full_de_miscellaneous": 0,
                "global_mmlu_full_de_moral_disputes": 0,
                "global_mmlu_full_de_moral_scenarios": 0,
                "global_mmlu_full_de_nutrition": 0,
                "global_mmlu_full_de_philosophy": 0,
                "global_mmlu_full_de_prehistory": 0,
                "global_mmlu_full_de_professional_accounting": 0,
                "global_mmlu_full_de_professional_law": 0,
                "global_mmlu_full_de_professional_medicine": 0,
                "global_mmlu_full_de_professional_psychology": 0,
                "global_mmlu_full_de_public_relations": 0,
                "global_mmlu_full_de_security_studies": 0,
                "global_mmlu_full_de_sociology": 0,
                "global_mmlu_full_de_us_foreign_policy": 0,
                "global_mmlu_full_de_virology": 0,
                "global_mmlu_full_de_world_religions": 0
              },
              "higher_is_better": {
                "global_mmlu_full_de": {
                  "acc": true
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "acc": true
                },
                "global_mmlu_full_de_anatomy": {
                  "acc": true
                },
                "global_mmlu_full_de_astronomy": {
                  "acc": true
                },
                "global_mmlu_full_de_business_ethics": {
                  "acc": true
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "acc": true
                },
                "global_mmlu_full_de_college_biology": {
                  "acc": true
                },
                "global_mmlu_full_de_college_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_de_college_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_de_college_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_college_medicine": {
                  "acc": true
                },
                "global_mmlu_full_de_college_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_computer_security": {
                  "acc": true
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_econometrics": {
                  "acc": true
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "acc": true
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_formal_logic": {
                  "acc": true
                },
                "global_mmlu_full_de_global_facts": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_biology": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_geography": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "acc": true
                },
                "global_mmlu_full_de_human_aging": {
                  "acc": true
                },
                "global_mmlu_full_de_human_sexuality": {
                  "acc": true
                },
                "global_mmlu_full_de_humanities": {
                  "acc": true
                },
                "global_mmlu_full_de_international_law": {
                  "acc": true
                },
                "global_mmlu_full_de_jurisprudence": {
                  "acc": true
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "acc": true
                },
                "global_mmlu_full_de_machine_learning": {
                  "acc": true
                },
                "global_mmlu_full_de_management": {
                  "acc": true
                },
                "global_mmlu_full_de_marketing": {
                  "acc": true
                },
                "global_mmlu_full_de_medical_genetics": {
                  "acc": true
                },
                "global_mmlu_full_de_miscellaneous": {
                  "acc": true
                },
                "global_mmlu_full_de_moral_disputes": {
                  "acc": true
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "acc": true
                },
                "global_mmlu_full_de_nutrition": {
                  "acc": true
                },
                "global_mmlu_full_de_other": {
                  "acc": true
                },
                "global_mmlu_full_de_philosophy": {
                  "acc": true
                },
                "global_mmlu_full_de_prehistory": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_accounting": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_law": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_medicine": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_psychology": {
                  "acc": true
                },
                "global_mmlu_full_de_public_relations": {
                  "acc": true
                },
                "global_mmlu_full_de_security_studies": {
                  "acc": true
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc": true
                },
                "global_mmlu_full_de_sociology": {
                  "acc": true
                },
                "global_mmlu_full_de_stem": {
                  "acc": true
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "acc": true
                },
                "global_mmlu_full_de_virology": {
                  "acc": true
                },
                "global_mmlu_full_de_world_religions": {
                  "acc": true
                }
              },
              "n-samples": {
                "global_mmlu_full_de_computer_security": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_college_biology": {
                  "original": 144,
                  "effective": 144
                },
                "global_mmlu_full_de_college_mathematics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "original": 235,
                  "effective": 235
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "original": 378,
                  "effective": 378
                },
                "global_mmlu_full_de_astronomy": {
                  "original": 152,
                  "effective": 152
                },
                "global_mmlu_full_de_high_school_physics": {
                  "original": 151,
                  "effective": 151
                },
                "global_mmlu_full_de_machine_learning": {
                  "original": 112,
                  "effective": 112
                },
                "global_mmlu_full_de_college_chemistry": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "original": 203,
                  "effective": 203
                },
                "global_mmlu_full_de_college_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "original": 270,
                  "effective": 270
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_biology": {
                  "original": 310,
                  "effective": 310
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "original": 145,
                  "effective": 145
                },
                "global_mmlu_full_de_college_physics": {
                  "original": 102,
                  "effective": 102
                },
                "global_mmlu_full_de_anatomy": {
                  "original": 135,
                  "effective": 135
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "original": 216,
                  "effective": 216
                },
                "global_mmlu_full_de_nutrition": {
                  "original": 306,
                  "effective": 306
                },
                "global_mmlu_full_de_professional_medicine": {
                  "original": 272,
                  "effective": 272
                },
                "global_mmlu_full_de_college_medicine": {
                  "original": 173,
                  "effective": 173
                },
                "global_mmlu_full_de_professional_accounting": {
                  "original": 282,
                  "effective": 282
                },
                "global_mmlu_full_de_marketing": {
                  "original": 234,
                  "effective": 234
                },
                "global_mmlu_full_de_miscellaneous": {
                  "original": 783,
                  "effective": 783
                },
                "global_mmlu_full_de_business_ethics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_virology": {
                  "original": 166,
                  "effective": 166
                },
                "global_mmlu_full_de_medical_genetics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_human_aging": {
                  "original": 223,
                  "effective": 223
                },
                "global_mmlu_full_de_global_facts": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "original": 265,
                  "effective": 265
                },
                "global_mmlu_full_de_management": {
                  "original": 103,
                  "effective": 103
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "original": 545,
                  "effective": 545
                },
                "global_mmlu_full_de_professional_psychology": {
                  "original": 612,
                  "effective": 612
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "original": 193,
                  "effective": 193
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "original": 390,
                  "effective": 390
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "original": 238,
                  "effective": 238
                },
                "global_mmlu_full_de_econometrics": {
                  "original": 114,
                  "effective": 114
                },
                "global_mmlu_full_de_high_school_geography": {
                  "original": 198,
                  "effective": 198
                },
                "global_mmlu_full_de_security_studies": {
                  "original": 245,
                  "effective": 245
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_public_relations": {
                  "original": 110,
                  "effective": 110
                },
                "global_mmlu_full_de_human_sexuality": {
                  "original": 131,
                  "effective": 131
                },
                "global_mmlu_full_de_sociology": {
                  "original": 201,
                  "effective": 201
                },
                "global_mmlu_full_de_professional_law": {
                  "original": 1534,
                  "effective": 1534
                },
                "global_mmlu_full_de_international_law": {
                  "original": 121,
                  "effective": 121
                },
                "global_mmlu_full_de_prehistory": {
                  "original": 324,
                  "effective": 324
                },
                "global_mmlu_full_de_world_religions": {
                  "original": 171,
                  "effective": 171
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "original": 237,
                  "effective": 237
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "original": 204,
                  "effective": 204
                },
                "global_mmlu_full_de_jurisprudence": {
                  "original": 108,
                  "effective": 108
                },
                "global_mmlu_full_de_formal_logic": {
                  "original": 126,
                  "effective": 126
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "original": 163,
                  "effective": 163
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "original": 165,
                  "effective": 165
                },
                "global_mmlu_full_de_philosophy": {
                  "original": 311,
                  "effective": 311
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "original": 895,
                  "effective": 895
                },
                "global_mmlu_full_de_moral_disputes": {
                  "original": 346,
                  "effective": 346
                }
              },
              "config": {
                "model": "hf",
                "model_args": "pretrained=GermanT5/t5-efficient-gc4-german-base-nl36,trust_remote_code=True",
                "model_num_parameters": 619357440,
                "model_dtype": "torch.float32",
                "model_revision": "main",
                "model_sha": "471c7616ac14cb86f0c177716315a92c26b57504",
                "batch_size": "auto",
                "batch_sizes": [
                  64
                ],
                "device": "cuda",
                "use_cache": null,
                "limit": null,
                "bootstrap_iters": 100000,
                "gen_kwargs": null,
                "random_seed": 42,
                "numpy_seed": 42,
                "torch_seed": 42,
                "fewshot_seed": 42
              },
              "git_hash": "a4103c7",
              "date": 1758647708.6131508,
              "pretty_env_info": "PyTorch version: 2.7.1+cu126\nIs debug build: False\nCUDA used to build PyTorch: 12.6\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.2 LTS (x86_64)\nGCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\nClang version: Could not collect\nCMake version: version 3.24.1\nLibc version: glibc-2.35\n\nPython version: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] (64-bit runtime)\nPython platform: Linux-6.8.0-1036-nvidia-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H200\nNvidia driver version: 580.82.07\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               224\nOn-line CPU(s) list:                  0-223\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480C\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   56\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          3800.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            5.3 MiB (112 instances)\nL1i cache:                            3.5 MiB (112 instances)\nL2 cache:                             224 MiB (112 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-55,112-167\nNUMA node1 CPU(s):                    56-111,168-223\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pytorch-lightning==2.5.2\n[pip3] torch==2.7.1\n[pip3] torchmetrics==1.8.0\n[pip3] triton==3.3.1\n[conda] Could not collect",
              "transformers_version": "4.54.1",
              "lm_eval_version": "0.4.9",
              "upper_git_hash": null,
              "tokenizer_pad_token": [
                "<pad>",
                "0"
              ],
              "tokenizer_eos_token": [
                "</s>",
                "1"
              ],
              "tokenizer_bos_token": [
                null,
                "None"
              ],
              "eot_token_id": 1,
              "max_length": 512,
              "task_hashes": {},
              "model_source": "hf",
              "model_name": "GermanT5/t5-efficient-gc4-german-base-nl36",
              "model_name_sanitized": "GermanT5__t5-efficient-gc4-german-base-nl36",
              "system_instruction": null,
              "system_instruction_sha": null,
              "fewshot_as_multiturn": false,
              "chat_template": null,
              "chat_template_sha": null,
              "start_time": 633854.237768892,
              "end_time": 634480.706835312,
              "total_evaluation_time_seconds": "626.4690664199879"
            },
            "duration_seconds": 672.4308197498322,
            "duration_minutes": 11.207180329163869,
            "command": "python -m lm_eval --model hf --model_args pretrained=GermanT5/t5-efficient-gc4-german-base-nl36 --tasks global_mmlu_full_de --device cuda --batch_size auto --output_path /netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/hf-germanT5-base_global_mmlu_de_0shot_20250923_191337 --num_fewshot 0 --trust_remote_code --seed 42 --wandb_args project=BA-T5-CrossLingual,entity=nikolas-rauscher-dfki,group=crosslingual_transfer_eval_full_15k-final,name=hf-germanT5-base_global_mmlu_de_0shot",
            "output_dir": "/netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/hf-germanT5-base_global_mmlu_de_0shot_20250923_191337",
            "status": "success"
          }
        }
      }
    },
    "gold-continued-pretraind-on-english-487k": {
      "model_config": {
        "source_path": "pretraining_logs_lr_001_OPTIMIZED_clean_restart/train/runs/2025-09-08_02-33-22/checkpoints/best/step-487500-val_ppl-3.72168.ckpt",
        "name": "gold-continued-pretraind-on-english-487k"
      },
      "model_path": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
      "metadata": {
        "training_steps": 487500,
        "val_perplexity": 3.72168,
        "run_date": "2025-09-08",
        "run_time": "02:33:22",
        "training_style": "restart",
        "run_directory": "2025-09-08_02-33-22",
        "learning_rate": null,
        "batch_size": null,
        "model_name": null,
        "actual_global_step": 487500,
        "epoch": 0,
        "lightning_version": "2.5.1.post0",
        "source_type": "checkpoint",
        "original_checkpoint": "/netscratch/nrauscher/projects/BA-hydra/pretraining_logs_lr_001_OPTIMIZED_clean_restart/train/runs/2025-09-08_02-33-22/checkpoints/best/step-487500-val_ppl-3.72168.ckpt"
      },
      "benchmarks": {
        "global_mmlu_en": {
          "0_shot": {
            "parse_status": "partial",
            "error": "Could not find primary metric",
            "raw_results": {
              "results": {
                "global_mmlu_full_en": {
                  "acc,none": 0.26869391824526423,
                  "acc_stderr,none": 0.003701075874715244,
                  "alias": "global_mmlu_full_en"
                },
                "global_mmlu_full_en_humanities": {
                  "acc,none": 0.24187035069075452,
                  "acc_stderr,none": 0.006231139077456077,
                  "alias": " - global_mmlu_full_en_humanities"
                },
                "global_mmlu_full_en_formal_logic": {
                  "alias": "  - global_mmlu_full_en_formal_logic",
                  "acc,none": 0.36507936507936506,
                  "acc_stderr,none": 0.04306241259127153
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "alias": "  - global_mmlu_full_en_high_school_european_history",
                  "acc,none": 0.2545454545454545,
                  "acc_stderr,none": 0.03401506715249039
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "alias": "  - global_mmlu_full_en_high_school_us_history",
                  "acc,none": 0.2549019607843137,
                  "acc_stderr,none": 0.030587591351604246
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "alias": "  - global_mmlu_full_en_high_school_world_history",
                  "acc,none": 0.20253164556962025,
                  "acc_stderr,none": 0.026160568246601464
                },
                "global_mmlu_full_en_international_law": {
                  "alias": "  - global_mmlu_full_en_international_law",
                  "acc,none": 0.14049586776859505,
                  "acc_stderr,none": 0.031722334260021585
                },
                "global_mmlu_full_en_jurisprudence": {
                  "alias": "  - global_mmlu_full_en_jurisprudence",
                  "acc,none": 0.2222222222222222,
                  "acc_stderr,none": 0.040191074725573483
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "alias": "  - global_mmlu_full_en_logical_fallacies",
                  "acc,none": 0.2331288343558282,
                  "acc_stderr,none": 0.0332201579577674
                },
                "global_mmlu_full_en_moral_disputes": {
                  "alias": "  - global_mmlu_full_en_moral_disputes",
                  "acc,none": 0.2138728323699422,
                  "acc_stderr,none": 0.022075709251757177
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "alias": "  - global_mmlu_full_en_moral_scenarios",
                  "acc,none": 0.27262569832402234,
                  "acc_stderr,none": 0.014893391735249603
                },
                "global_mmlu_full_en_philosophy": {
                  "alias": "  - global_mmlu_full_en_philosophy",
                  "acc,none": 0.24115755627009647,
                  "acc_stderr,none": 0.024296594034763426
                },
                "global_mmlu_full_en_prehistory": {
                  "alias": "  - global_mmlu_full_en_prehistory",
                  "acc,none": 0.22530864197530864,
                  "acc_stderr,none": 0.023246202647819746
                },
                "global_mmlu_full_en_professional_law": {
                  "alias": "  - global_mmlu_full_en_professional_law",
                  "acc,none": 0.24445893089960888,
                  "acc_stderr,none": 0.010976425013113897
                },
                "global_mmlu_full_en_world_religions": {
                  "alias": "  - global_mmlu_full_en_world_religions",
                  "acc,none": 0.17543859649122806,
                  "acc_stderr,none": 0.029170885500727686
                },
                "global_mmlu_full_en_other": {
                  "acc,none": 0.2510460251046025,
                  "acc_stderr,none": 0.007640743094814657,
                  "alias": " - global_mmlu_full_en_other"
                },
                "global_mmlu_full_en_business_ethics": {
                  "alias": "  - global_mmlu_full_en_business_ethics",
                  "acc,none": 0.21,
                  "acc_stderr,none": 0.040936018074033256
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "alias": "  - global_mmlu_full_en_clinical_knowledge",
                  "acc,none": 0.2981132075471698,
                  "acc_stderr,none": 0.028152837942493875
                },
                "global_mmlu_full_en_college_medicine": {
                  "alias": "  - global_mmlu_full_en_college_medicine",
                  "acc,none": 0.3352601156069364,
                  "acc_stderr,none": 0.03599586301247078
                },
                "global_mmlu_full_en_global_facts": {
                  "alias": "  - global_mmlu_full_en_global_facts",
                  "acc,none": 0.18,
                  "acc_stderr,none": 0.038612291966536955
                },
                "global_mmlu_full_en_human_aging": {
                  "alias": "  - global_mmlu_full_en_human_aging",
                  "acc,none": 0.10762331838565023,
                  "acc_stderr,none": 0.020799400082880008
                },
                "global_mmlu_full_en_management": {
                  "alias": "  - global_mmlu_full_en_management",
                  "acc,none": 0.3786407766990291,
                  "acc_stderr,none": 0.048026946982589726
                },
                "global_mmlu_full_en_marketing": {
                  "alias": "  - global_mmlu_full_en_marketing",
                  "acc,none": 0.19658119658119658,
                  "acc_stderr,none": 0.02603538609895129
                },
                "global_mmlu_full_en_medical_genetics": {
                  "alias": "  - global_mmlu_full_en_medical_genetics",
                  "acc,none": 0.23,
                  "acc_stderr,none": 0.04229525846816505
                },
                "global_mmlu_full_en_miscellaneous": {
                  "alias": "  - global_mmlu_full_en_miscellaneous",
                  "acc,none": 0.20434227330779056,
                  "acc_stderr,none": 0.0144191239809319
                },
                "global_mmlu_full_en_nutrition": {
                  "alias": "  - global_mmlu_full_en_nutrition",
                  "acc,none": 0.29411764705882354,
                  "acc_stderr,none": 0.026090162504279035
                },
                "global_mmlu_full_en_professional_accounting": {
                  "alias": "  - global_mmlu_full_en_professional_accounting",
                  "acc,none": 0.24113475177304963,
                  "acc_stderr,none": 0.025518731049537755
                },
                "global_mmlu_full_en_professional_medicine": {
                  "alias": "  - global_mmlu_full_en_professional_medicine",
                  "acc,none": 0.4485294117647059,
                  "acc_stderr,none": 0.030211479609121593
                },
                "global_mmlu_full_en_virology": {
                  "alias": "  - global_mmlu_full_en_virology",
                  "acc,none": 0.1927710843373494,
                  "acc_stderr,none": 0.03070982405056527
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc,none": 0.31069223269418267,
                  "acc_stderr,none": 0.008280143338017945,
                  "alias": " - global_mmlu_full_en_social_sciences"
                },
                "global_mmlu_full_en_econometrics": {
                  "alias": "  - global_mmlu_full_en_econometrics",
                  "acc,none": 0.24561403508771928,
                  "acc_stderr,none": 0.04049339297748142
                },
                "global_mmlu_full_en_high_school_geography": {
                  "alias": "  - global_mmlu_full_en_high_school_geography",
                  "acc,none": 0.35353535353535354,
                  "acc_stderr,none": 0.03406086723547153
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "alias": "  - global_mmlu_full_en_high_school_government_and_politics",
                  "acc,none": 0.3626943005181347,
                  "acc_stderr,none": 0.03469713791704372
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "alias": "  - global_mmlu_full_en_high_school_macroeconomics",
                  "acc,none": 0.3641025641025641,
                  "acc_stderr,none": 0.024396672985094774
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "alias": "  - global_mmlu_full_en_high_school_microeconomics",
                  "acc,none": 0.3487394957983193,
                  "acc_stderr,none": 0.030956636328566548
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "alias": "  - global_mmlu_full_en_high_school_psychology",
                  "acc,none": 0.3486238532110092,
                  "acc_stderr,none": 0.020431254090714328
                },
                "global_mmlu_full_en_human_sexuality": {
                  "alias": "  - global_mmlu_full_en_human_sexuality",
                  "acc,none": 0.2824427480916031,
                  "acc_stderr,none": 0.03948406125768361
                },
                "global_mmlu_full_en_professional_psychology": {
                  "alias": "  - global_mmlu_full_en_professional_psychology",
                  "acc,none": 0.2173202614379085,
                  "acc_stderr,none": 0.016684820929148594
                },
                "global_mmlu_full_en_public_relations": {
                  "alias": "  - global_mmlu_full_en_public_relations",
                  "acc,none": 0.22727272727272727,
                  "acc_stderr,none": 0.04013964554072775
                },
                "global_mmlu_full_en_security_studies": {
                  "alias": "  - global_mmlu_full_en_security_studies",
                  "acc,none": 0.4,
                  "acc_stderr,none": 0.031362502409358936
                },
                "global_mmlu_full_en_sociology": {
                  "alias": "  - global_mmlu_full_en_sociology",
                  "acc,none": 0.26865671641791045,
                  "acc_stderr,none": 0.03134328358208954
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "alias": "  - global_mmlu_full_en_us_foreign_policy",
                  "acc,none": 0.26,
                  "acc_stderr,none": 0.0440844002276808
                },
                "global_mmlu_full_en_stem": {
                  "acc,none": 0.2851252775134792,
                  "acc_stderr,none": 0.007953030581767431,
                  "alias": " - global_mmlu_full_en_stem"
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "alias": "  - global_mmlu_full_en_abstract_algebra",
                  "acc,none": 0.21,
                  "acc_stderr,none": 0.040936018074033256
                },
                "global_mmlu_full_en_anatomy": {
                  "alias": "  - global_mmlu_full_en_anatomy",
                  "acc,none": 0.22962962962962963,
                  "acc_stderr,none": 0.03633384414073465
                },
                "global_mmlu_full_en_astronomy": {
                  "alias": "  - global_mmlu_full_en_astronomy",
                  "acc,none": 0.3355263157894737,
                  "acc_stderr,none": 0.03842498559395269
                },
                "global_mmlu_full_en_college_biology": {
                  "alias": "  - global_mmlu_full_en_college_biology",
                  "acc,none": 0.2569444444444444,
                  "acc_stderr,none": 0.03653946969442099
                },
                "global_mmlu_full_en_college_chemistry": {
                  "alias": "  - global_mmlu_full_en_college_chemistry",
                  "acc,none": 0.41,
                  "acc_stderr,none": 0.04943110704237103
                },
                "global_mmlu_full_en_college_computer_science": {
                  "alias": "  - global_mmlu_full_en_college_computer_science",
                  "acc,none": 0.33,
                  "acc_stderr,none": 0.047258156262526045
                },
                "global_mmlu_full_en_college_mathematics": {
                  "alias": "  - global_mmlu_full_en_college_mathematics",
                  "acc,none": 0.31,
                  "acc_stderr,none": 0.04648231987117316
                },
                "global_mmlu_full_en_college_physics": {
                  "alias": "  - global_mmlu_full_en_college_physics",
                  "acc,none": 0.37254901960784315,
                  "acc_stderr,none": 0.04810840148082634
                },
                "global_mmlu_full_en_computer_security": {
                  "alias": "  - global_mmlu_full_en_computer_security",
                  "acc,none": 0.19,
                  "acc_stderr,none": 0.03942772444036623
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "alias": "  - global_mmlu_full_en_conceptual_physics",
                  "acc,none": 0.2170212765957447,
                  "acc_stderr,none": 0.02694748312149623
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "alias": "  - global_mmlu_full_en_electrical_engineering",
                  "acc,none": 0.22758620689655173,
                  "acc_stderr,none": 0.03493950380131184
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "alias": "  - global_mmlu_full_en_elementary_mathematics",
                  "acc,none": 0.26455026455026454,
                  "acc_stderr,none": 0.022717467897708607
                },
                "global_mmlu_full_en_high_school_biology": {
                  "alias": "  - global_mmlu_full_en_high_school_biology",
                  "acc,none": 0.31290322580645163,
                  "acc_stderr,none": 0.02637756702864586
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "alias": "  - global_mmlu_full_en_high_school_chemistry",
                  "acc,none": 0.28078817733990147,
                  "acc_stderr,none": 0.03161856335358609
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "alias": "  - global_mmlu_full_en_high_school_computer_science",
                  "acc,none": 0.19,
                  "acc_stderr,none": 0.03942772444036625
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "alias": "  - global_mmlu_full_en_high_school_mathematics",
                  "acc,none": 0.25925925925925924,
                  "acc_stderr,none": 0.02671924078371216
                },
                "global_mmlu_full_en_high_school_physics": {
                  "alias": "  - global_mmlu_full_en_high_school_physics",
                  "acc,none": 0.33112582781456956,
                  "acc_stderr,none": 0.038425817186598696
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "alias": "  - global_mmlu_full_en_high_school_statistics",
                  "acc,none": 0.4722222222222222,
                  "acc_stderr,none": 0.0340470532865388
                },
                "global_mmlu_full_en_machine_learning": {
                  "alias": "  - global_mmlu_full_en_machine_learning",
                  "acc,none": 0.16071428571428573,
                  "acc_stderr,none": 0.0348594609647574
                }
              },
              "groups": {
                "global_mmlu_full_en": {
                  "acc,none": 0.26869391824526423,
                  "acc_stderr,none": 0.003701075874715244,
                  "alias": "global_mmlu_full_en"
                },
                "global_mmlu_full_en_humanities": {
                  "acc,none": 0.24187035069075452,
                  "acc_stderr,none": 0.006231139077456077,
                  "alias": " - global_mmlu_full_en_humanities"
                },
                "global_mmlu_full_en_other": {
                  "acc,none": 0.2510460251046025,
                  "acc_stderr,none": 0.007640743094814657,
                  "alias": " - global_mmlu_full_en_other"
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc,none": 0.31069223269418267,
                  "acc_stderr,none": 0.008280143338017945,
                  "alias": " - global_mmlu_full_en_social_sciences"
                },
                "global_mmlu_full_en_stem": {
                  "acc,none": 0.2851252775134792,
                  "acc_stderr,none": 0.007953030581767431,
                  "alias": " - global_mmlu_full_en_stem"
                }
              },
              "group_subtasks": {
                "global_mmlu_full_en_humanities": [
                  "global_mmlu_full_en_world_religions",
                  "global_mmlu_full_en_international_law",
                  "global_mmlu_full_en_moral_scenarios",
                  "global_mmlu_full_en_high_school_us_history",
                  "global_mmlu_full_en_jurisprudence",
                  "global_mmlu_full_en_high_school_world_history",
                  "global_mmlu_full_en_logical_fallacies",
                  "global_mmlu_full_en_philosophy",
                  "global_mmlu_full_en_professional_law",
                  "global_mmlu_full_en_high_school_european_history",
                  "global_mmlu_full_en_formal_logic",
                  "global_mmlu_full_en_moral_disputes",
                  "global_mmlu_full_en_prehistory"
                ],
                "global_mmlu_full_en_social_sciences": [
                  "global_mmlu_full_en_us_foreign_policy",
                  "global_mmlu_full_en_public_relations",
                  "global_mmlu_full_en_econometrics",
                  "global_mmlu_full_en_high_school_microeconomics",
                  "global_mmlu_full_en_human_sexuality",
                  "global_mmlu_full_en_security_studies",
                  "global_mmlu_full_en_professional_psychology",
                  "global_mmlu_full_en_high_school_geography",
                  "global_mmlu_full_en_high_school_government_and_politics",
                  "global_mmlu_full_en_high_school_psychology",
                  "global_mmlu_full_en_high_school_macroeconomics",
                  "global_mmlu_full_en_sociology"
                ],
                "global_mmlu_full_en_other": [
                  "global_mmlu_full_en_clinical_knowledge",
                  "global_mmlu_full_en_medical_genetics",
                  "global_mmlu_full_en_professional_medicine",
                  "global_mmlu_full_en_business_ethics",
                  "global_mmlu_full_en_marketing",
                  "global_mmlu_full_en_global_facts",
                  "global_mmlu_full_en_college_medicine",
                  "global_mmlu_full_en_human_aging",
                  "global_mmlu_full_en_management",
                  "global_mmlu_full_en_nutrition",
                  "global_mmlu_full_en_virology",
                  "global_mmlu_full_en_professional_accounting",
                  "global_mmlu_full_en_miscellaneous"
                ],
                "global_mmlu_full_en_stem": [
                  "global_mmlu_full_en_high_school_statistics",
                  "global_mmlu_full_en_computer_security",
                  "global_mmlu_full_en_machine_learning",
                  "global_mmlu_full_en_abstract_algebra",
                  "global_mmlu_full_en_astronomy",
                  "global_mmlu_full_en_college_computer_science",
                  "global_mmlu_full_en_high_school_biology",
                  "global_mmlu_full_en_high_school_computer_science",
                  "global_mmlu_full_en_high_school_chemistry",
                  "global_mmlu_full_en_elementary_mathematics",
                  "global_mmlu_full_en_college_physics",
                  "global_mmlu_full_en_high_school_physics",
                  "global_mmlu_full_en_college_chemistry",
                  "global_mmlu_full_en_college_biology",
                  "global_mmlu_full_en_college_mathematics",
                  "global_mmlu_full_en_conceptual_physics",
                  "global_mmlu_full_en_anatomy",
                  "global_mmlu_full_en_high_school_mathematics",
                  "global_mmlu_full_en_electrical_engineering"
                ],
                "global_mmlu_full_en": [
                  "global_mmlu_full_en_stem",
                  "global_mmlu_full_en_other",
                  "global_mmlu_full_en_social_sciences",
                  "global_mmlu_full_en_humanities"
                ]
              },
              "configs": {
                "global_mmlu_full_en_abstract_algebra": {
                  "task": "global_mmlu_full_en_abstract_algebra",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cda20>, subject='abstract_algebra')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_anatomy": {
                  "task": "global_mmlu_full_en_anatomy",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cd090>, subject='anatomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_astronomy": {
                  "task": "global_mmlu_full_en_astronomy",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cc4c0>, subject='astronomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_business_ethics": {
                  "task": "global_mmlu_full_en_business_ethics",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03bb490>, subject='business_ethics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "task": "global_mmlu_full_en_clinical_knowledge",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cc9d0>, subject='clinical_knowledge')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_biology": {
                  "task": "global_mmlu_full_en_college_biology",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685ce4d0>, subject='college_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_chemistry": {
                  "task": "global_mmlu_full_en_college_chemistry",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cd360>, subject='college_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_computer_science": {
                  "task": "global_mmlu_full_en_college_computer_science",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cc700>, subject='college_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_mathematics": {
                  "task": "global_mmlu_full_en_college_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cc8b0>, subject='college_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_medicine": {
                  "task": "global_mmlu_full_en_college_medicine",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03b9bd0>, subject='college_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_college_physics": {
                  "task": "global_mmlu_full_en_college_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cc790>, subject='college_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_computer_security": {
                  "task": "global_mmlu_full_en_computer_security",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cedd0>, subject='computer_security')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "task": "global_mmlu_full_en_conceptual_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cc5e0>, subject='conceptual_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_econometrics": {
                  "task": "global_mmlu_full_en_econometrics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03b9a20>, subject='econometrics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "task": "global_mmlu_full_en_electrical_engineering",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cc310>, subject='electrical_engineering')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "task": "global_mmlu_full_en_elementary_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cc940>, subject='elementary_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_formal_logic": {
                  "task": "global_mmlu_full_en_formal_logic",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efdfe049990>, subject='formal_logic')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_global_facts": {
                  "task": "global_mmlu_full_en_global_facts",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03bacb0>, subject='global_facts')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_biology": {
                  "task": "global_mmlu_full_en_high_school_biology",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cfe20>, subject='high_school_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "task": "global_mmlu_full_en_high_school_chemistry",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685ceb90>, subject='high_school_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "task": "global_mmlu_full_en_high_school_computer_science",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cf250>, subject='high_school_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "task": "global_mmlu_full_en_high_school_european_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde06145e0>, subject='high_school_european_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_geography": {
                  "task": "global_mmlu_full_en_high_school_geography",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde06175b0>, subject='high_school_geography')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "task": "global_mmlu_full_en_high_school_government_and_politics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03ba0e0>, subject='high_school_government_and_politics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "task": "global_mmlu_full_en_high_school_macroeconomics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03b96c0>, subject='high_school_macroeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "task": "global_mmlu_full_en_high_school_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cc280>, subject='high_school_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "task": "global_mmlu_full_en_high_school_microeconomics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03ba320>, subject='high_school_microeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_physics": {
                  "task": "global_mmlu_full_en_high_school_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685ce560>, subject='high_school_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "task": "global_mmlu_full_en_high_school_psychology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03b8790>, subject='high_school_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "task": "global_mmlu_full_en_high_school_statistics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cd120>, subject='high_school_statistics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "task": "global_mmlu_full_en_high_school_us_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde06151b0>, subject='high_school_us_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "task": "global_mmlu_full_en_high_school_world_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde0617be0>, subject='high_school_world_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_human_aging": {
                  "task": "global_mmlu_full_en_human_aging",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03bad40>, subject='human_aging')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_human_sexuality": {
                  "task": "global_mmlu_full_en_human_sexuality",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03b9f30>, subject='human_sexuality')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_international_law": {
                  "task": "global_mmlu_full_en_international_law",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde0616b00>, subject='international_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_jurisprudence": {
                  "task": "global_mmlu_full_en_jurisprudence",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde0615510>, subject='jurisprudence')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "task": "global_mmlu_full_en_logical_fallacies",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde0616ef0>, subject='logical_fallacies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_machine_learning": {
                  "task": "global_mmlu_full_en_machine_learning",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685ce950>, subject='machine_learning')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_management": {
                  "task": "global_mmlu_full_en_management",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03bb6d0>, subject='management')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_marketing": {
                  "task": "global_mmlu_full_en_marketing",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03badd0>, subject='marketing')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_medical_genetics": {
                  "task": "global_mmlu_full_en_medical_genetics",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cd6c0>, subject='medical_genetics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_miscellaneous": {
                  "task": "global_mmlu_full_en_miscellaneous",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03bb370>, subject='miscellaneous')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_moral_disputes": {
                  "task": "global_mmlu_full_en_moral_disputes",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde06149d0>, subject='moral_disputes')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "task": "global_mmlu_full_en_moral_scenarios",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde0616d40>, subject='moral_scenarios')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_nutrition": {
                  "task": "global_mmlu_full_en_nutrition",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03ba4d0>, subject='nutrition')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_philosophy": {
                  "task": "global_mmlu_full_en_philosophy",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde06156c0>, subject='philosophy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_prehistory": {
                  "task": "global_mmlu_full_en_prehistory",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde11f0280>, subject='prehistory')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_professional_accounting": {
                  "task": "global_mmlu_full_en_professional_accounting",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03b8b80>, subject='professional_accounting')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_professional_law": {
                  "task": "global_mmlu_full_en_professional_law",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde0614dc0>, subject='professional_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_professional_medicine": {
                  "task": "global_mmlu_full_en_professional_medicine",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efb685cc820>, subject='professional_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_professional_psychology": {
                  "task": "global_mmlu_full_en_professional_psychology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde0617f40>, subject='professional_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_public_relations": {
                  "task": "global_mmlu_full_en_public_relations",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03bb520>, subject='public_relations')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_security_studies": {
                  "task": "global_mmlu_full_en_security_studies",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03b8550>, subject='security_studies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_sociology": {
                  "task": "global_mmlu_full_en_sociology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03b8ee0>, subject='sociology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "task": "global_mmlu_full_en_us_foreign_policy",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03b83a0>, subject='us_foreign_policy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_virology": {
                  "task": "global_mmlu_full_en_virology",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde03bb760>, subject='virology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_en_world_religions": {
                  "task": "global_mmlu_full_en_world_religions",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7efde06171c0>, subject='world_religions')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                }
              },
              "versions": {
                "global_mmlu_full_en": 0.0,
                "global_mmlu_full_en_abstract_algebra": 0.0,
                "global_mmlu_full_en_anatomy": 0.0,
                "global_mmlu_full_en_astronomy": 0.0,
                "global_mmlu_full_en_business_ethics": 0.0,
                "global_mmlu_full_en_clinical_knowledge": 0.0,
                "global_mmlu_full_en_college_biology": 0.0,
                "global_mmlu_full_en_college_chemistry": 0.0,
                "global_mmlu_full_en_college_computer_science": 0.0,
                "global_mmlu_full_en_college_mathematics": 0.0,
                "global_mmlu_full_en_college_medicine": 0.0,
                "global_mmlu_full_en_college_physics": 0.0,
                "global_mmlu_full_en_computer_security": 0.0,
                "global_mmlu_full_en_conceptual_physics": 0.0,
                "global_mmlu_full_en_econometrics": 0.0,
                "global_mmlu_full_en_electrical_engineering": 0.0,
                "global_mmlu_full_en_elementary_mathematics": 0.0,
                "global_mmlu_full_en_formal_logic": 0.0,
                "global_mmlu_full_en_global_facts": 0.0,
                "global_mmlu_full_en_high_school_biology": 0.0,
                "global_mmlu_full_en_high_school_chemistry": 0.0,
                "global_mmlu_full_en_high_school_computer_science": 0.0,
                "global_mmlu_full_en_high_school_european_history": 0.0,
                "global_mmlu_full_en_high_school_geography": 0.0,
                "global_mmlu_full_en_high_school_government_and_politics": 0.0,
                "global_mmlu_full_en_high_school_macroeconomics": 0.0,
                "global_mmlu_full_en_high_school_mathematics": 0.0,
                "global_mmlu_full_en_high_school_microeconomics": 0.0,
                "global_mmlu_full_en_high_school_physics": 0.0,
                "global_mmlu_full_en_high_school_psychology": 0.0,
                "global_mmlu_full_en_high_school_statistics": 0.0,
                "global_mmlu_full_en_high_school_us_history": 0.0,
                "global_mmlu_full_en_high_school_world_history": 0.0,
                "global_mmlu_full_en_human_aging": 0.0,
                "global_mmlu_full_en_human_sexuality": 0.0,
                "global_mmlu_full_en_humanities": 0.0,
                "global_mmlu_full_en_international_law": 0.0,
                "global_mmlu_full_en_jurisprudence": 0.0,
                "global_mmlu_full_en_logical_fallacies": 0.0,
                "global_mmlu_full_en_machine_learning": 0.0,
                "global_mmlu_full_en_management": 0.0,
                "global_mmlu_full_en_marketing": 0.0,
                "global_mmlu_full_en_medical_genetics": 0.0,
                "global_mmlu_full_en_miscellaneous": 0.0,
                "global_mmlu_full_en_moral_disputes": 0.0,
                "global_mmlu_full_en_moral_scenarios": 0.0,
                "global_mmlu_full_en_nutrition": 0.0,
                "global_mmlu_full_en_other": 0.0,
                "global_mmlu_full_en_philosophy": 0.0,
                "global_mmlu_full_en_prehistory": 0.0,
                "global_mmlu_full_en_professional_accounting": 0.0,
                "global_mmlu_full_en_professional_law": 0.0,
                "global_mmlu_full_en_professional_medicine": 0.0,
                "global_mmlu_full_en_professional_psychology": 0.0,
                "global_mmlu_full_en_public_relations": 0.0,
                "global_mmlu_full_en_security_studies": 0.0,
                "global_mmlu_full_en_social_sciences": 0.0,
                "global_mmlu_full_en_sociology": 0.0,
                "global_mmlu_full_en_stem": 0.0,
                "global_mmlu_full_en_us_foreign_policy": 0.0,
                "global_mmlu_full_en_virology": 0.0,
                "global_mmlu_full_en_world_religions": 0.0
              },
              "n-shot": {
                "global_mmlu_full_en_abstract_algebra": 0,
                "global_mmlu_full_en_anatomy": 0,
                "global_mmlu_full_en_astronomy": 0,
                "global_mmlu_full_en_business_ethics": 0,
                "global_mmlu_full_en_clinical_knowledge": 0,
                "global_mmlu_full_en_college_biology": 0,
                "global_mmlu_full_en_college_chemistry": 0,
                "global_mmlu_full_en_college_computer_science": 0,
                "global_mmlu_full_en_college_mathematics": 0,
                "global_mmlu_full_en_college_medicine": 0,
                "global_mmlu_full_en_college_physics": 0,
                "global_mmlu_full_en_computer_security": 0,
                "global_mmlu_full_en_conceptual_physics": 0,
                "global_mmlu_full_en_econometrics": 0,
                "global_mmlu_full_en_electrical_engineering": 0,
                "global_mmlu_full_en_elementary_mathematics": 0,
                "global_mmlu_full_en_formal_logic": 0,
                "global_mmlu_full_en_global_facts": 0,
                "global_mmlu_full_en_high_school_biology": 0,
                "global_mmlu_full_en_high_school_chemistry": 0,
                "global_mmlu_full_en_high_school_computer_science": 0,
                "global_mmlu_full_en_high_school_european_history": 0,
                "global_mmlu_full_en_high_school_geography": 0,
                "global_mmlu_full_en_high_school_government_and_politics": 0,
                "global_mmlu_full_en_high_school_macroeconomics": 0,
                "global_mmlu_full_en_high_school_mathematics": 0,
                "global_mmlu_full_en_high_school_microeconomics": 0,
                "global_mmlu_full_en_high_school_physics": 0,
                "global_mmlu_full_en_high_school_psychology": 0,
                "global_mmlu_full_en_high_school_statistics": 0,
                "global_mmlu_full_en_high_school_us_history": 0,
                "global_mmlu_full_en_high_school_world_history": 0,
                "global_mmlu_full_en_human_aging": 0,
                "global_mmlu_full_en_human_sexuality": 0,
                "global_mmlu_full_en_international_law": 0,
                "global_mmlu_full_en_jurisprudence": 0,
                "global_mmlu_full_en_logical_fallacies": 0,
                "global_mmlu_full_en_machine_learning": 0,
                "global_mmlu_full_en_management": 0,
                "global_mmlu_full_en_marketing": 0,
                "global_mmlu_full_en_medical_genetics": 0,
                "global_mmlu_full_en_miscellaneous": 0,
                "global_mmlu_full_en_moral_disputes": 0,
                "global_mmlu_full_en_moral_scenarios": 0,
                "global_mmlu_full_en_nutrition": 0,
                "global_mmlu_full_en_philosophy": 0,
                "global_mmlu_full_en_prehistory": 0,
                "global_mmlu_full_en_professional_accounting": 0,
                "global_mmlu_full_en_professional_law": 0,
                "global_mmlu_full_en_professional_medicine": 0,
                "global_mmlu_full_en_professional_psychology": 0,
                "global_mmlu_full_en_public_relations": 0,
                "global_mmlu_full_en_security_studies": 0,
                "global_mmlu_full_en_sociology": 0,
                "global_mmlu_full_en_us_foreign_policy": 0,
                "global_mmlu_full_en_virology": 0,
                "global_mmlu_full_en_world_religions": 0
              },
              "higher_is_better": {
                "global_mmlu_full_en": {
                  "acc": true
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "acc": true
                },
                "global_mmlu_full_en_anatomy": {
                  "acc": true
                },
                "global_mmlu_full_en_astronomy": {
                  "acc": true
                },
                "global_mmlu_full_en_business_ethics": {
                  "acc": true
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "acc": true
                },
                "global_mmlu_full_en_college_biology": {
                  "acc": true
                },
                "global_mmlu_full_en_college_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_en_college_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_en_college_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_college_medicine": {
                  "acc": true
                },
                "global_mmlu_full_en_college_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_computer_security": {
                  "acc": true
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_econometrics": {
                  "acc": true
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "acc": true
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_formal_logic": {
                  "acc": true
                },
                "global_mmlu_full_en_global_facts": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_biology": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_geography": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "acc": true
                },
                "global_mmlu_full_en_human_aging": {
                  "acc": true
                },
                "global_mmlu_full_en_human_sexuality": {
                  "acc": true
                },
                "global_mmlu_full_en_humanities": {
                  "acc": true
                },
                "global_mmlu_full_en_international_law": {
                  "acc": true
                },
                "global_mmlu_full_en_jurisprudence": {
                  "acc": true
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "acc": true
                },
                "global_mmlu_full_en_machine_learning": {
                  "acc": true
                },
                "global_mmlu_full_en_management": {
                  "acc": true
                },
                "global_mmlu_full_en_marketing": {
                  "acc": true
                },
                "global_mmlu_full_en_medical_genetics": {
                  "acc": true
                },
                "global_mmlu_full_en_miscellaneous": {
                  "acc": true
                },
                "global_mmlu_full_en_moral_disputes": {
                  "acc": true
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "acc": true
                },
                "global_mmlu_full_en_nutrition": {
                  "acc": true
                },
                "global_mmlu_full_en_other": {
                  "acc": true
                },
                "global_mmlu_full_en_philosophy": {
                  "acc": true
                },
                "global_mmlu_full_en_prehistory": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_accounting": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_law": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_medicine": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_psychology": {
                  "acc": true
                },
                "global_mmlu_full_en_public_relations": {
                  "acc": true
                },
                "global_mmlu_full_en_security_studies": {
                  "acc": true
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc": true
                },
                "global_mmlu_full_en_sociology": {
                  "acc": true
                },
                "global_mmlu_full_en_stem": {
                  "acc": true
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "acc": true
                },
                "global_mmlu_full_en_virology": {
                  "acc": true
                },
                "global_mmlu_full_en_world_religions": {
                  "acc": true
                }
              },
              "n-samples": {
                "global_mmlu_full_en_high_school_statistics": {
                  "original": 216,
                  "effective": 216
                },
                "global_mmlu_full_en_computer_security": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_machine_learning": {
                  "original": 112,
                  "effective": 112
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_astronomy": {
                  "original": 152,
                  "effective": 152
                },
                "global_mmlu_full_en_college_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_high_school_biology": {
                  "original": 310,
                  "effective": 310
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "original": 203,
                  "effective": 203
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "original": 378,
                  "effective": 378
                },
                "global_mmlu_full_en_college_physics": {
                  "original": 102,
                  "effective": 102
                },
                "global_mmlu_full_en_high_school_physics": {
                  "original": 151,
                  "effective": 151
                },
                "global_mmlu_full_en_college_chemistry": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_college_biology": {
                  "original": 144,
                  "effective": 144
                },
                "global_mmlu_full_en_college_mathematics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "original": 235,
                  "effective": 235
                },
                "global_mmlu_full_en_anatomy": {
                  "original": 135,
                  "effective": 135
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "original": 270,
                  "effective": 270
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "original": 145,
                  "effective": 145
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "original": 265,
                  "effective": 265
                },
                "global_mmlu_full_en_medical_genetics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_professional_medicine": {
                  "original": 272,
                  "effective": 272
                },
                "global_mmlu_full_en_business_ethics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_marketing": {
                  "original": 234,
                  "effective": 234
                },
                "global_mmlu_full_en_global_facts": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_college_medicine": {
                  "original": 173,
                  "effective": 173
                },
                "global_mmlu_full_en_human_aging": {
                  "original": 223,
                  "effective": 223
                },
                "global_mmlu_full_en_management": {
                  "original": 103,
                  "effective": 103
                },
                "global_mmlu_full_en_nutrition": {
                  "original": 306,
                  "effective": 306
                },
                "global_mmlu_full_en_virology": {
                  "original": 166,
                  "effective": 166
                },
                "global_mmlu_full_en_professional_accounting": {
                  "original": 282,
                  "effective": 282
                },
                "global_mmlu_full_en_miscellaneous": {
                  "original": 783,
                  "effective": 783
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_public_relations": {
                  "original": 110,
                  "effective": 110
                },
                "global_mmlu_full_en_econometrics": {
                  "original": 114,
                  "effective": 114
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "original": 238,
                  "effective": 238
                },
                "global_mmlu_full_en_human_sexuality": {
                  "original": 131,
                  "effective": 131
                },
                "global_mmlu_full_en_security_studies": {
                  "original": 245,
                  "effective": 245
                },
                "global_mmlu_full_en_professional_psychology": {
                  "original": 612,
                  "effective": 612
                },
                "global_mmlu_full_en_high_school_geography": {
                  "original": 198,
                  "effective": 198
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "original": 193,
                  "effective": 193
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "original": 545,
                  "effective": 545
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "original": 390,
                  "effective": 390
                },
                "global_mmlu_full_en_sociology": {
                  "original": 201,
                  "effective": 201
                },
                "global_mmlu_full_en_world_religions": {
                  "original": 171,
                  "effective": 171
                },
                "global_mmlu_full_en_international_law": {
                  "original": 121,
                  "effective": 121
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "original": 895,
                  "effective": 895
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "original": 204,
                  "effective": 204
                },
                "global_mmlu_full_en_jurisprudence": {
                  "original": 108,
                  "effective": 108
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "original": 237,
                  "effective": 237
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "original": 163,
                  "effective": 163
                },
                "global_mmlu_full_en_philosophy": {
                  "original": 311,
                  "effective": 311
                },
                "global_mmlu_full_en_professional_law": {
                  "original": 1534,
                  "effective": 1534
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "original": 165,
                  "effective": 165
                },
                "global_mmlu_full_en_formal_logic": {
                  "original": 126,
                  "effective": 126
                },
                "global_mmlu_full_en_moral_disputes": {
                  "original": 346,
                  "effective": 346
                },
                "global_mmlu_full_en_prehistory": {
                  "original": 324,
                  "effective": 324
                }
              },
              "config": {
                "model": "hf",
                "model_args": "pretrained=/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k,trust_remote_code=True,local_files_only=True,trust_remote_code=True",
                "model_num_parameters": 222903552,
                "model_dtype": "torch.float32",
                "model_revision": "main",
                "model_sha": "",
                "batch_size": "auto",
                "batch_sizes": [
                  64
                ],
                "device": "cuda",
                "use_cache": null,
                "limit": null,
                "bootstrap_iters": 100000,
                "gen_kwargs": null,
                "random_seed": 42,
                "numpy_seed": 42,
                "torch_seed": 42,
                "fewshot_seed": 42
              },
              "git_hash": "a4103c7",
              "date": 1758648362.6228013,
              "pretty_env_info": "PyTorch version: 2.7.1+cu126\nIs debug build: False\nCUDA used to build PyTorch: 12.6\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.2 LTS (x86_64)\nGCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\nClang version: Could not collect\nCMake version: version 3.24.1\nLibc version: glibc-2.35\n\nPython version: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] (64-bit runtime)\nPython platform: Linux-6.8.0-1036-nvidia-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H200\nNvidia driver version: 580.82.07\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               224\nOn-line CPU(s) list:                  0-223\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480C\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   56\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          3800.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            5.3 MiB (112 instances)\nL1i cache:                            3.5 MiB (112 instances)\nL2 cache:                             224 MiB (112 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-55,112-167\nNUMA node1 CPU(s):                    56-111,168-223\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pytorch-lightning==2.5.2\n[pip3] torch==2.7.1\n[pip3] torchmetrics==1.8.0\n[pip3] triton==3.3.1\n[conda] Could not collect",
              "transformers_version": "4.54.1",
              "lm_eval_version": "0.4.9",
              "upper_git_hash": null,
              "tokenizer_pad_token": [
                "<pad>",
                "0"
              ],
              "tokenizer_eos_token": [
                "</s>",
                "1"
              ],
              "tokenizer_bos_token": [
                null,
                "None"
              ],
              "eot_token_id": 1,
              "max_length": 512,
              "task_hashes": {},
              "model_source": "hf",
              "model_name": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
              "model_name_sanitized": "__netscratch__nrauscher__projects__BA-hydra__evaluation__models__temp__gold-continued-pretraind-on-english-487k",
              "system_instruction": null,
              "system_instruction_sha": null,
              "fewshot_as_multiturn": false,
              "chat_template": null,
              "chat_template_sha": null,
              "start_time": 634506.012692563,
              "end_time": 634838.025236542,
              "total_evaluation_time_seconds": "332.01254397898447"
            },
            "duration_seconds": 378.717826128006,
            "duration_minutes": 6.3119637688001,
            "command": "python -m lm_eval --model hf --model_args pretrained=/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k,trust_remote_code=True,local_files_only=True --tasks global_mmlu_full_en --device cuda --batch_size auto --output_path /netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/gold-continued-pretraind-on-english-487k_global_mmlu_en_0shot_20250923_192428 --num_fewshot 0 --trust_remote_code --seed 42 --wandb_args project=BA-T5-CrossLingual,entity=nikolas-rauscher-dfki,group=crosslingual_transfer_eval_full_15k-final,name=gold-continued-pretraind-on-english-487k_global_mmlu_en_0shot",
            "output_dir": "/netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/gold-continued-pretraind-on-english-487k_global_mmlu_en_0shot_20250923_192428",
            "status": "success"
          }
        },
        "global_mmlu_de": {
          "0_shot": {
            "parse_status": "partial",
            "error": "Could not find primary metric",
            "raw_results": {
              "results": {
                "global_mmlu_full_de": {
                  "acc,none": 0.2687651331719128,
                  "acc_stderr,none": 0.00370090029657946,
                  "alias": "global_mmlu_full_de"
                },
                "global_mmlu_full_de_humanities": {
                  "acc,none": 0.2414452709883103,
                  "acc_stderr,none": 0.006226964347709165,
                  "alias": " - global_mmlu_full_de_humanities"
                },
                "global_mmlu_full_de_formal_logic": {
                  "alias": "  - global_mmlu_full_de_formal_logic",
                  "acc,none": 0.36507936507936506,
                  "acc_stderr,none": 0.04306241259127153
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "alias": "  - global_mmlu_full_de_high_school_european_history",
                  "acc,none": 0.2545454545454545,
                  "acc_stderr,none": 0.03401506715249039
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "alias": "  - global_mmlu_full_de_high_school_us_history",
                  "acc,none": 0.2549019607843137,
                  "acc_stderr,none": 0.030587591351604246
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "alias": "  - global_mmlu_full_de_high_school_world_history",
                  "acc,none": 0.20253164556962025,
                  "acc_stderr,none": 0.026160568246601464
                },
                "global_mmlu_full_de_international_law": {
                  "alias": "  - global_mmlu_full_de_international_law",
                  "acc,none": 0.14049586776859505,
                  "acc_stderr,none": 0.031722334260021585
                },
                "global_mmlu_full_de_jurisprudence": {
                  "alias": "  - global_mmlu_full_de_jurisprudence",
                  "acc,none": 0.2037037037037037,
                  "acc_stderr,none": 0.038935425188248475
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "alias": "  - global_mmlu_full_de_logical_fallacies",
                  "acc,none": 0.2331288343558282,
                  "acc_stderr,none": 0.0332201579577674
                },
                "global_mmlu_full_de_moral_disputes": {
                  "alias": "  - global_mmlu_full_de_moral_disputes",
                  "acc,none": 0.2138728323699422,
                  "acc_stderr,none": 0.022075709251757177
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "alias": "  - global_mmlu_full_de_moral_scenarios",
                  "acc,none": 0.27262569832402234,
                  "acc_stderr,none": 0.014893391735249603
                },
                "global_mmlu_full_de_philosophy": {
                  "alias": "  - global_mmlu_full_de_philosophy",
                  "acc,none": 0.24115755627009647,
                  "acc_stderr,none": 0.024296594034763426
                },
                "global_mmlu_full_de_prehistory": {
                  "alias": "  - global_mmlu_full_de_prehistory",
                  "acc,none": 0.22530864197530864,
                  "acc_stderr,none": 0.023246202647819746
                },
                "global_mmlu_full_de_professional_law": {
                  "alias": "  - global_mmlu_full_de_professional_law",
                  "acc,none": 0.24445893089960888,
                  "acc_stderr,none": 0.010976425013113897
                },
                "global_mmlu_full_de_world_religions": {
                  "alias": "  - global_mmlu_full_de_world_religions",
                  "acc,none": 0.17543859649122806,
                  "acc_stderr,none": 0.029170885500727686
                },
                "global_mmlu_full_de_other": {
                  "acc,none": 0.2513678789829417,
                  "acc_stderr,none": 0.007644350095427473,
                  "alias": " - global_mmlu_full_de_other"
                },
                "global_mmlu_full_de_business_ethics": {
                  "alias": "  - global_mmlu_full_de_business_ethics",
                  "acc,none": 0.21,
                  "acc_stderr,none": 0.040936018074033256
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "alias": "  - global_mmlu_full_de_clinical_knowledge",
                  "acc,none": 0.2981132075471698,
                  "acc_stderr,none": 0.028152837942493875
                },
                "global_mmlu_full_de_college_medicine": {
                  "alias": "  - global_mmlu_full_de_college_medicine",
                  "acc,none": 0.3352601156069364,
                  "acc_stderr,none": 0.03599586301247078
                },
                "global_mmlu_full_de_global_facts": {
                  "alias": "  - global_mmlu_full_de_global_facts",
                  "acc,none": 0.18,
                  "acc_stderr,none": 0.038612291966536955
                },
                "global_mmlu_full_de_human_aging": {
                  "alias": "  - global_mmlu_full_de_human_aging",
                  "acc,none": 0.10762331838565023,
                  "acc_stderr,none": 0.020799400082880008
                },
                "global_mmlu_full_de_management": {
                  "alias": "  - global_mmlu_full_de_management",
                  "acc,none": 0.3786407766990291,
                  "acc_stderr,none": 0.048026946982589726
                },
                "global_mmlu_full_de_marketing": {
                  "alias": "  - global_mmlu_full_de_marketing",
                  "acc,none": 0.19658119658119658,
                  "acc_stderr,none": 0.02603538609895129
                },
                "global_mmlu_full_de_medical_genetics": {
                  "alias": "  - global_mmlu_full_de_medical_genetics",
                  "acc,none": 0.24,
                  "acc_stderr,none": 0.04292346959909282
                },
                "global_mmlu_full_de_miscellaneous": {
                  "alias": "  - global_mmlu_full_de_miscellaneous",
                  "acc,none": 0.20434227330779056,
                  "acc_stderr,none": 0.0144191239809319
                },
                "global_mmlu_full_de_nutrition": {
                  "alias": "  - global_mmlu_full_de_nutrition",
                  "acc,none": 0.29411764705882354,
                  "acc_stderr,none": 0.026090162504279035
                },
                "global_mmlu_full_de_professional_accounting": {
                  "alias": "  - global_mmlu_full_de_professional_accounting",
                  "acc,none": 0.24113475177304963,
                  "acc_stderr,none": 0.025518731049537755
                },
                "global_mmlu_full_de_professional_medicine": {
                  "alias": "  - global_mmlu_full_de_professional_medicine",
                  "acc,none": 0.4485294117647059,
                  "acc_stderr,none": 0.030211479609121593
                },
                "global_mmlu_full_de_virology": {
                  "alias": "  - global_mmlu_full_de_virology",
                  "acc,none": 0.1927710843373494,
                  "acc_stderr,none": 0.03070982405056527
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc,none": 0.31069223269418267,
                  "acc_stderr,none": 0.008278554546998305,
                  "alias": " - global_mmlu_full_de_social_sciences"
                },
                "global_mmlu_full_de_econometrics": {
                  "alias": "  - global_mmlu_full_de_econometrics",
                  "acc,none": 0.23684210526315788,
                  "acc_stderr,none": 0.039994238792813344
                },
                "global_mmlu_full_de_high_school_geography": {
                  "alias": "  - global_mmlu_full_de_high_school_geography",
                  "acc,none": 0.35353535353535354,
                  "acc_stderr,none": 0.03406086723547153
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "alias": "  - global_mmlu_full_de_high_school_government_and_politics",
                  "acc,none": 0.36787564766839376,
                  "acc_stderr,none": 0.034801756684660366
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "alias": "  - global_mmlu_full_de_high_school_macroeconomics",
                  "acc,none": 0.3641025641025641,
                  "acc_stderr,none": 0.024396672985094774
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "alias": "  - global_mmlu_full_de_high_school_microeconomics",
                  "acc,none": 0.3487394957983193,
                  "acc_stderr,none": 0.030956636328566548
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "alias": "  - global_mmlu_full_de_high_school_psychology",
                  "acc,none": 0.3486238532110092,
                  "acc_stderr,none": 0.020431254090714328
                },
                "global_mmlu_full_de_human_sexuality": {
                  "alias": "  - global_mmlu_full_de_human_sexuality",
                  "acc,none": 0.2824427480916031,
                  "acc_stderr,none": 0.03948406125768361
                },
                "global_mmlu_full_de_professional_psychology": {
                  "alias": "  - global_mmlu_full_de_professional_psychology",
                  "acc,none": 0.2173202614379085,
                  "acc_stderr,none": 0.016684820929148594
                },
                "global_mmlu_full_de_public_relations": {
                  "alias": "  - global_mmlu_full_de_public_relations",
                  "acc,none": 0.22727272727272727,
                  "acc_stderr,none": 0.04013964554072775
                },
                "global_mmlu_full_de_security_studies": {
                  "alias": "  - global_mmlu_full_de_security_studies",
                  "acc,none": 0.4,
                  "acc_stderr,none": 0.031362502409358936
                },
                "global_mmlu_full_de_sociology": {
                  "alias": "  - global_mmlu_full_de_sociology",
                  "acc,none": 0.26865671641791045,
                  "acc_stderr,none": 0.03134328358208954
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "alias": "  - global_mmlu_full_de_us_foreign_policy",
                  "acc,none": 0.26,
                  "acc_stderr,none": 0.0440844002276808
                },
                "global_mmlu_full_de_stem": {
                  "acc,none": 0.2857595940374247,
                  "acc_stderr,none": 0.007956917285722405,
                  "alias": " - global_mmlu_full_de_stem"
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "alias": "  - global_mmlu_full_de_abstract_algebra",
                  "acc,none": 0.21,
                  "acc_stderr,none": 0.040936018074033256
                },
                "global_mmlu_full_de_anatomy": {
                  "alias": "  - global_mmlu_full_de_anatomy",
                  "acc,none": 0.22962962962962963,
                  "acc_stderr,none": 0.03633384414073465
                },
                "global_mmlu_full_de_astronomy": {
                  "alias": "  - global_mmlu_full_de_astronomy",
                  "acc,none": 0.3355263157894737,
                  "acc_stderr,none": 0.03842498559395269
                },
                "global_mmlu_full_de_college_biology": {
                  "alias": "  - global_mmlu_full_de_college_biology",
                  "acc,none": 0.2638888888888889,
                  "acc_stderr,none": 0.03685651095897532
                },
                "global_mmlu_full_de_college_chemistry": {
                  "alias": "  - global_mmlu_full_de_college_chemistry",
                  "acc,none": 0.41,
                  "acc_stderr,none": 0.04943110704237103
                },
                "global_mmlu_full_de_college_computer_science": {
                  "alias": "  - global_mmlu_full_de_college_computer_science",
                  "acc,none": 0.33,
                  "acc_stderr,none": 0.047258156262526045
                },
                "global_mmlu_full_de_college_mathematics": {
                  "alias": "  - global_mmlu_full_de_college_mathematics",
                  "acc,none": 0.31,
                  "acc_stderr,none": 0.04648231987117316
                },
                "global_mmlu_full_de_college_physics": {
                  "alias": "  - global_mmlu_full_de_college_physics",
                  "acc,none": 0.37254901960784315,
                  "acc_stderr,none": 0.04810840148082634
                },
                "global_mmlu_full_de_computer_security": {
                  "alias": "  - global_mmlu_full_de_computer_security",
                  "acc,none": 0.18,
                  "acc_stderr,none": 0.038612291966536955
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "alias": "  - global_mmlu_full_de_conceptual_physics",
                  "acc,none": 0.20851063829787234,
                  "acc_stderr,none": 0.026556982117838728
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "alias": "  - global_mmlu_full_de_electrical_engineering",
                  "acc,none": 0.2413793103448276,
                  "acc_stderr,none": 0.03565998174135302
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "alias": "  - global_mmlu_full_de_elementary_mathematics",
                  "acc,none": 0.26455026455026454,
                  "acc_stderr,none": 0.022717467897708607
                },
                "global_mmlu_full_de_high_school_biology": {
                  "alias": "  - global_mmlu_full_de_high_school_biology",
                  "acc,none": 0.3161290322580645,
                  "acc_stderr,none": 0.026450874489042767
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "alias": "  - global_mmlu_full_de_high_school_chemistry",
                  "acc,none": 0.28078817733990147,
                  "acc_stderr,none": 0.03161856335358609
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "alias": "  - global_mmlu_full_de_high_school_computer_science",
                  "acc,none": 0.19,
                  "acc_stderr,none": 0.03942772444036625
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "alias": "  - global_mmlu_full_de_high_school_mathematics",
                  "acc,none": 0.26296296296296295,
                  "acc_stderr,none": 0.026842057873833706
                },
                "global_mmlu_full_de_high_school_physics": {
                  "alias": "  - global_mmlu_full_de_high_school_physics",
                  "acc,none": 0.33112582781456956,
                  "acc_stderr,none": 0.038425817186598696
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "alias": "  - global_mmlu_full_de_high_school_statistics",
                  "acc,none": 0.4722222222222222,
                  "acc_stderr,none": 0.0340470532865388
                },
                "global_mmlu_full_de_machine_learning": {
                  "alias": "  - global_mmlu_full_de_machine_learning",
                  "acc,none": 0.16071428571428573,
                  "acc_stderr,none": 0.0348594609647574
                }
              },
              "groups": {
                "global_mmlu_full_de": {
                  "acc,none": 0.2687651331719128,
                  "acc_stderr,none": 0.00370090029657946,
                  "alias": "global_mmlu_full_de"
                },
                "global_mmlu_full_de_humanities": {
                  "acc,none": 0.2414452709883103,
                  "acc_stderr,none": 0.006226964347709165,
                  "alias": " - global_mmlu_full_de_humanities"
                },
                "global_mmlu_full_de_other": {
                  "acc,none": 0.2513678789829417,
                  "acc_stderr,none": 0.007644350095427473,
                  "alias": " - global_mmlu_full_de_other"
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc,none": 0.31069223269418267,
                  "acc_stderr,none": 0.008278554546998305,
                  "alias": " - global_mmlu_full_de_social_sciences"
                },
                "global_mmlu_full_de_stem": {
                  "acc,none": 0.2857595940374247,
                  "acc_stderr,none": 0.007956917285722405,
                  "alias": " - global_mmlu_full_de_stem"
                }
              },
              "group_subtasks": {
                "global_mmlu_full_de_humanities": [
                  "global_mmlu_full_de_professional_law",
                  "global_mmlu_full_de_international_law",
                  "global_mmlu_full_de_prehistory",
                  "global_mmlu_full_de_world_religions",
                  "global_mmlu_full_de_high_school_world_history",
                  "global_mmlu_full_de_high_school_us_history",
                  "global_mmlu_full_de_jurisprudence",
                  "global_mmlu_full_de_formal_logic",
                  "global_mmlu_full_de_logical_fallacies",
                  "global_mmlu_full_de_high_school_european_history",
                  "global_mmlu_full_de_philosophy",
                  "global_mmlu_full_de_moral_scenarios",
                  "global_mmlu_full_de_moral_disputes"
                ],
                "global_mmlu_full_de_social_sciences": [
                  "global_mmlu_full_de_high_school_psychology",
                  "global_mmlu_full_de_professional_psychology",
                  "global_mmlu_full_de_high_school_government_and_politics",
                  "global_mmlu_full_de_high_school_macroeconomics",
                  "global_mmlu_full_de_high_school_microeconomics",
                  "global_mmlu_full_de_econometrics",
                  "global_mmlu_full_de_high_school_geography",
                  "global_mmlu_full_de_security_studies",
                  "global_mmlu_full_de_us_foreign_policy",
                  "global_mmlu_full_de_public_relations",
                  "global_mmlu_full_de_human_sexuality",
                  "global_mmlu_full_de_sociology"
                ],
                "global_mmlu_full_de_other": [
                  "global_mmlu_full_de_nutrition",
                  "global_mmlu_full_de_professional_medicine",
                  "global_mmlu_full_de_college_medicine",
                  "global_mmlu_full_de_professional_accounting",
                  "global_mmlu_full_de_marketing",
                  "global_mmlu_full_de_miscellaneous",
                  "global_mmlu_full_de_business_ethics",
                  "global_mmlu_full_de_virology",
                  "global_mmlu_full_de_medical_genetics",
                  "global_mmlu_full_de_human_aging",
                  "global_mmlu_full_de_global_facts",
                  "global_mmlu_full_de_clinical_knowledge",
                  "global_mmlu_full_de_management"
                ],
                "global_mmlu_full_de_stem": [
                  "global_mmlu_full_de_computer_security",
                  "global_mmlu_full_de_college_biology",
                  "global_mmlu_full_de_college_mathematics",
                  "global_mmlu_full_de_conceptual_physics",
                  "global_mmlu_full_de_elementary_mathematics",
                  "global_mmlu_full_de_astronomy",
                  "global_mmlu_full_de_high_school_physics",
                  "global_mmlu_full_de_machine_learning",
                  "global_mmlu_full_de_college_chemistry",
                  "global_mmlu_full_de_high_school_chemistry",
                  "global_mmlu_full_de_college_computer_science",
                  "global_mmlu_full_de_high_school_computer_science",
                  "global_mmlu_full_de_high_school_mathematics",
                  "global_mmlu_full_de_abstract_algebra",
                  "global_mmlu_full_de_high_school_biology",
                  "global_mmlu_full_de_electrical_engineering",
                  "global_mmlu_full_de_college_physics",
                  "global_mmlu_full_de_anatomy",
                  "global_mmlu_full_de_high_school_statistics"
                ],
                "global_mmlu_full_de": [
                  "global_mmlu_full_de_stem",
                  "global_mmlu_full_de_other",
                  "global_mmlu_full_de_social_sciences",
                  "global_mmlu_full_de_humanities"
                ]
              },
              "configs": {
                "global_mmlu_full_de_abstract_algebra": {
                  "task": "global_mmlu_full_de_abstract_algebra",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e2057b490>, subject='abstract_algebra')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_anatomy": {
                  "task": "global_mmlu_full_de_anatomy",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e20578ca0>, subject='anatomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_astronomy": {
                  "task": "global_mmlu_full_de_astronomy",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e20579000>, subject='astronomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_business_ethics": {
                  "task": "global_mmlu_full_de_business_ethics",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a45e0>, subject='business_ethics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "task": "global_mmlu_full_de_clinical_knowledge",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a64d0>, subject='clinical_knowledge')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_biology": {
                  "task": "global_mmlu_full_de_college_biology",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e2057a3b0>, subject='college_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_chemistry": {
                  "task": "global_mmlu_full_de_college_chemistry",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e2057b7f0>, subject='college_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_computer_science": {
                  "task": "global_mmlu_full_de_college_computer_science",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e2057b5b0>, subject='college_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_mathematics": {
                  "task": "global_mmlu_full_de_college_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e294015a0>, subject='college_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_medicine": {
                  "task": "global_mmlu_full_de_college_medicine",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a4700>, subject='college_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_college_physics": {
                  "task": "global_mmlu_full_de_college_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e2057ad40>, subject='college_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_computer_security": {
                  "task": "global_mmlu_full_de_computer_security",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e20579ea0>, subject='computer_security')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "task": "global_mmlu_full_de_conceptual_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e2057aef0>, subject='conceptual_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_econometrics": {
                  "task": "global_mmlu_full_de_econometrics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a5360>, subject='econometrics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "task": "global_mmlu_full_de_electrical_engineering",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e2057a440>, subject='electrical_engineering')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "task": "global_mmlu_full_de_elementary_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e2057b6d0>, subject='elementary_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_formal_logic": {
                  "task": "global_mmlu_full_de_formal_logic",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7750b4188550>, subject='formal_logic')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_global_facts": {
                  "task": "global_mmlu_full_de_global_facts",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a4790>, subject='global_facts')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_biology": {
                  "task": "global_mmlu_full_de_high_school_biology",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e2057aa70>, subject='high_school_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "task": "global_mmlu_full_de_high_school_chemistry",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e2057b1c0>, subject='high_school_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "task": "global_mmlu_full_de_high_school_computer_science",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e2057a680>, subject='high_school_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "task": "global_mmlu_full_de_high_school_european_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7750b41896c0>, subject='high_school_european_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_geography": {
                  "task": "global_mmlu_full_de_high_school_geography",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7750b418b910>, subject='high_school_geography')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "task": "global_mmlu_full_de_high_school_government_and_politics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a5090>, subject='high_school_government_and_politics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "task": "global_mmlu_full_de_high_school_macroeconomics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a65f0>, subject='high_school_macroeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "task": "global_mmlu_full_de_high_school_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e205791b0>, subject='high_school_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "task": "global_mmlu_full_de_high_school_microeconomics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a4e50>, subject='high_school_microeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_physics": {
                  "task": "global_mmlu_full_de_high_school_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e2057be20>, subject='high_school_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "task": "global_mmlu_full_de_high_school_psychology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a6f80>, subject='high_school_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "task": "global_mmlu_full_de_high_school_statistics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e20578160>, subject='high_school_statistics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "task": "global_mmlu_full_de_high_school_us_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7750b418a9e0>, subject='high_school_us_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "task": "global_mmlu_full_de_high_school_world_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7750b4188790>, subject='high_school_world_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_human_aging": {
                  "task": "global_mmlu_full_de_human_aging",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a5c60>, subject='human_aging')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_human_sexuality": {
                  "task": "global_mmlu_full_de_human_sexuality",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7750b4189c60>, subject='human_sexuality')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_international_law": {
                  "task": "global_mmlu_full_de_international_law",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7750b4189480>, subject='international_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_jurisprudence": {
                  "task": "global_mmlu_full_de_jurisprudence",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7750b418b0a0>, subject='jurisprudence')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "task": "global_mmlu_full_de_logical_fallacies",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7750b41885e0>, subject='logical_fallacies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_machine_learning": {
                  "task": "global_mmlu_full_de_machine_learning",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e2057b130>, subject='machine_learning')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_management": {
                  "task": "global_mmlu_full_de_management",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a70a0>, subject='management')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_marketing": {
                  "task": "global_mmlu_full_de_marketing",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a72e0>, subject='marketing')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_medical_genetics": {
                  "task": "global_mmlu_full_de_medical_genetics",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a79a0>, subject='medical_genetics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_miscellaneous": {
                  "task": "global_mmlu_full_de_miscellaneous",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a6440>, subject='miscellaneous')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_moral_disputes": {
                  "task": "global_mmlu_full_de_moral_disputes",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8fdf0160>, subject='moral_disputes')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "task": "global_mmlu_full_de_moral_scenarios",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7750b4188f70>, subject='moral_scenarios')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_nutrition": {
                  "task": "global_mmlu_full_de_nutrition",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e2057a0e0>, subject='nutrition')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_philosophy": {
                  "task": "global_mmlu_full_de_philosophy",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7750b8d19990>, subject='philosophy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_prehistory": {
                  "task": "global_mmlu_full_de_prehistory",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7750b4188b80>, subject='prehistory')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_professional_accounting": {
                  "task": "global_mmlu_full_de_professional_accounting",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a5f30>, subject='professional_accounting')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_professional_law": {
                  "task": "global_mmlu_full_de_professional_law",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7750b4188c10>, subject='professional_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_professional_medicine": {
                  "task": "global_mmlu_full_de_professional_medicine",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e205783a0>, subject='professional_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_professional_psychology": {
                  "task": "global_mmlu_full_de_professional_psychology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a5000>, subject='professional_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_public_relations": {
                  "task": "global_mmlu_full_de_public_relations",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7750b418a320>, subject='public_relations')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_security_studies": {
                  "task": "global_mmlu_full_de_security_studies",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7750b4189360>, subject='security_studies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_sociology": {
                  "task": "global_mmlu_full_de_sociology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7750b418b520>, subject='sociology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "task": "global_mmlu_full_de_us_foreign_policy",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a43a0>, subject='us_foreign_policy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_virology": {
                  "task": "global_mmlu_full_de_virology",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x774e8c1a4c10>, subject='virology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                },
                "global_mmlu_full_de_world_religions": {
                  "task": "global_mmlu_full_de_world_religions",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x7750b418aef0>, subject='world_religions')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
                    "trust_remote_code": true,
                    "local_files_only": true
                  }
                }
              },
              "versions": {
                "global_mmlu_full_de": 0.0,
                "global_mmlu_full_de_abstract_algebra": 0.0,
                "global_mmlu_full_de_anatomy": 0.0,
                "global_mmlu_full_de_astronomy": 0.0,
                "global_mmlu_full_de_business_ethics": 0.0,
                "global_mmlu_full_de_clinical_knowledge": 0.0,
                "global_mmlu_full_de_college_biology": 0.0,
                "global_mmlu_full_de_college_chemistry": 0.0,
                "global_mmlu_full_de_college_computer_science": 0.0,
                "global_mmlu_full_de_college_mathematics": 0.0,
                "global_mmlu_full_de_college_medicine": 0.0,
                "global_mmlu_full_de_college_physics": 0.0,
                "global_mmlu_full_de_computer_security": 0.0,
                "global_mmlu_full_de_conceptual_physics": 0.0,
                "global_mmlu_full_de_econometrics": 0.0,
                "global_mmlu_full_de_electrical_engineering": 0.0,
                "global_mmlu_full_de_elementary_mathematics": 0.0,
                "global_mmlu_full_de_formal_logic": 0.0,
                "global_mmlu_full_de_global_facts": 0.0,
                "global_mmlu_full_de_high_school_biology": 0.0,
                "global_mmlu_full_de_high_school_chemistry": 0.0,
                "global_mmlu_full_de_high_school_computer_science": 0.0,
                "global_mmlu_full_de_high_school_european_history": 0.0,
                "global_mmlu_full_de_high_school_geography": 0.0,
                "global_mmlu_full_de_high_school_government_and_politics": 0.0,
                "global_mmlu_full_de_high_school_macroeconomics": 0.0,
                "global_mmlu_full_de_high_school_mathematics": 0.0,
                "global_mmlu_full_de_high_school_microeconomics": 0.0,
                "global_mmlu_full_de_high_school_physics": 0.0,
                "global_mmlu_full_de_high_school_psychology": 0.0,
                "global_mmlu_full_de_high_school_statistics": 0.0,
                "global_mmlu_full_de_high_school_us_history": 0.0,
                "global_mmlu_full_de_high_school_world_history": 0.0,
                "global_mmlu_full_de_human_aging": 0.0,
                "global_mmlu_full_de_human_sexuality": 0.0,
                "global_mmlu_full_de_humanities": 0.0,
                "global_mmlu_full_de_international_law": 0.0,
                "global_mmlu_full_de_jurisprudence": 0.0,
                "global_mmlu_full_de_logical_fallacies": 0.0,
                "global_mmlu_full_de_machine_learning": 0.0,
                "global_mmlu_full_de_management": 0.0,
                "global_mmlu_full_de_marketing": 0.0,
                "global_mmlu_full_de_medical_genetics": 0.0,
                "global_mmlu_full_de_miscellaneous": 0.0,
                "global_mmlu_full_de_moral_disputes": 0.0,
                "global_mmlu_full_de_moral_scenarios": 0.0,
                "global_mmlu_full_de_nutrition": 0.0,
                "global_mmlu_full_de_other": 0.0,
                "global_mmlu_full_de_philosophy": 0.0,
                "global_mmlu_full_de_prehistory": 0.0,
                "global_mmlu_full_de_professional_accounting": 0.0,
                "global_mmlu_full_de_professional_law": 0.0,
                "global_mmlu_full_de_professional_medicine": 0.0,
                "global_mmlu_full_de_professional_psychology": 0.0,
                "global_mmlu_full_de_public_relations": 0.0,
                "global_mmlu_full_de_security_studies": 0.0,
                "global_mmlu_full_de_social_sciences": 0.0,
                "global_mmlu_full_de_sociology": 0.0,
                "global_mmlu_full_de_stem": 0.0,
                "global_mmlu_full_de_us_foreign_policy": 0.0,
                "global_mmlu_full_de_virology": 0.0,
                "global_mmlu_full_de_world_religions": 0.0
              },
              "n-shot": {
                "global_mmlu_full_de_abstract_algebra": 0,
                "global_mmlu_full_de_anatomy": 0,
                "global_mmlu_full_de_astronomy": 0,
                "global_mmlu_full_de_business_ethics": 0,
                "global_mmlu_full_de_clinical_knowledge": 0,
                "global_mmlu_full_de_college_biology": 0,
                "global_mmlu_full_de_college_chemistry": 0,
                "global_mmlu_full_de_college_computer_science": 0,
                "global_mmlu_full_de_college_mathematics": 0,
                "global_mmlu_full_de_college_medicine": 0,
                "global_mmlu_full_de_college_physics": 0,
                "global_mmlu_full_de_computer_security": 0,
                "global_mmlu_full_de_conceptual_physics": 0,
                "global_mmlu_full_de_econometrics": 0,
                "global_mmlu_full_de_electrical_engineering": 0,
                "global_mmlu_full_de_elementary_mathematics": 0,
                "global_mmlu_full_de_formal_logic": 0,
                "global_mmlu_full_de_global_facts": 0,
                "global_mmlu_full_de_high_school_biology": 0,
                "global_mmlu_full_de_high_school_chemistry": 0,
                "global_mmlu_full_de_high_school_computer_science": 0,
                "global_mmlu_full_de_high_school_european_history": 0,
                "global_mmlu_full_de_high_school_geography": 0,
                "global_mmlu_full_de_high_school_government_and_politics": 0,
                "global_mmlu_full_de_high_school_macroeconomics": 0,
                "global_mmlu_full_de_high_school_mathematics": 0,
                "global_mmlu_full_de_high_school_microeconomics": 0,
                "global_mmlu_full_de_high_school_physics": 0,
                "global_mmlu_full_de_high_school_psychology": 0,
                "global_mmlu_full_de_high_school_statistics": 0,
                "global_mmlu_full_de_high_school_us_history": 0,
                "global_mmlu_full_de_high_school_world_history": 0,
                "global_mmlu_full_de_human_aging": 0,
                "global_mmlu_full_de_human_sexuality": 0,
                "global_mmlu_full_de_international_law": 0,
                "global_mmlu_full_de_jurisprudence": 0,
                "global_mmlu_full_de_logical_fallacies": 0,
                "global_mmlu_full_de_machine_learning": 0,
                "global_mmlu_full_de_management": 0,
                "global_mmlu_full_de_marketing": 0,
                "global_mmlu_full_de_medical_genetics": 0,
                "global_mmlu_full_de_miscellaneous": 0,
                "global_mmlu_full_de_moral_disputes": 0,
                "global_mmlu_full_de_moral_scenarios": 0,
                "global_mmlu_full_de_nutrition": 0,
                "global_mmlu_full_de_philosophy": 0,
                "global_mmlu_full_de_prehistory": 0,
                "global_mmlu_full_de_professional_accounting": 0,
                "global_mmlu_full_de_professional_law": 0,
                "global_mmlu_full_de_professional_medicine": 0,
                "global_mmlu_full_de_professional_psychology": 0,
                "global_mmlu_full_de_public_relations": 0,
                "global_mmlu_full_de_security_studies": 0,
                "global_mmlu_full_de_sociology": 0,
                "global_mmlu_full_de_us_foreign_policy": 0,
                "global_mmlu_full_de_virology": 0,
                "global_mmlu_full_de_world_religions": 0
              },
              "higher_is_better": {
                "global_mmlu_full_de": {
                  "acc": true
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "acc": true
                },
                "global_mmlu_full_de_anatomy": {
                  "acc": true
                },
                "global_mmlu_full_de_astronomy": {
                  "acc": true
                },
                "global_mmlu_full_de_business_ethics": {
                  "acc": true
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "acc": true
                },
                "global_mmlu_full_de_college_biology": {
                  "acc": true
                },
                "global_mmlu_full_de_college_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_de_college_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_de_college_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_college_medicine": {
                  "acc": true
                },
                "global_mmlu_full_de_college_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_computer_security": {
                  "acc": true
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_econometrics": {
                  "acc": true
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "acc": true
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_formal_logic": {
                  "acc": true
                },
                "global_mmlu_full_de_global_facts": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_biology": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_geography": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "acc": true
                },
                "global_mmlu_full_de_human_aging": {
                  "acc": true
                },
                "global_mmlu_full_de_human_sexuality": {
                  "acc": true
                },
                "global_mmlu_full_de_humanities": {
                  "acc": true
                },
                "global_mmlu_full_de_international_law": {
                  "acc": true
                },
                "global_mmlu_full_de_jurisprudence": {
                  "acc": true
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "acc": true
                },
                "global_mmlu_full_de_machine_learning": {
                  "acc": true
                },
                "global_mmlu_full_de_management": {
                  "acc": true
                },
                "global_mmlu_full_de_marketing": {
                  "acc": true
                },
                "global_mmlu_full_de_medical_genetics": {
                  "acc": true
                },
                "global_mmlu_full_de_miscellaneous": {
                  "acc": true
                },
                "global_mmlu_full_de_moral_disputes": {
                  "acc": true
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "acc": true
                },
                "global_mmlu_full_de_nutrition": {
                  "acc": true
                },
                "global_mmlu_full_de_other": {
                  "acc": true
                },
                "global_mmlu_full_de_philosophy": {
                  "acc": true
                },
                "global_mmlu_full_de_prehistory": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_accounting": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_law": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_medicine": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_psychology": {
                  "acc": true
                },
                "global_mmlu_full_de_public_relations": {
                  "acc": true
                },
                "global_mmlu_full_de_security_studies": {
                  "acc": true
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc": true
                },
                "global_mmlu_full_de_sociology": {
                  "acc": true
                },
                "global_mmlu_full_de_stem": {
                  "acc": true
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "acc": true
                },
                "global_mmlu_full_de_virology": {
                  "acc": true
                },
                "global_mmlu_full_de_world_religions": {
                  "acc": true
                }
              },
              "n-samples": {
                "global_mmlu_full_de_computer_security": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_college_biology": {
                  "original": 144,
                  "effective": 144
                },
                "global_mmlu_full_de_college_mathematics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "original": 235,
                  "effective": 235
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "original": 378,
                  "effective": 378
                },
                "global_mmlu_full_de_astronomy": {
                  "original": 152,
                  "effective": 152
                },
                "global_mmlu_full_de_high_school_physics": {
                  "original": 151,
                  "effective": 151
                },
                "global_mmlu_full_de_machine_learning": {
                  "original": 112,
                  "effective": 112
                },
                "global_mmlu_full_de_college_chemistry": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "original": 203,
                  "effective": 203
                },
                "global_mmlu_full_de_college_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "original": 270,
                  "effective": 270
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_biology": {
                  "original": 310,
                  "effective": 310
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "original": 145,
                  "effective": 145
                },
                "global_mmlu_full_de_college_physics": {
                  "original": 102,
                  "effective": 102
                },
                "global_mmlu_full_de_anatomy": {
                  "original": 135,
                  "effective": 135
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "original": 216,
                  "effective": 216
                },
                "global_mmlu_full_de_nutrition": {
                  "original": 306,
                  "effective": 306
                },
                "global_mmlu_full_de_professional_medicine": {
                  "original": 272,
                  "effective": 272
                },
                "global_mmlu_full_de_college_medicine": {
                  "original": 173,
                  "effective": 173
                },
                "global_mmlu_full_de_professional_accounting": {
                  "original": 282,
                  "effective": 282
                },
                "global_mmlu_full_de_marketing": {
                  "original": 234,
                  "effective": 234
                },
                "global_mmlu_full_de_miscellaneous": {
                  "original": 783,
                  "effective": 783
                },
                "global_mmlu_full_de_business_ethics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_virology": {
                  "original": 166,
                  "effective": 166
                },
                "global_mmlu_full_de_medical_genetics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_human_aging": {
                  "original": 223,
                  "effective": 223
                },
                "global_mmlu_full_de_global_facts": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "original": 265,
                  "effective": 265
                },
                "global_mmlu_full_de_management": {
                  "original": 103,
                  "effective": 103
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "original": 545,
                  "effective": 545
                },
                "global_mmlu_full_de_professional_psychology": {
                  "original": 612,
                  "effective": 612
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "original": 193,
                  "effective": 193
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "original": 390,
                  "effective": 390
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "original": 238,
                  "effective": 238
                },
                "global_mmlu_full_de_econometrics": {
                  "original": 114,
                  "effective": 114
                },
                "global_mmlu_full_de_high_school_geography": {
                  "original": 198,
                  "effective": 198
                },
                "global_mmlu_full_de_security_studies": {
                  "original": 245,
                  "effective": 245
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_public_relations": {
                  "original": 110,
                  "effective": 110
                },
                "global_mmlu_full_de_human_sexuality": {
                  "original": 131,
                  "effective": 131
                },
                "global_mmlu_full_de_sociology": {
                  "original": 201,
                  "effective": 201
                },
                "global_mmlu_full_de_professional_law": {
                  "original": 1534,
                  "effective": 1534
                },
                "global_mmlu_full_de_international_law": {
                  "original": 121,
                  "effective": 121
                },
                "global_mmlu_full_de_prehistory": {
                  "original": 324,
                  "effective": 324
                },
                "global_mmlu_full_de_world_religions": {
                  "original": 171,
                  "effective": 171
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "original": 237,
                  "effective": 237
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "original": 204,
                  "effective": 204
                },
                "global_mmlu_full_de_jurisprudence": {
                  "original": 108,
                  "effective": 108
                },
                "global_mmlu_full_de_formal_logic": {
                  "original": 126,
                  "effective": 126
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "original": 163,
                  "effective": 163
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "original": 165,
                  "effective": 165
                },
                "global_mmlu_full_de_philosophy": {
                  "original": 311,
                  "effective": 311
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "original": 895,
                  "effective": 895
                },
                "global_mmlu_full_de_moral_disputes": {
                  "original": 346,
                  "effective": 346
                }
              },
              "config": {
                "model": "hf",
                "model_args": "pretrained=/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k,trust_remote_code=True,local_files_only=True,trust_remote_code=True",
                "model_num_parameters": 222903552,
                "model_dtype": "torch.float32",
                "model_revision": "main",
                "model_sha": "",
                "batch_size": "auto",
                "batch_sizes": [
                  64
                ],
                "device": "cuda",
                "use_cache": null,
                "limit": null,
                "bootstrap_iters": 100000,
                "gen_kwargs": null,
                "random_seed": 42,
                "numpy_seed": 42,
                "torch_seed": 42,
                "fewshot_seed": 42
              },
              "git_hash": "a4103c7",
              "date": 1758648742.951584,
              "pretty_env_info": "PyTorch version: 2.7.1+cu126\nIs debug build: False\nCUDA used to build PyTorch: 12.6\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.2 LTS (x86_64)\nGCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\nClang version: Could not collect\nCMake version: version 3.24.1\nLibc version: glibc-2.35\n\nPython version: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] (64-bit runtime)\nPython platform: Linux-6.8.0-1036-nvidia-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H200\nNvidia driver version: 580.82.07\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               224\nOn-line CPU(s) list:                  0-223\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480C\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   56\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          3800.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            5.3 MiB (112 instances)\nL1i cache:                            3.5 MiB (112 instances)\nL2 cache:                             224 MiB (112 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-55,112-167\nNUMA node1 CPU(s):                    56-111,168-223\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pytorch-lightning==2.5.2\n[pip3] torch==2.7.1\n[pip3] torchmetrics==1.8.0\n[pip3] triton==3.3.1\n[conda] Could not collect",
              "transformers_version": "4.54.1",
              "lm_eval_version": "0.4.9",
              "upper_git_hash": null,
              "tokenizer_pad_token": [
                "<pad>",
                "0"
              ],
              "tokenizer_eos_token": [
                "</s>",
                "1"
              ],
              "tokenizer_bos_token": [
                null,
                "None"
              ],
              "eot_token_id": 1,
              "max_length": 512,
              "task_hashes": {},
              "model_source": "hf",
              "model_name": "/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k",
              "model_name_sanitized": "__netscratch__nrauscher__projects__BA-hydra__evaluation__models__temp__gold-continued-pretraind-on-english-487k",
              "system_instruction": null,
              "system_instruction_sha": null,
              "fewshot_as_multiturn": false,
              "chat_template": null,
              "chat_template_sha": null,
              "start_time": 634886.189945359,
              "end_time": 635237.92923796,
              "total_evaluation_time_seconds": "351.73929260100704"
            },
            "duration_seconds": 400.74186086654663,
            "duration_minutes": 6.679031014442444,
            "command": "python -m lm_eval --model hf --model_args pretrained=/netscratch/nrauscher/projects/BA-hydra/evaluation/models/temp/gold-continued-pretraind-on-english-487k,trust_remote_code=True,local_files_only=True --tasks global_mmlu_full_de --device cuda --batch_size auto --output_path /netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/gold-continued-pretraind-on-english-487k_global_mmlu_de_0shot_20250923_193047 --num_fewshot 0 --trust_remote_code --seed 42 --wandb_args project=BA-T5-CrossLingual,entity=nikolas-rauscher-dfki,group=crosslingual_transfer_eval_full_15k-final,name=gold-continued-pretraind-on-english-487k_global_mmlu_de_0shot",
            "output_dir": "/netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/gold-continued-pretraind-on-english-487k_global_mmlu_de_0shot_20250923_193047",
            "status": "success"
          }
        }
      }
    },
    "t5-base": {
      "model_config": {
        "source_path": "t5-base",
        "name": "t5-base"
      },
      "model_path": "t5-base",
      "metadata": {
        "source_type": "huggingface",
        "original_source": "t5-base"
      },
      "benchmarks": {
        "global_mmlu_en": {
          "0_shot": {
            "parse_status": "partial",
            "error": "Could not find primary metric",
            "raw_results": {
              "results": {
                "global_mmlu_full_en": {
                  "acc,none": 0.22895598917533114,
                  "acc_stderr,none": 0.00354046338086419,
                  "alias": "global_mmlu_full_en"
                },
                "global_mmlu_full_en_humanities": {
                  "acc,none": 0.2405951115834219,
                  "acc_stderr,none": 0.006230437150944743,
                  "alias": " - global_mmlu_full_en_humanities"
                },
                "global_mmlu_full_en_formal_logic": {
                  "alias": "  - global_mmlu_full_en_formal_logic",
                  "acc,none": 0.2698412698412698,
                  "acc_stderr,none": 0.03970158273235172
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "alias": "  - global_mmlu_full_en_high_school_european_history",
                  "acc,none": 0.21818181818181817,
                  "acc_stderr,none": 0.03225078108306289
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "alias": "  - global_mmlu_full_en_high_school_us_history",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.03039153369274154
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "alias": "  - global_mmlu_full_en_high_school_world_history",
                  "acc,none": 0.26582278481012656,
                  "acc_stderr,none": 0.028756799629658335
                },
                "global_mmlu_full_en_international_law": {
                  "alias": "  - global_mmlu_full_en_international_law",
                  "acc,none": 0.2396694214876033,
                  "acc_stderr,none": 0.03896878985070417
                },
                "global_mmlu_full_en_jurisprudence": {
                  "alias": "  - global_mmlu_full_en_jurisprudence",
                  "acc,none": 0.24074074074074073,
                  "acc_stderr,none": 0.04133119440243838
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "alias": "  - global_mmlu_full_en_logical_fallacies",
                  "acc,none": 0.22085889570552147,
                  "acc_stderr,none": 0.032591773927421776
                },
                "global_mmlu_full_en_moral_disputes": {
                  "alias": "  - global_mmlu_full_en_moral_disputes",
                  "acc,none": 0.24566473988439305,
                  "acc_stderr,none": 0.02317629820399201
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "alias": "  - global_mmlu_full_en_moral_scenarios",
                  "acc,none": 0.23798882681564246,
                  "acc_stderr,none": 0.014242630070574885
                },
                "global_mmlu_full_en_philosophy": {
                  "alias": "  - global_mmlu_full_en_philosophy",
                  "acc,none": 0.1864951768488746,
                  "acc_stderr,none": 0.02212243977248077
                },
                "global_mmlu_full_en_prehistory": {
                  "alias": "  - global_mmlu_full_en_prehistory",
                  "acc,none": 0.21604938271604937,
                  "acc_stderr,none": 0.022899162918445806
                },
                "global_mmlu_full_en_professional_law": {
                  "alias": "  - global_mmlu_full_en_professional_law",
                  "acc,none": 0.24511082138200782,
                  "acc_stderr,none": 0.010986307870045507
                },
                "global_mmlu_full_en_world_religions": {
                  "alias": "  - global_mmlu_full_en_world_religions",
                  "acc,none": 0.3216374269005848,
                  "acc_stderr,none": 0.03582529442573122
                },
                "global_mmlu_full_en_other": {
                  "acc,none": 0.23913743160605086,
                  "acc_stderr,none": 0.00763619665854928,
                  "alias": " - global_mmlu_full_en_other"
                },
                "global_mmlu_full_en_business_ethics": {
                  "alias": "  - global_mmlu_full_en_business_ethics",
                  "acc,none": 0.3,
                  "acc_stderr,none": 0.046056618647183814
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "alias": "  - global_mmlu_full_en_clinical_knowledge",
                  "acc,none": 0.2188679245283019,
                  "acc_stderr,none": 0.025447863825108604
                },
                "global_mmlu_full_en_college_medicine": {
                  "alias": "  - global_mmlu_full_en_college_medicine",
                  "acc,none": 0.20809248554913296,
                  "acc_stderr,none": 0.030952890217749884
                },
                "global_mmlu_full_en_global_facts": {
                  "alias": "  - global_mmlu_full_en_global_facts",
                  "acc,none": 0.18,
                  "acc_stderr,none": 0.038612291966536955
                },
                "global_mmlu_full_en_human_aging": {
                  "alias": "  - global_mmlu_full_en_human_aging",
                  "acc,none": 0.3094170403587444,
                  "acc_stderr,none": 0.031024411740572206
                },
                "global_mmlu_full_en_management": {
                  "alias": "  - global_mmlu_full_en_management",
                  "acc,none": 0.17475728155339806,
                  "acc_stderr,none": 0.03760178006026621
                },
                "global_mmlu_full_en_marketing": {
                  "alias": "  - global_mmlu_full_en_marketing",
                  "acc,none": 0.2905982905982906,
                  "acc_stderr,none": 0.029745048572674057
                },
                "global_mmlu_full_en_medical_genetics": {
                  "alias": "  - global_mmlu_full_en_medical_genetics",
                  "acc,none": 0.3,
                  "acc_stderr,none": 0.046056618647183814
                },
                "global_mmlu_full_en_miscellaneous": {
                  "alias": "  - global_mmlu_full_en_miscellaneous",
                  "acc,none": 0.23627075351213284,
                  "acc_stderr,none": 0.015190473717037509
                },
                "global_mmlu_full_en_nutrition": {
                  "alias": "  - global_mmlu_full_en_nutrition",
                  "acc,none": 0.2222222222222222,
                  "acc_stderr,none": 0.023805186524888142
                },
                "global_mmlu_full_en_professional_accounting": {
                  "alias": "  - global_mmlu_full_en_professional_accounting",
                  "acc,none": 0.23404255319148937,
                  "acc_stderr,none": 0.025257861359432407
                },
                "global_mmlu_full_en_professional_medicine": {
                  "alias": "  - global_mmlu_full_en_professional_medicine",
                  "acc,none": 0.18382352941176472,
                  "acc_stderr,none": 0.02352924218519311
                },
                "global_mmlu_full_en_virology": {
                  "alias": "  - global_mmlu_full_en_virology",
                  "acc,none": 0.28313253012048195,
                  "acc_stderr,none": 0.03507295431370518
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc,none": 0.216769580760481,
                  "acc_stderr,none": 0.007426216140417928,
                  "alias": " - global_mmlu_full_en_social_sciences"
                },
                "global_mmlu_full_en_econometrics": {
                  "alias": "  - global_mmlu_full_en_econometrics",
                  "acc,none": 0.23684210526315788,
                  "acc_stderr,none": 0.039994238792813386
                },
                "global_mmlu_full_en_high_school_geography": {
                  "alias": "  - global_mmlu_full_en_high_school_geography",
                  "acc,none": 0.17676767676767677,
                  "acc_stderr,none": 0.027178752639044915
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "alias": "  - global_mmlu_full_en_high_school_government_and_politics",
                  "acc,none": 0.19689119170984457,
                  "acc_stderr,none": 0.02869787397186069
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "alias": "  - global_mmlu_full_en_high_school_macroeconomics",
                  "acc,none": 0.20256410256410257,
                  "acc_stderr,none": 0.020377660970371397
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "alias": "  - global_mmlu_full_en_high_school_microeconomics",
                  "acc,none": 0.21008403361344538,
                  "acc_stderr,none": 0.026461398717471874
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "alias": "  - global_mmlu_full_en_high_school_psychology",
                  "acc,none": 0.1963302752293578,
                  "acc_stderr,none": 0.017030719339154336
                },
                "global_mmlu_full_en_human_sexuality": {
                  "alias": "  - global_mmlu_full_en_human_sexuality",
                  "acc,none": 0.2595419847328244,
                  "acc_stderr,none": 0.03844876139785271
                },
                "global_mmlu_full_en_professional_psychology": {
                  "alias": "  - global_mmlu_full_en_professional_psychology",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.01751781884501444
                },
                "global_mmlu_full_en_public_relations": {
                  "alias": "  - global_mmlu_full_en_public_relations",
                  "acc,none": 0.21818181818181817,
                  "acc_stderr,none": 0.03955932861795833
                },
                "global_mmlu_full_en_security_studies": {
                  "alias": "  - global_mmlu_full_en_security_studies",
                  "acc,none": 0.18775510204081633,
                  "acc_stderr,none": 0.02500025603954622
                },
                "global_mmlu_full_en_sociology": {
                  "alias": "  - global_mmlu_full_en_sociology",
                  "acc,none": 0.22885572139303484,
                  "acc_stderr,none": 0.029705284056772422
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "alias": "  - global_mmlu_full_en_us_foreign_policy",
                  "acc,none": 0.28,
                  "acc_stderr,none": 0.045126085985421276
                },
                "global_mmlu_full_en_stem": {
                  "acc,none": 0.2134475103076435,
                  "acc_stderr,none": 0.007281774955070491,
                  "alias": " - global_mmlu_full_en_stem"
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "alias": "  - global_mmlu_full_en_abstract_algebra",
                  "acc,none": 0.22,
                  "acc_stderr,none": 0.04163331998932269
                },
                "global_mmlu_full_en_anatomy": {
                  "alias": "  - global_mmlu_full_en_anatomy",
                  "acc,none": 0.18518518518518517,
                  "acc_stderr,none": 0.03355677216313142
                },
                "global_mmlu_full_en_astronomy": {
                  "alias": "  - global_mmlu_full_en_astronomy",
                  "acc,none": 0.17763157894736842,
                  "acc_stderr,none": 0.031103182383123398
                },
                "global_mmlu_full_en_college_biology": {
                  "alias": "  - global_mmlu_full_en_college_biology",
                  "acc,none": 0.2708333333333333,
                  "acc_stderr,none": 0.037161774375660164
                },
                "global_mmlu_full_en_college_chemistry": {
                  "alias": "  - global_mmlu_full_en_college_chemistry",
                  "acc,none": 0.2,
                  "acc_stderr,none": 0.040201512610368445
                },
                "global_mmlu_full_en_college_computer_science": {
                  "alias": "  - global_mmlu_full_en_college_computer_science",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.04351941398892446
                },
                "global_mmlu_full_en_college_mathematics": {
                  "alias": "  - global_mmlu_full_en_college_mathematics",
                  "acc,none": 0.21,
                  "acc_stderr,none": 0.040936018074033256
                },
                "global_mmlu_full_en_college_physics": {
                  "alias": "  - global_mmlu_full_en_college_physics",
                  "acc,none": 0.21568627450980393,
                  "acc_stderr,none": 0.040925639582376556
                },
                "global_mmlu_full_en_computer_security": {
                  "alias": "  - global_mmlu_full_en_computer_security",
                  "acc,none": 0.29,
                  "acc_stderr,none": 0.045604802157206845
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "alias": "  - global_mmlu_full_en_conceptual_physics",
                  "acc,none": 0.2680851063829787,
                  "acc_stderr,none": 0.028957342788342347
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "alias": "  - global_mmlu_full_en_electrical_engineering",
                  "acc,none": 0.23448275862068965,
                  "acc_stderr,none": 0.035306258743465914
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "alias": "  - global_mmlu_full_en_elementary_mathematics",
                  "acc,none": 0.20899470899470898,
                  "acc_stderr,none": 0.020940481565334835
                },
                "global_mmlu_full_en_high_school_biology": {
                  "alias": "  - global_mmlu_full_en_high_school_biology",
                  "acc,none": 0.1774193548387097,
                  "acc_stderr,none": 0.021732540689329265
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "alias": "  - global_mmlu_full_en_high_school_chemistry",
                  "acc,none": 0.15763546798029557,
                  "acc_stderr,none": 0.0256390141311724
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "alias": "  - global_mmlu_full_en_high_school_computer_science",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.04351941398892446
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "alias": "  - global_mmlu_full_en_high_school_mathematics",
                  "acc,none": 0.2111111111111111,
                  "acc_stderr,none": 0.02488211685765508
                },
                "global_mmlu_full_en_high_school_physics": {
                  "alias": "  - global_mmlu_full_en_high_school_physics",
                  "acc,none": 0.1986754966887417,
                  "acc_stderr,none": 0.032578473844367746
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "alias": "  - global_mmlu_full_en_high_school_statistics",
                  "acc,none": 0.1527777777777778,
                  "acc_stderr,none": 0.02453632602613422
                },
                "global_mmlu_full_en_machine_learning": {
                  "alias": "  - global_mmlu_full_en_machine_learning",
                  "acc,none": 0.3125,
                  "acc_stderr,none": 0.043994650575715215
                }
              },
              "groups": {
                "global_mmlu_full_en": {
                  "acc,none": 0.22895598917533114,
                  "acc_stderr,none": 0.00354046338086419,
                  "alias": "global_mmlu_full_en"
                },
                "global_mmlu_full_en_humanities": {
                  "acc,none": 0.2405951115834219,
                  "acc_stderr,none": 0.006230437150944743,
                  "alias": " - global_mmlu_full_en_humanities"
                },
                "global_mmlu_full_en_other": {
                  "acc,none": 0.23913743160605086,
                  "acc_stderr,none": 0.00763619665854928,
                  "alias": " - global_mmlu_full_en_other"
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc,none": 0.216769580760481,
                  "acc_stderr,none": 0.007426216140417928,
                  "alias": " - global_mmlu_full_en_social_sciences"
                },
                "global_mmlu_full_en_stem": {
                  "acc,none": 0.2134475103076435,
                  "acc_stderr,none": 0.007281774955070491,
                  "alias": " - global_mmlu_full_en_stem"
                }
              },
              "group_subtasks": {
                "global_mmlu_full_en_humanities": [
                  "global_mmlu_full_en_world_religions",
                  "global_mmlu_full_en_international_law",
                  "global_mmlu_full_en_moral_scenarios",
                  "global_mmlu_full_en_high_school_us_history",
                  "global_mmlu_full_en_jurisprudence",
                  "global_mmlu_full_en_high_school_world_history",
                  "global_mmlu_full_en_logical_fallacies",
                  "global_mmlu_full_en_philosophy",
                  "global_mmlu_full_en_professional_law",
                  "global_mmlu_full_en_high_school_european_history",
                  "global_mmlu_full_en_formal_logic",
                  "global_mmlu_full_en_moral_disputes",
                  "global_mmlu_full_en_prehistory"
                ],
                "global_mmlu_full_en_social_sciences": [
                  "global_mmlu_full_en_us_foreign_policy",
                  "global_mmlu_full_en_public_relations",
                  "global_mmlu_full_en_econometrics",
                  "global_mmlu_full_en_high_school_microeconomics",
                  "global_mmlu_full_en_human_sexuality",
                  "global_mmlu_full_en_security_studies",
                  "global_mmlu_full_en_professional_psychology",
                  "global_mmlu_full_en_high_school_geography",
                  "global_mmlu_full_en_high_school_government_and_politics",
                  "global_mmlu_full_en_high_school_psychology",
                  "global_mmlu_full_en_high_school_macroeconomics",
                  "global_mmlu_full_en_sociology"
                ],
                "global_mmlu_full_en_other": [
                  "global_mmlu_full_en_clinical_knowledge",
                  "global_mmlu_full_en_medical_genetics",
                  "global_mmlu_full_en_professional_medicine",
                  "global_mmlu_full_en_business_ethics",
                  "global_mmlu_full_en_marketing",
                  "global_mmlu_full_en_global_facts",
                  "global_mmlu_full_en_college_medicine",
                  "global_mmlu_full_en_human_aging",
                  "global_mmlu_full_en_management",
                  "global_mmlu_full_en_nutrition",
                  "global_mmlu_full_en_virology",
                  "global_mmlu_full_en_professional_accounting",
                  "global_mmlu_full_en_miscellaneous"
                ],
                "global_mmlu_full_en_stem": [
                  "global_mmlu_full_en_high_school_statistics",
                  "global_mmlu_full_en_computer_security",
                  "global_mmlu_full_en_machine_learning",
                  "global_mmlu_full_en_abstract_algebra",
                  "global_mmlu_full_en_astronomy",
                  "global_mmlu_full_en_college_computer_science",
                  "global_mmlu_full_en_high_school_biology",
                  "global_mmlu_full_en_high_school_computer_science",
                  "global_mmlu_full_en_high_school_chemistry",
                  "global_mmlu_full_en_elementary_mathematics",
                  "global_mmlu_full_en_college_physics",
                  "global_mmlu_full_en_high_school_physics",
                  "global_mmlu_full_en_college_chemistry",
                  "global_mmlu_full_en_college_biology",
                  "global_mmlu_full_en_college_mathematics",
                  "global_mmlu_full_en_conceptual_physics",
                  "global_mmlu_full_en_anatomy",
                  "global_mmlu_full_en_high_school_mathematics",
                  "global_mmlu_full_en_electrical_engineering"
                ],
                "global_mmlu_full_en": [
                  "global_mmlu_full_en_stem",
                  "global_mmlu_full_en_other",
                  "global_mmlu_full_en_social_sciences",
                  "global_mmlu_full_en_humanities"
                ]
              },
              "configs": {
                "global_mmlu_full_en_abstract_algebra": {
                  "task": "global_mmlu_full_en_abstract_algebra",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450e8c0>, subject='abstract_algebra')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_anatomy": {
                  "task": "global_mmlu_full_en_anatomy",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450e4d0>, subject='anatomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_astronomy": {
                  "task": "global_mmlu_full_en_astronomy",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450ca60>, subject='astronomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_business_ethics": {
                  "task": "global_mmlu_full_en_business_ethics",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450c160>, subject='business_ethics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "task": "global_mmlu_full_en_clinical_knowledge",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee02464d0>, subject='clinical_knowledge')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_college_biology": {
                  "task": "global_mmlu_full_en_college_biology",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450e200>, subject='college_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_college_chemistry": {
                  "task": "global_mmlu_full_en_college_chemistry",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450e7a0>, subject='college_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_college_computer_science": {
                  "task": "global_mmlu_full_en_college_computer_science",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450f490>, subject='college_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_college_mathematics": {
                  "task": "global_mmlu_full_en_college_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450d2d0>, subject='college_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_college_medicine": {
                  "task": "global_mmlu_full_en_college_medicine",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee02441f0>, subject='college_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_college_physics": {
                  "task": "global_mmlu_full_en_college_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450eb90>, subject='college_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_computer_security": {
                  "task": "global_mmlu_full_en_computer_security",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450ff40>, subject='computer_security')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "task": "global_mmlu_full_en_conceptual_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450d120>, subject='conceptual_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_econometrics": {
                  "task": "global_mmlu_full_en_econometrics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee02467a0>, subject='econometrics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "task": "global_mmlu_full_en_electrical_engineering",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0245a20>, subject='electrical_engineering')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "task": "global_mmlu_full_en_elementary_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450dab0>, subject='elementary_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_formal_logic": {
                  "task": "global_mmlu_full_en_formal_logic",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee07681f0>, subject='formal_logic')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_global_facts": {
                  "task": "global_mmlu_full_en_global_facts",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee02465f0>, subject='global_facts')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_high_school_biology": {
                  "task": "global_mmlu_full_en_high_school_biology",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450d000>, subject='high_school_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "task": "global_mmlu_full_en_high_school_chemistry",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450c430>, subject='high_school_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "task": "global_mmlu_full_en_high_school_computer_science",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450df30>, subject='high_school_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "task": "global_mmlu_full_en_high_school_european_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0978b80>, subject='high_school_european_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_high_school_geography": {
                  "task": "global_mmlu_full_en_high_school_geography",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee097bd00>, subject='high_school_geography')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "task": "global_mmlu_full_en_high_school_government_and_politics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0979630>, subject='high_school_government_and_politics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "task": "global_mmlu_full_en_high_school_macroeconomics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0245120>, subject='high_school_macroeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "task": "global_mmlu_full_en_high_school_mathematics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450d360>, subject='high_school_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "task": "global_mmlu_full_en_high_school_microeconomics",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0244a60>, subject='high_school_microeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_high_school_physics": {
                  "task": "global_mmlu_full_en_high_school_physics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450e710>, subject='high_school_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "task": "global_mmlu_full_en_high_school_psychology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0244430>, subject='high_school_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "task": "global_mmlu_full_en_high_school_statistics",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450ce50>, subject='high_school_statistics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "task": "global_mmlu_full_en_high_school_us_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee097bd90>, subject='high_school_us_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "task": "global_mmlu_full_en_high_school_world_history",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee09789d0>, subject='high_school_world_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_human_aging": {
                  "task": "global_mmlu_full_en_human_aging",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0245d80>, subject='human_aging')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_human_sexuality": {
                  "task": "global_mmlu_full_en_human_sexuality",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0244160>, subject='human_sexuality')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_international_law": {
                  "task": "global_mmlu_full_en_international_law",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee097b2e0>, subject='international_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_jurisprudence": {
                  "task": "global_mmlu_full_en_jurisprudence",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee097a9e0>, subject='jurisprudence')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "task": "global_mmlu_full_en_logical_fallacies",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0979990>, subject='logical_fallacies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_machine_learning": {
                  "task": "global_mmlu_full_en_machine_learning",
                  "tag": "global_mmlu_full_en_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450f370>, subject='machine_learning')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_management": {
                  "task": "global_mmlu_full_en_management",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0247eb0>, subject='management')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_marketing": {
                  "task": "global_mmlu_full_en_marketing",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0247880>, subject='marketing')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_medical_genetics": {
                  "task": "global_mmlu_full_en_medical_genetics",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450c550>, subject='medical_genetics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_miscellaneous": {
                  "task": "global_mmlu_full_en_miscellaneous",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0245630>, subject='miscellaneous')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_moral_disputes": {
                  "task": "global_mmlu_full_en_moral_disputes",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0978790>, subject='moral_disputes')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "task": "global_mmlu_full_en_moral_scenarios",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee097b9a0>, subject='moral_scenarios')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_nutrition": {
                  "task": "global_mmlu_full_en_nutrition",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0245b40>, subject='nutrition')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_philosophy": {
                  "task": "global_mmlu_full_en_philosophy",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0978550>, subject='philosophy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_prehistory": {
                  "task": "global_mmlu_full_en_prehistory",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ef03312d0>, subject='prehistory')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_professional_accounting": {
                  "task": "global_mmlu_full_en_professional_accounting",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0246e60>, subject='professional_accounting')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_professional_law": {
                  "task": "global_mmlu_full_en_professional_law",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee09793f0>, subject='professional_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_professional_medicine": {
                  "task": "global_mmlu_full_en_professional_medicine",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746c6450c0d0>, subject='professional_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_professional_psychology": {
                  "task": "global_mmlu_full_en_professional_psychology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0245750>, subject='professional_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_public_relations": {
                  "task": "global_mmlu_full_en_public_relations",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee02448b0>, subject='public_relations')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_security_studies": {
                  "task": "global_mmlu_full_en_security_studies",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee02451b0>, subject='security_studies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_sociology": {
                  "task": "global_mmlu_full_en_sociology",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0244310>, subject='sociology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "task": "global_mmlu_full_en_us_foreign_policy",
                  "tag": "global_mmlu_full_en_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee0245e10>, subject='us_foreign_policy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_virology": {
                  "task": "global_mmlu_full_en_virology",
                  "tag": "global_mmlu_full_en_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee02463b0>, subject='virology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_en_world_religions": {
                  "task": "global_mmlu_full_en_world_religions",
                  "tag": "global_mmlu_full_en_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "en",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x746ee097a200>, subject='world_religions')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                }
              },
              "versions": {
                "global_mmlu_full_en": 0.0,
                "global_mmlu_full_en_abstract_algebra": 0.0,
                "global_mmlu_full_en_anatomy": 0.0,
                "global_mmlu_full_en_astronomy": 0.0,
                "global_mmlu_full_en_business_ethics": 0.0,
                "global_mmlu_full_en_clinical_knowledge": 0.0,
                "global_mmlu_full_en_college_biology": 0.0,
                "global_mmlu_full_en_college_chemistry": 0.0,
                "global_mmlu_full_en_college_computer_science": 0.0,
                "global_mmlu_full_en_college_mathematics": 0.0,
                "global_mmlu_full_en_college_medicine": 0.0,
                "global_mmlu_full_en_college_physics": 0.0,
                "global_mmlu_full_en_computer_security": 0.0,
                "global_mmlu_full_en_conceptual_physics": 0.0,
                "global_mmlu_full_en_econometrics": 0.0,
                "global_mmlu_full_en_electrical_engineering": 0.0,
                "global_mmlu_full_en_elementary_mathematics": 0.0,
                "global_mmlu_full_en_formal_logic": 0.0,
                "global_mmlu_full_en_global_facts": 0.0,
                "global_mmlu_full_en_high_school_biology": 0.0,
                "global_mmlu_full_en_high_school_chemistry": 0.0,
                "global_mmlu_full_en_high_school_computer_science": 0.0,
                "global_mmlu_full_en_high_school_european_history": 0.0,
                "global_mmlu_full_en_high_school_geography": 0.0,
                "global_mmlu_full_en_high_school_government_and_politics": 0.0,
                "global_mmlu_full_en_high_school_macroeconomics": 0.0,
                "global_mmlu_full_en_high_school_mathematics": 0.0,
                "global_mmlu_full_en_high_school_microeconomics": 0.0,
                "global_mmlu_full_en_high_school_physics": 0.0,
                "global_mmlu_full_en_high_school_psychology": 0.0,
                "global_mmlu_full_en_high_school_statistics": 0.0,
                "global_mmlu_full_en_high_school_us_history": 0.0,
                "global_mmlu_full_en_high_school_world_history": 0.0,
                "global_mmlu_full_en_human_aging": 0.0,
                "global_mmlu_full_en_human_sexuality": 0.0,
                "global_mmlu_full_en_humanities": 0.0,
                "global_mmlu_full_en_international_law": 0.0,
                "global_mmlu_full_en_jurisprudence": 0.0,
                "global_mmlu_full_en_logical_fallacies": 0.0,
                "global_mmlu_full_en_machine_learning": 0.0,
                "global_mmlu_full_en_management": 0.0,
                "global_mmlu_full_en_marketing": 0.0,
                "global_mmlu_full_en_medical_genetics": 0.0,
                "global_mmlu_full_en_miscellaneous": 0.0,
                "global_mmlu_full_en_moral_disputes": 0.0,
                "global_mmlu_full_en_moral_scenarios": 0.0,
                "global_mmlu_full_en_nutrition": 0.0,
                "global_mmlu_full_en_other": 0.0,
                "global_mmlu_full_en_philosophy": 0.0,
                "global_mmlu_full_en_prehistory": 0.0,
                "global_mmlu_full_en_professional_accounting": 0.0,
                "global_mmlu_full_en_professional_law": 0.0,
                "global_mmlu_full_en_professional_medicine": 0.0,
                "global_mmlu_full_en_professional_psychology": 0.0,
                "global_mmlu_full_en_public_relations": 0.0,
                "global_mmlu_full_en_security_studies": 0.0,
                "global_mmlu_full_en_social_sciences": 0.0,
                "global_mmlu_full_en_sociology": 0.0,
                "global_mmlu_full_en_stem": 0.0,
                "global_mmlu_full_en_us_foreign_policy": 0.0,
                "global_mmlu_full_en_virology": 0.0,
                "global_mmlu_full_en_world_religions": 0.0
              },
              "n-shot": {
                "global_mmlu_full_en_abstract_algebra": 0,
                "global_mmlu_full_en_anatomy": 0,
                "global_mmlu_full_en_astronomy": 0,
                "global_mmlu_full_en_business_ethics": 0,
                "global_mmlu_full_en_clinical_knowledge": 0,
                "global_mmlu_full_en_college_biology": 0,
                "global_mmlu_full_en_college_chemistry": 0,
                "global_mmlu_full_en_college_computer_science": 0,
                "global_mmlu_full_en_college_mathematics": 0,
                "global_mmlu_full_en_college_medicine": 0,
                "global_mmlu_full_en_college_physics": 0,
                "global_mmlu_full_en_computer_security": 0,
                "global_mmlu_full_en_conceptual_physics": 0,
                "global_mmlu_full_en_econometrics": 0,
                "global_mmlu_full_en_electrical_engineering": 0,
                "global_mmlu_full_en_elementary_mathematics": 0,
                "global_mmlu_full_en_formal_logic": 0,
                "global_mmlu_full_en_global_facts": 0,
                "global_mmlu_full_en_high_school_biology": 0,
                "global_mmlu_full_en_high_school_chemistry": 0,
                "global_mmlu_full_en_high_school_computer_science": 0,
                "global_mmlu_full_en_high_school_european_history": 0,
                "global_mmlu_full_en_high_school_geography": 0,
                "global_mmlu_full_en_high_school_government_and_politics": 0,
                "global_mmlu_full_en_high_school_macroeconomics": 0,
                "global_mmlu_full_en_high_school_mathematics": 0,
                "global_mmlu_full_en_high_school_microeconomics": 0,
                "global_mmlu_full_en_high_school_physics": 0,
                "global_mmlu_full_en_high_school_psychology": 0,
                "global_mmlu_full_en_high_school_statistics": 0,
                "global_mmlu_full_en_high_school_us_history": 0,
                "global_mmlu_full_en_high_school_world_history": 0,
                "global_mmlu_full_en_human_aging": 0,
                "global_mmlu_full_en_human_sexuality": 0,
                "global_mmlu_full_en_international_law": 0,
                "global_mmlu_full_en_jurisprudence": 0,
                "global_mmlu_full_en_logical_fallacies": 0,
                "global_mmlu_full_en_machine_learning": 0,
                "global_mmlu_full_en_management": 0,
                "global_mmlu_full_en_marketing": 0,
                "global_mmlu_full_en_medical_genetics": 0,
                "global_mmlu_full_en_miscellaneous": 0,
                "global_mmlu_full_en_moral_disputes": 0,
                "global_mmlu_full_en_moral_scenarios": 0,
                "global_mmlu_full_en_nutrition": 0,
                "global_mmlu_full_en_philosophy": 0,
                "global_mmlu_full_en_prehistory": 0,
                "global_mmlu_full_en_professional_accounting": 0,
                "global_mmlu_full_en_professional_law": 0,
                "global_mmlu_full_en_professional_medicine": 0,
                "global_mmlu_full_en_professional_psychology": 0,
                "global_mmlu_full_en_public_relations": 0,
                "global_mmlu_full_en_security_studies": 0,
                "global_mmlu_full_en_sociology": 0,
                "global_mmlu_full_en_us_foreign_policy": 0,
                "global_mmlu_full_en_virology": 0,
                "global_mmlu_full_en_world_religions": 0
              },
              "higher_is_better": {
                "global_mmlu_full_en": {
                  "acc": true
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "acc": true
                },
                "global_mmlu_full_en_anatomy": {
                  "acc": true
                },
                "global_mmlu_full_en_astronomy": {
                  "acc": true
                },
                "global_mmlu_full_en_business_ethics": {
                  "acc": true
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "acc": true
                },
                "global_mmlu_full_en_college_biology": {
                  "acc": true
                },
                "global_mmlu_full_en_college_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_en_college_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_en_college_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_college_medicine": {
                  "acc": true
                },
                "global_mmlu_full_en_college_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_computer_security": {
                  "acc": true
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_econometrics": {
                  "acc": true
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "acc": true
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_formal_logic": {
                  "acc": true
                },
                "global_mmlu_full_en_global_facts": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_biology": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_geography": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_physics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_statistics": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "acc": true
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "acc": true
                },
                "global_mmlu_full_en_human_aging": {
                  "acc": true
                },
                "global_mmlu_full_en_human_sexuality": {
                  "acc": true
                },
                "global_mmlu_full_en_humanities": {
                  "acc": true
                },
                "global_mmlu_full_en_international_law": {
                  "acc": true
                },
                "global_mmlu_full_en_jurisprudence": {
                  "acc": true
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "acc": true
                },
                "global_mmlu_full_en_machine_learning": {
                  "acc": true
                },
                "global_mmlu_full_en_management": {
                  "acc": true
                },
                "global_mmlu_full_en_marketing": {
                  "acc": true
                },
                "global_mmlu_full_en_medical_genetics": {
                  "acc": true
                },
                "global_mmlu_full_en_miscellaneous": {
                  "acc": true
                },
                "global_mmlu_full_en_moral_disputes": {
                  "acc": true
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "acc": true
                },
                "global_mmlu_full_en_nutrition": {
                  "acc": true
                },
                "global_mmlu_full_en_other": {
                  "acc": true
                },
                "global_mmlu_full_en_philosophy": {
                  "acc": true
                },
                "global_mmlu_full_en_prehistory": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_accounting": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_law": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_medicine": {
                  "acc": true
                },
                "global_mmlu_full_en_professional_psychology": {
                  "acc": true
                },
                "global_mmlu_full_en_public_relations": {
                  "acc": true
                },
                "global_mmlu_full_en_security_studies": {
                  "acc": true
                },
                "global_mmlu_full_en_social_sciences": {
                  "acc": true
                },
                "global_mmlu_full_en_sociology": {
                  "acc": true
                },
                "global_mmlu_full_en_stem": {
                  "acc": true
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "acc": true
                },
                "global_mmlu_full_en_virology": {
                  "acc": true
                },
                "global_mmlu_full_en_world_religions": {
                  "acc": true
                }
              },
              "n-samples": {
                "global_mmlu_full_en_high_school_statistics": {
                  "original": 216,
                  "effective": 216
                },
                "global_mmlu_full_en_computer_security": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_machine_learning": {
                  "original": 112,
                  "effective": 112
                },
                "global_mmlu_full_en_abstract_algebra": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_astronomy": {
                  "original": 152,
                  "effective": 152
                },
                "global_mmlu_full_en_college_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_high_school_biology": {
                  "original": 310,
                  "effective": 310
                },
                "global_mmlu_full_en_high_school_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_high_school_chemistry": {
                  "original": 203,
                  "effective": 203
                },
                "global_mmlu_full_en_elementary_mathematics": {
                  "original": 378,
                  "effective": 378
                },
                "global_mmlu_full_en_college_physics": {
                  "original": 102,
                  "effective": 102
                },
                "global_mmlu_full_en_high_school_physics": {
                  "original": 151,
                  "effective": 151
                },
                "global_mmlu_full_en_college_chemistry": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_college_biology": {
                  "original": 144,
                  "effective": 144
                },
                "global_mmlu_full_en_college_mathematics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_conceptual_physics": {
                  "original": 235,
                  "effective": 235
                },
                "global_mmlu_full_en_anatomy": {
                  "original": 135,
                  "effective": 135
                },
                "global_mmlu_full_en_high_school_mathematics": {
                  "original": 270,
                  "effective": 270
                },
                "global_mmlu_full_en_electrical_engineering": {
                  "original": 145,
                  "effective": 145
                },
                "global_mmlu_full_en_clinical_knowledge": {
                  "original": 265,
                  "effective": 265
                },
                "global_mmlu_full_en_medical_genetics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_professional_medicine": {
                  "original": 272,
                  "effective": 272
                },
                "global_mmlu_full_en_business_ethics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_marketing": {
                  "original": 234,
                  "effective": 234
                },
                "global_mmlu_full_en_global_facts": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_college_medicine": {
                  "original": 173,
                  "effective": 173
                },
                "global_mmlu_full_en_human_aging": {
                  "original": 223,
                  "effective": 223
                },
                "global_mmlu_full_en_management": {
                  "original": 103,
                  "effective": 103
                },
                "global_mmlu_full_en_nutrition": {
                  "original": 306,
                  "effective": 306
                },
                "global_mmlu_full_en_virology": {
                  "original": 166,
                  "effective": 166
                },
                "global_mmlu_full_en_professional_accounting": {
                  "original": 282,
                  "effective": 282
                },
                "global_mmlu_full_en_miscellaneous": {
                  "original": 783,
                  "effective": 783
                },
                "global_mmlu_full_en_us_foreign_policy": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_en_public_relations": {
                  "original": 110,
                  "effective": 110
                },
                "global_mmlu_full_en_econometrics": {
                  "original": 114,
                  "effective": 114
                },
                "global_mmlu_full_en_high_school_microeconomics": {
                  "original": 238,
                  "effective": 238
                },
                "global_mmlu_full_en_human_sexuality": {
                  "original": 131,
                  "effective": 131
                },
                "global_mmlu_full_en_security_studies": {
                  "original": 245,
                  "effective": 245
                },
                "global_mmlu_full_en_professional_psychology": {
                  "original": 612,
                  "effective": 612
                },
                "global_mmlu_full_en_high_school_geography": {
                  "original": 198,
                  "effective": 198
                },
                "global_mmlu_full_en_high_school_government_and_politics": {
                  "original": 193,
                  "effective": 193
                },
                "global_mmlu_full_en_high_school_psychology": {
                  "original": 545,
                  "effective": 545
                },
                "global_mmlu_full_en_high_school_macroeconomics": {
                  "original": 390,
                  "effective": 390
                },
                "global_mmlu_full_en_sociology": {
                  "original": 201,
                  "effective": 201
                },
                "global_mmlu_full_en_world_religions": {
                  "original": 171,
                  "effective": 171
                },
                "global_mmlu_full_en_international_law": {
                  "original": 121,
                  "effective": 121
                },
                "global_mmlu_full_en_moral_scenarios": {
                  "original": 895,
                  "effective": 895
                },
                "global_mmlu_full_en_high_school_us_history": {
                  "original": 204,
                  "effective": 204
                },
                "global_mmlu_full_en_jurisprudence": {
                  "original": 108,
                  "effective": 108
                },
                "global_mmlu_full_en_high_school_world_history": {
                  "original": 237,
                  "effective": 237
                },
                "global_mmlu_full_en_logical_fallacies": {
                  "original": 163,
                  "effective": 163
                },
                "global_mmlu_full_en_philosophy": {
                  "original": 311,
                  "effective": 311
                },
                "global_mmlu_full_en_professional_law": {
                  "original": 1534,
                  "effective": 1534
                },
                "global_mmlu_full_en_high_school_european_history": {
                  "original": 165,
                  "effective": 165
                },
                "global_mmlu_full_en_formal_logic": {
                  "original": 126,
                  "effective": 126
                },
                "global_mmlu_full_en_moral_disputes": {
                  "original": 346,
                  "effective": 346
                },
                "global_mmlu_full_en_prehistory": {
                  "original": 324,
                  "effective": 324
                }
              },
              "config": {
                "model": "hf",
                "model_args": "pretrained=t5-base,trust_remote_code=True",
                "model_num_parameters": 222903552,
                "model_dtype": "torch.float32",
                "model_revision": "main",
                "model_sha": "a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1",
                "batch_size": "auto",
                "batch_sizes": [
                  64
                ],
                "device": "cuda",
                "use_cache": null,
                "limit": null,
                "bootstrap_iters": 100000,
                "gen_kwargs": null,
                "random_seed": 42,
                "numpy_seed": 42,
                "torch_seed": 42,
                "fewshot_seed": 42
              },
              "git_hash": "a4103c7",
              "date": 1758648364.998552,
              "pretty_env_info": "PyTorch version: 2.7.1+cu126\nIs debug build: False\nCUDA used to build PyTorch: 12.6\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.2 LTS (x86_64)\nGCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\nClang version: Could not collect\nCMake version: version 3.24.1\nLibc version: glibc-2.35\n\nPython version: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] (64-bit runtime)\nPython platform: Linux-6.8.0-1036-nvidia-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H200\nNvidia driver version: 580.82.07\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               224\nOn-line CPU(s) list:                  0-223\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480C\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   56\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          3800.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            5.3 MiB (112 instances)\nL1i cache:                            3.5 MiB (112 instances)\nL2 cache:                             224 MiB (112 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-55,112-167\nNUMA node1 CPU(s):                    56-111,168-223\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pytorch-lightning==2.5.2\n[pip3] torch==2.7.1\n[pip3] torchmetrics==1.8.0\n[pip3] triton==3.3.1\n[conda] Could not collect",
              "transformers_version": "4.54.1",
              "lm_eval_version": "0.4.9",
              "upper_git_hash": null,
              "tokenizer_pad_token": [
                "<pad>",
                "0"
              ],
              "tokenizer_eos_token": [
                "</s>",
                "1"
              ],
              "tokenizer_bos_token": [
                null,
                "None"
              ],
              "eot_token_id": 1,
              "max_length": 512,
              "task_hashes": {},
              "model_source": "hf",
              "model_name": "t5-base",
              "model_name_sanitized": "t5-base",
              "system_instruction": null,
              "system_instruction_sha": null,
              "fewshot_as_multiturn": false,
              "chat_template": null,
              "chat_template_sha": null,
              "start_time": 634509.33918747,
              "end_time": 634838.684993197,
              "total_evaluation_time_seconds": "329.3458057269454"
            },
            "duration_seconds": 374.55918860435486,
            "duration_minutes": 6.2426531434059145,
            "command": "python -m lm_eval --model hf --model_args pretrained=t5-base --tasks global_mmlu_full_en --device cuda --batch_size auto --output_path /netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/t5-base_global_mmlu_en_0shot_20250923_192433 --num_fewshot 0 --trust_remote_code --seed 42 --wandb_args project=BA-T5-CrossLingual,entity=nikolas-rauscher-dfki,group=crosslingual_transfer_eval_full_15k-final,name=t5-base_global_mmlu_en_0shot",
            "output_dir": "/netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/t5-base_global_mmlu_en_0shot_20250923_192433",
            "status": "success"
          }
        },
        "global_mmlu_de": {
          "0_shot": {
            "parse_status": "partial",
            "error": "Could not find primary metric",
            "raw_results": {
              "results": {
                "global_mmlu_full_de": {
                  "acc,none": 0.22895598917533114,
                  "acc_stderr,none": 0.003540987553872822,
                  "alias": "global_mmlu_full_de"
                },
                "global_mmlu_full_de_humanities": {
                  "acc,none": 0.23995749202975558,
                  "acc_stderr,none": 0.006225294928154789,
                  "alias": " - global_mmlu_full_de_humanities"
                },
                "global_mmlu_full_de_formal_logic": {
                  "alias": "  - global_mmlu_full_de_formal_logic",
                  "acc,none": 0.23809523809523808,
                  "acc_stderr,none": 0.03809523809523809
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "alias": "  - global_mmlu_full_de_high_school_european_history",
                  "acc,none": 0.21818181818181817,
                  "acc_stderr,none": 0.03225078108306289
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "alias": "  - global_mmlu_full_de_high_school_us_history",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.03039153369274154
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "alias": "  - global_mmlu_full_de_high_school_world_history",
                  "acc,none": 0.270042194092827,
                  "acc_stderr,none": 0.028900721906293426
                },
                "global_mmlu_full_de_international_law": {
                  "alias": "  - global_mmlu_full_de_international_law",
                  "acc,none": 0.2396694214876033,
                  "acc_stderr,none": 0.03896878985070417
                },
                "global_mmlu_full_de_jurisprudence": {
                  "alias": "  - global_mmlu_full_de_jurisprudence",
                  "acc,none": 0.25,
                  "acc_stderr,none": 0.04186091791394607
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "alias": "  - global_mmlu_full_de_logical_fallacies",
                  "acc,none": 0.22085889570552147,
                  "acc_stderr,none": 0.032591773927421776
                },
                "global_mmlu_full_de_moral_disputes": {
                  "alias": "  - global_mmlu_full_de_moral_disputes",
                  "acc,none": 0.23699421965317918,
                  "acc_stderr,none": 0.02289408248992599
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "alias": "  - global_mmlu_full_de_moral_scenarios",
                  "acc,none": 0.23798882681564246,
                  "acc_stderr,none": 0.014242630070574885
                },
                "global_mmlu_full_de_philosophy": {
                  "alias": "  - global_mmlu_full_de_philosophy",
                  "acc,none": 0.18971061093247588,
                  "acc_stderr,none": 0.022268196258783225
                },
                "global_mmlu_full_de_prehistory": {
                  "alias": "  - global_mmlu_full_de_prehistory",
                  "acc,none": 0.21604938271604937,
                  "acc_stderr,none": 0.022899162918445813
                },
                "global_mmlu_full_de_professional_law": {
                  "alias": "  - global_mmlu_full_de_professional_law",
                  "acc,none": 0.2457627118644068,
                  "acc_stderr,none": 0.01099615663514269
                },
                "global_mmlu_full_de_world_religions": {
                  "alias": "  - global_mmlu_full_de_world_religions",
                  "acc,none": 0.3216374269005848,
                  "acc_stderr,none": 0.03582529442573122
                },
                "global_mmlu_full_de_other": {
                  "acc,none": 0.23913743160605086,
                  "acc_stderr,none": 0.00763515350881118,
                  "alias": " - global_mmlu_full_de_other"
                },
                "global_mmlu_full_de_business_ethics": {
                  "alias": "  - global_mmlu_full_de_business_ethics",
                  "acc,none": 0.3,
                  "acc_stderr,none": 0.046056618647183814
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "alias": "  - global_mmlu_full_de_clinical_knowledge",
                  "acc,none": 0.21509433962264152,
                  "acc_stderr,none": 0.025288394502891377
                },
                "global_mmlu_full_de_college_medicine": {
                  "alias": "  - global_mmlu_full_de_college_medicine",
                  "acc,none": 0.2023121387283237,
                  "acc_stderr,none": 0.030631145539198816
                },
                "global_mmlu_full_de_global_facts": {
                  "alias": "  - global_mmlu_full_de_global_facts",
                  "acc,none": 0.2,
                  "acc_stderr,none": 0.04020151261036845
                },
                "global_mmlu_full_de_human_aging": {
                  "alias": "  - global_mmlu_full_de_human_aging",
                  "acc,none": 0.3183856502242152,
                  "acc_stderr,none": 0.03126580522513713
                },
                "global_mmlu_full_de_management": {
                  "alias": "  - global_mmlu_full_de_management",
                  "acc,none": 0.17475728155339806,
                  "acc_stderr,none": 0.03760178006026621
                },
                "global_mmlu_full_de_marketing": {
                  "alias": "  - global_mmlu_full_de_marketing",
                  "acc,none": 0.2863247863247863,
                  "acc_stderr,none": 0.029614323690456655
                },
                "global_mmlu_full_de_medical_genetics": {
                  "alias": "  - global_mmlu_full_de_medical_genetics",
                  "acc,none": 0.3,
                  "acc_stderr,none": 0.046056618647183814
                },
                "global_mmlu_full_de_miscellaneous": {
                  "alias": "  - global_mmlu_full_de_miscellaneous",
                  "acc,none": 0.23627075351213284,
                  "acc_stderr,none": 0.015190473717037509
                },
                "global_mmlu_full_de_nutrition": {
                  "alias": "  - global_mmlu_full_de_nutrition",
                  "acc,none": 0.21895424836601307,
                  "acc_stderr,none": 0.02367908986180772
                },
                "global_mmlu_full_de_professional_accounting": {
                  "alias": "  - global_mmlu_full_de_professional_accounting",
                  "acc,none": 0.23404255319148937,
                  "acc_stderr,none": 0.025257861359432407
                },
                "global_mmlu_full_de_professional_medicine": {
                  "alias": "  - global_mmlu_full_de_professional_medicine",
                  "acc,none": 0.18382352941176472,
                  "acc_stderr,none": 0.02352924218519311
                },
                "global_mmlu_full_de_virology": {
                  "alias": "  - global_mmlu_full_de_virology",
                  "acc,none": 0.28313253012048195,
                  "acc_stderr,none": 0.03507295431370518
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc,none": 0.2183945401364966,
                  "acc_stderr,none": 0.007446326011321208,
                  "alias": " - global_mmlu_full_de_social_sciences"
                },
                "global_mmlu_full_de_econometrics": {
                  "alias": "  - global_mmlu_full_de_econometrics",
                  "acc,none": 0.23684210526315788,
                  "acc_stderr,none": 0.039994238792813386
                },
                "global_mmlu_full_de_high_school_geography": {
                  "alias": "  - global_mmlu_full_de_high_school_geography",
                  "acc,none": 0.17676767676767677,
                  "acc_stderr,none": 0.027178752639044915
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "alias": "  - global_mmlu_full_de_high_school_government_and_politics",
                  "acc,none": 0.19689119170984457,
                  "acc_stderr,none": 0.02869787397186069
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "alias": "  - global_mmlu_full_de_high_school_macroeconomics",
                  "acc,none": 0.2076923076923077,
                  "acc_stderr,none": 0.020567539567246797
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "alias": "  - global_mmlu_full_de_high_school_microeconomics",
                  "acc,none": 0.21008403361344538,
                  "acc_stderr,none": 0.026461398717471874
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "alias": "  - global_mmlu_full_de_high_school_psychology",
                  "acc,none": 0.1981651376146789,
                  "acc_stderr,none": 0.017090573804217905
                },
                "global_mmlu_full_de_human_sexuality": {
                  "alias": "  - global_mmlu_full_de_human_sexuality",
                  "acc,none": 0.2595419847328244,
                  "acc_stderr,none": 0.03844876139785271
                },
                "global_mmlu_full_de_professional_psychology": {
                  "alias": "  - global_mmlu_full_de_professional_psychology",
                  "acc,none": 0.25163398692810457,
                  "acc_stderr,none": 0.01755581809132225
                },
                "global_mmlu_full_de_public_relations": {
                  "alias": "  - global_mmlu_full_de_public_relations",
                  "acc,none": 0.21818181818181817,
                  "acc_stderr,none": 0.03955932861795833
                },
                "global_mmlu_full_de_security_studies": {
                  "alias": "  - global_mmlu_full_de_security_studies",
                  "acc,none": 0.18775510204081633,
                  "acc_stderr,none": 0.02500025603954622
                },
                "global_mmlu_full_de_sociology": {
                  "alias": "  - global_mmlu_full_de_sociology",
                  "acc,none": 0.23383084577114427,
                  "acc_stderr,none": 0.029929415408348398
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "alias": "  - global_mmlu_full_de_us_foreign_policy",
                  "acc,none": 0.28,
                  "acc_stderr,none": 0.045126085985421276
                },
                "global_mmlu_full_de_stem": {
                  "acc,none": 0.21281319378369806,
                  "acc_stderr,none": 0.007278128452233221,
                  "alias": " - global_mmlu_full_de_stem"
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "alias": "  - global_mmlu_full_de_abstract_algebra",
                  "acc,none": 0.22,
                  "acc_stderr,none": 0.04163331998932269
                },
                "global_mmlu_full_de_anatomy": {
                  "alias": "  - global_mmlu_full_de_anatomy",
                  "acc,none": 0.18518518518518517,
                  "acc_stderr,none": 0.03355677216313142
                },
                "global_mmlu_full_de_astronomy": {
                  "alias": "  - global_mmlu_full_de_astronomy",
                  "acc,none": 0.18421052631578946,
                  "acc_stderr,none": 0.0315469804508223
                },
                "global_mmlu_full_de_college_biology": {
                  "alias": "  - global_mmlu_full_de_college_biology",
                  "acc,none": 0.2638888888888889,
                  "acc_stderr,none": 0.03685651095897532
                },
                "global_mmlu_full_de_college_chemistry": {
                  "alias": "  - global_mmlu_full_de_college_chemistry",
                  "acc,none": 0.19,
                  "acc_stderr,none": 0.03942772444036623
                },
                "global_mmlu_full_de_college_computer_science": {
                  "alias": "  - global_mmlu_full_de_college_computer_science",
                  "acc,none": 0.24,
                  "acc_stderr,none": 0.04292346959909282
                },
                "global_mmlu_full_de_college_mathematics": {
                  "alias": "  - global_mmlu_full_de_college_mathematics",
                  "acc,none": 0.21,
                  "acc_stderr,none": 0.040936018074033256
                },
                "global_mmlu_full_de_college_physics": {
                  "alias": "  - global_mmlu_full_de_college_physics",
                  "acc,none": 0.21568627450980393,
                  "acc_stderr,none": 0.040925639582376556
                },
                "global_mmlu_full_de_computer_security": {
                  "alias": "  - global_mmlu_full_de_computer_security",
                  "acc,none": 0.29,
                  "acc_stderr,none": 0.045604802157206845
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "alias": "  - global_mmlu_full_de_conceptual_physics",
                  "acc,none": 0.251063829787234,
                  "acc_stderr,none": 0.028346963777162452
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "alias": "  - global_mmlu_full_de_electrical_engineering",
                  "acc,none": 0.2413793103448276,
                  "acc_stderr,none": 0.03565998174135302
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "alias": "  - global_mmlu_full_de_elementary_mathematics",
                  "acc,none": 0.20899470899470898,
                  "acc_stderr,none": 0.020940481565334835
                },
                "global_mmlu_full_de_high_school_biology": {
                  "alias": "  - global_mmlu_full_de_high_school_biology",
                  "acc,none": 0.18064516129032257,
                  "acc_stderr,none": 0.021886178567172555
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "alias": "  - global_mmlu_full_de_high_school_chemistry",
                  "acc,none": 0.15763546798029557,
                  "acc_stderr,none": 0.0256390141311724
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "alias": "  - global_mmlu_full_de_high_school_computer_science",
                  "acc,none": 0.26,
                  "acc_stderr,none": 0.04408440022768079
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "alias": "  - global_mmlu_full_de_high_school_mathematics",
                  "acc,none": 0.2111111111111111,
                  "acc_stderr,none": 0.02488211685765508
                },
                "global_mmlu_full_de_high_school_physics": {
                  "alias": "  - global_mmlu_full_de_high_school_physics",
                  "acc,none": 0.1986754966887417,
                  "acc_stderr,none": 0.032578473844367746
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "alias": "  - global_mmlu_full_de_high_school_statistics",
                  "acc,none": 0.1574074074074074,
                  "acc_stderr,none": 0.024837173518242384
                },
                "global_mmlu_full_de_machine_learning": {
                  "alias": "  - global_mmlu_full_de_machine_learning",
                  "acc,none": 0.3125,
                  "acc_stderr,none": 0.043994650575715215
                }
              },
              "groups": {
                "global_mmlu_full_de": {
                  "acc,none": 0.22895598917533114,
                  "acc_stderr,none": 0.003540987553872822,
                  "alias": "global_mmlu_full_de"
                },
                "global_mmlu_full_de_humanities": {
                  "acc,none": 0.23995749202975558,
                  "acc_stderr,none": 0.006225294928154789,
                  "alias": " - global_mmlu_full_de_humanities"
                },
                "global_mmlu_full_de_other": {
                  "acc,none": 0.23913743160605086,
                  "acc_stderr,none": 0.00763515350881118,
                  "alias": " - global_mmlu_full_de_other"
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc,none": 0.2183945401364966,
                  "acc_stderr,none": 0.007446326011321208,
                  "alias": " - global_mmlu_full_de_social_sciences"
                },
                "global_mmlu_full_de_stem": {
                  "acc,none": 0.21281319378369806,
                  "acc_stderr,none": 0.007278128452233221,
                  "alias": " - global_mmlu_full_de_stem"
                }
              },
              "group_subtasks": {
                "global_mmlu_full_de_humanities": [
                  "global_mmlu_full_de_professional_law",
                  "global_mmlu_full_de_international_law",
                  "global_mmlu_full_de_prehistory",
                  "global_mmlu_full_de_world_religions",
                  "global_mmlu_full_de_high_school_world_history",
                  "global_mmlu_full_de_high_school_us_history",
                  "global_mmlu_full_de_jurisprudence",
                  "global_mmlu_full_de_formal_logic",
                  "global_mmlu_full_de_logical_fallacies",
                  "global_mmlu_full_de_high_school_european_history",
                  "global_mmlu_full_de_philosophy",
                  "global_mmlu_full_de_moral_scenarios",
                  "global_mmlu_full_de_moral_disputes"
                ],
                "global_mmlu_full_de_social_sciences": [
                  "global_mmlu_full_de_high_school_psychology",
                  "global_mmlu_full_de_professional_psychology",
                  "global_mmlu_full_de_high_school_government_and_politics",
                  "global_mmlu_full_de_high_school_macroeconomics",
                  "global_mmlu_full_de_high_school_microeconomics",
                  "global_mmlu_full_de_econometrics",
                  "global_mmlu_full_de_high_school_geography",
                  "global_mmlu_full_de_security_studies",
                  "global_mmlu_full_de_us_foreign_policy",
                  "global_mmlu_full_de_public_relations",
                  "global_mmlu_full_de_human_sexuality",
                  "global_mmlu_full_de_sociology"
                ],
                "global_mmlu_full_de_other": [
                  "global_mmlu_full_de_nutrition",
                  "global_mmlu_full_de_professional_medicine",
                  "global_mmlu_full_de_college_medicine",
                  "global_mmlu_full_de_professional_accounting",
                  "global_mmlu_full_de_marketing",
                  "global_mmlu_full_de_miscellaneous",
                  "global_mmlu_full_de_business_ethics",
                  "global_mmlu_full_de_virology",
                  "global_mmlu_full_de_medical_genetics",
                  "global_mmlu_full_de_human_aging",
                  "global_mmlu_full_de_global_facts",
                  "global_mmlu_full_de_clinical_knowledge",
                  "global_mmlu_full_de_management"
                ],
                "global_mmlu_full_de_stem": [
                  "global_mmlu_full_de_computer_security",
                  "global_mmlu_full_de_college_biology",
                  "global_mmlu_full_de_college_mathematics",
                  "global_mmlu_full_de_conceptual_physics",
                  "global_mmlu_full_de_elementary_mathematics",
                  "global_mmlu_full_de_astronomy",
                  "global_mmlu_full_de_high_school_physics",
                  "global_mmlu_full_de_machine_learning",
                  "global_mmlu_full_de_college_chemistry",
                  "global_mmlu_full_de_high_school_chemistry",
                  "global_mmlu_full_de_college_computer_science",
                  "global_mmlu_full_de_high_school_computer_science",
                  "global_mmlu_full_de_high_school_mathematics",
                  "global_mmlu_full_de_abstract_algebra",
                  "global_mmlu_full_de_high_school_biology",
                  "global_mmlu_full_de_electrical_engineering",
                  "global_mmlu_full_de_college_physics",
                  "global_mmlu_full_de_anatomy",
                  "global_mmlu_full_de_high_school_statistics"
                ],
                "global_mmlu_full_de": [
                  "global_mmlu_full_de_stem",
                  "global_mmlu_full_de_other",
                  "global_mmlu_full_de_social_sciences",
                  "global_mmlu_full_de_humanities"
                ]
              },
              "configs": {
                "global_mmlu_full_de_abstract_algebra": {
                  "task": "global_mmlu_full_de_abstract_algebra",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c957e0>, subject='abstract_algebra')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_anatomy": {
                  "task": "global_mmlu_full_de_anatomy",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c944c0>, subject='anatomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_astronomy": {
                  "task": "global_mmlu_full_de_astronomy",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c96e60>, subject='astronomy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_business_ethics": {
                  "task": "global_mmlu_full_de_business_ethics",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441dd80>, subject='business_ethics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "task": "global_mmlu_full_de_clinical_knowledge",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441f400>, subject='clinical_knowledge')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_college_biology": {
                  "task": "global_mmlu_full_de_college_biology",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79ad0f9b4310>, subject='college_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_college_chemistry": {
                  "task": "global_mmlu_full_de_college_chemistry",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c95f30>, subject='college_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_college_computer_science": {
                  "task": "global_mmlu_full_de_college_computer_science",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c94ca0>, subject='college_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_college_mathematics": {
                  "task": "global_mmlu_full_de_college_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c96950>, subject='college_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_college_medicine": {
                  "task": "global_mmlu_full_de_college_medicine",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441fd90>, subject='college_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_college_physics": {
                  "task": "global_mmlu_full_de_college_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c95900>, subject='college_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_computer_security": {
                  "task": "global_mmlu_full_de_computer_security",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79ad0f9b4820>, subject='computer_security')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "task": "global_mmlu_full_de_conceptual_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c97be0>, subject='conceptual_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_econometrics": {
                  "task": "global_mmlu_full_de_econometrics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441dc60>, subject='econometrics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "task": "global_mmlu_full_de_electrical_engineering",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c95d80>, subject='electrical_engineering')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "task": "global_mmlu_full_de_elementary_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c94f70>, subject='elementary_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_formal_logic": {
                  "task": "global_mmlu_full_de_formal_logic",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc4b201f0>, subject='formal_logic')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_global_facts": {
                  "task": "global_mmlu_full_de_global_facts",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441f130>, subject='global_facts')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_high_school_biology": {
                  "task": "global_mmlu_full_de_high_school_biology",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c94670>, subject='high_school_biology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "task": "global_mmlu_full_de_high_school_chemistry",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c968c0>, subject='high_school_chemistry')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "task": "global_mmlu_full_de_high_school_computer_science",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c976d0>, subject='high_school_computer_science')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "task": "global_mmlu_full_de_high_school_european_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc4b20790>, subject='high_school_european_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_high_school_geography": {
                  "task": "global_mmlu_full_de_high_school_geography",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441c5e0>, subject='high_school_geography')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "task": "global_mmlu_full_de_high_school_government_and_politics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441e0e0>, subject='high_school_government_and_politics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "task": "global_mmlu_full_de_high_school_macroeconomics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc4b23760>, subject='high_school_macroeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "task": "global_mmlu_full_de_high_school_mathematics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c955a0>, subject='high_school_mathematics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "task": "global_mmlu_full_de_high_school_microeconomics",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441d5a0>, subject='high_school_microeconomics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_high_school_physics": {
                  "task": "global_mmlu_full_de_high_school_physics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c95fc0>, subject='high_school_physics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "task": "global_mmlu_full_de_high_school_psychology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441e680>, subject='high_school_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "task": "global_mmlu_full_de_high_school_statistics",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441ec20>, subject='high_school_statistics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "task": "global_mmlu_full_de_high_school_us_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc4b20f70>, subject='high_school_us_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "task": "global_mmlu_full_de_high_school_world_history",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc4b21ab0>, subject='high_school_world_history')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_human_aging": {
                  "task": "global_mmlu_full_de_human_aging",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441cca0>, subject='human_aging')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_human_sexuality": {
                  "task": "global_mmlu_full_de_human_sexuality",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc4b22050>, subject='human_sexuality')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_international_law": {
                  "task": "global_mmlu_full_de_international_law",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc4b22c20>, subject='international_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_jurisprudence": {
                  "task": "global_mmlu_full_de_jurisprudence",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc4b205e0>, subject='jurisprudence')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "task": "global_mmlu_full_de_logical_fallacies",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc4b20d30>, subject='logical_fallacies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_machine_learning": {
                  "task": "global_mmlu_full_de_machine_learning",
                  "tag": "global_mmlu_full_de_stem_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c977f0>, subject='machine_learning')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_management": {
                  "task": "global_mmlu_full_de_management",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441f490>, subject='management')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_marketing": {
                  "task": "global_mmlu_full_de_marketing",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441dab0>, subject='marketing')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_medical_genetics": {
                  "task": "global_mmlu_full_de_medical_genetics",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441d120>, subject='medical_genetics')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_miscellaneous": {
                  "task": "global_mmlu_full_de_miscellaneous",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441ee60>, subject='miscellaneous')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_moral_disputes": {
                  "task": "global_mmlu_full_de_moral_disputes",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acd44f92d0>, subject='moral_disputes')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "task": "global_mmlu_full_de_moral_scenarios",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc4b20e50>, subject='moral_scenarios')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_nutrition": {
                  "task": "global_mmlu_full_de_nutrition",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441ca60>, subject='nutrition')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_philosophy": {
                  "task": "global_mmlu_full_de_philosophy",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc493fbe0>, subject='philosophy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_prehistory": {
                  "task": "global_mmlu_full_de_prehistory",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc4b23be0>, subject='prehistory')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_professional_accounting": {
                  "task": "global_mmlu_full_de_professional_accounting",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c95000>, subject='professional_accounting')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_professional_law": {
                  "task": "global_mmlu_full_de_professional_law",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc4b23f40>, subject='professional_law')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_professional_medicine": {
                  "task": "global_mmlu_full_de_professional_medicine",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79aa32c94160>, subject='professional_medicine')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_professional_psychology": {
                  "task": "global_mmlu_full_de_professional_psychology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441c310>, subject='professional_psychology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_public_relations": {
                  "task": "global_mmlu_full_de_public_relations",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc4b20dc0>, subject='public_relations')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_security_studies": {
                  "task": "global_mmlu_full_de_security_studies",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc4b227a0>, subject='security_studies')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_sociology": {
                  "task": "global_mmlu_full_de_sociology",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc4b211b0>, subject='sociology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "task": "global_mmlu_full_de_us_foreign_policy",
                  "tag": "global_mmlu_full_de_social_sciences_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc4b239a0>, subject='us_foreign_policy')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_virology": {
                  "task": "global_mmlu_full_de_virology",
                  "tag": "global_mmlu_full_de_other_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc441c670>, subject='virology')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                },
                "global_mmlu_full_de_world_religions": {
                  "task": "global_mmlu_full_de_world_religions",
                  "tag": "global_mmlu_full_de_humanities_tasks",
                  "dataset_path": "CohereForAI/Global-MMLU",
                  "dataset_name": "de",
                  "test_split": "test",
                  "fewshot_split": "dev",
                  "process_docs": "functools.partial(<function process_docs at 0x79acc4b236d0>, subject='world_religions')",
                  "doc_to_text": "{{question.strip()}}\nA. {{option_a}}\nB. {{option_b}}\nC. {{option_c}}\nD. {{option_d}}\nAnswer:",
                  "doc_to_target": "answer",
                  "unsafe_code": false,
                  "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                  ],
                  "description": "",
                  "target_delimiter": " ",
                  "fewshot_delimiter": "\n\n",
                  "fewshot_config": {
                    "sampler": "first_n"
                  },
                  "num_fewshot": 0,
                  "metric_list": [
                    {
                      "metric": "acc",
                      "aggregation": "mean",
                      "higher_is_better": true
                    }
                  ],
                  "output_type": "multiple_choice",
                  "repeats": 1,
                  "should_decontaminate": false,
                  "metadata": {
                    "version": 0.0,
                    "pretrained": "t5-base"
                  }
                }
              },
              "versions": {
                "global_mmlu_full_de": 0.0,
                "global_mmlu_full_de_abstract_algebra": 0.0,
                "global_mmlu_full_de_anatomy": 0.0,
                "global_mmlu_full_de_astronomy": 0.0,
                "global_mmlu_full_de_business_ethics": 0.0,
                "global_mmlu_full_de_clinical_knowledge": 0.0,
                "global_mmlu_full_de_college_biology": 0.0,
                "global_mmlu_full_de_college_chemistry": 0.0,
                "global_mmlu_full_de_college_computer_science": 0.0,
                "global_mmlu_full_de_college_mathematics": 0.0,
                "global_mmlu_full_de_college_medicine": 0.0,
                "global_mmlu_full_de_college_physics": 0.0,
                "global_mmlu_full_de_computer_security": 0.0,
                "global_mmlu_full_de_conceptual_physics": 0.0,
                "global_mmlu_full_de_econometrics": 0.0,
                "global_mmlu_full_de_electrical_engineering": 0.0,
                "global_mmlu_full_de_elementary_mathematics": 0.0,
                "global_mmlu_full_de_formal_logic": 0.0,
                "global_mmlu_full_de_global_facts": 0.0,
                "global_mmlu_full_de_high_school_biology": 0.0,
                "global_mmlu_full_de_high_school_chemistry": 0.0,
                "global_mmlu_full_de_high_school_computer_science": 0.0,
                "global_mmlu_full_de_high_school_european_history": 0.0,
                "global_mmlu_full_de_high_school_geography": 0.0,
                "global_mmlu_full_de_high_school_government_and_politics": 0.0,
                "global_mmlu_full_de_high_school_macroeconomics": 0.0,
                "global_mmlu_full_de_high_school_mathematics": 0.0,
                "global_mmlu_full_de_high_school_microeconomics": 0.0,
                "global_mmlu_full_de_high_school_physics": 0.0,
                "global_mmlu_full_de_high_school_psychology": 0.0,
                "global_mmlu_full_de_high_school_statistics": 0.0,
                "global_mmlu_full_de_high_school_us_history": 0.0,
                "global_mmlu_full_de_high_school_world_history": 0.0,
                "global_mmlu_full_de_human_aging": 0.0,
                "global_mmlu_full_de_human_sexuality": 0.0,
                "global_mmlu_full_de_humanities": 0.0,
                "global_mmlu_full_de_international_law": 0.0,
                "global_mmlu_full_de_jurisprudence": 0.0,
                "global_mmlu_full_de_logical_fallacies": 0.0,
                "global_mmlu_full_de_machine_learning": 0.0,
                "global_mmlu_full_de_management": 0.0,
                "global_mmlu_full_de_marketing": 0.0,
                "global_mmlu_full_de_medical_genetics": 0.0,
                "global_mmlu_full_de_miscellaneous": 0.0,
                "global_mmlu_full_de_moral_disputes": 0.0,
                "global_mmlu_full_de_moral_scenarios": 0.0,
                "global_mmlu_full_de_nutrition": 0.0,
                "global_mmlu_full_de_other": 0.0,
                "global_mmlu_full_de_philosophy": 0.0,
                "global_mmlu_full_de_prehistory": 0.0,
                "global_mmlu_full_de_professional_accounting": 0.0,
                "global_mmlu_full_de_professional_law": 0.0,
                "global_mmlu_full_de_professional_medicine": 0.0,
                "global_mmlu_full_de_professional_psychology": 0.0,
                "global_mmlu_full_de_public_relations": 0.0,
                "global_mmlu_full_de_security_studies": 0.0,
                "global_mmlu_full_de_social_sciences": 0.0,
                "global_mmlu_full_de_sociology": 0.0,
                "global_mmlu_full_de_stem": 0.0,
                "global_mmlu_full_de_us_foreign_policy": 0.0,
                "global_mmlu_full_de_virology": 0.0,
                "global_mmlu_full_de_world_religions": 0.0
              },
              "n-shot": {
                "global_mmlu_full_de_abstract_algebra": 0,
                "global_mmlu_full_de_anatomy": 0,
                "global_mmlu_full_de_astronomy": 0,
                "global_mmlu_full_de_business_ethics": 0,
                "global_mmlu_full_de_clinical_knowledge": 0,
                "global_mmlu_full_de_college_biology": 0,
                "global_mmlu_full_de_college_chemistry": 0,
                "global_mmlu_full_de_college_computer_science": 0,
                "global_mmlu_full_de_college_mathematics": 0,
                "global_mmlu_full_de_college_medicine": 0,
                "global_mmlu_full_de_college_physics": 0,
                "global_mmlu_full_de_computer_security": 0,
                "global_mmlu_full_de_conceptual_physics": 0,
                "global_mmlu_full_de_econometrics": 0,
                "global_mmlu_full_de_electrical_engineering": 0,
                "global_mmlu_full_de_elementary_mathematics": 0,
                "global_mmlu_full_de_formal_logic": 0,
                "global_mmlu_full_de_global_facts": 0,
                "global_mmlu_full_de_high_school_biology": 0,
                "global_mmlu_full_de_high_school_chemistry": 0,
                "global_mmlu_full_de_high_school_computer_science": 0,
                "global_mmlu_full_de_high_school_european_history": 0,
                "global_mmlu_full_de_high_school_geography": 0,
                "global_mmlu_full_de_high_school_government_and_politics": 0,
                "global_mmlu_full_de_high_school_macroeconomics": 0,
                "global_mmlu_full_de_high_school_mathematics": 0,
                "global_mmlu_full_de_high_school_microeconomics": 0,
                "global_mmlu_full_de_high_school_physics": 0,
                "global_mmlu_full_de_high_school_psychology": 0,
                "global_mmlu_full_de_high_school_statistics": 0,
                "global_mmlu_full_de_high_school_us_history": 0,
                "global_mmlu_full_de_high_school_world_history": 0,
                "global_mmlu_full_de_human_aging": 0,
                "global_mmlu_full_de_human_sexuality": 0,
                "global_mmlu_full_de_international_law": 0,
                "global_mmlu_full_de_jurisprudence": 0,
                "global_mmlu_full_de_logical_fallacies": 0,
                "global_mmlu_full_de_machine_learning": 0,
                "global_mmlu_full_de_management": 0,
                "global_mmlu_full_de_marketing": 0,
                "global_mmlu_full_de_medical_genetics": 0,
                "global_mmlu_full_de_miscellaneous": 0,
                "global_mmlu_full_de_moral_disputes": 0,
                "global_mmlu_full_de_moral_scenarios": 0,
                "global_mmlu_full_de_nutrition": 0,
                "global_mmlu_full_de_philosophy": 0,
                "global_mmlu_full_de_prehistory": 0,
                "global_mmlu_full_de_professional_accounting": 0,
                "global_mmlu_full_de_professional_law": 0,
                "global_mmlu_full_de_professional_medicine": 0,
                "global_mmlu_full_de_professional_psychology": 0,
                "global_mmlu_full_de_public_relations": 0,
                "global_mmlu_full_de_security_studies": 0,
                "global_mmlu_full_de_sociology": 0,
                "global_mmlu_full_de_us_foreign_policy": 0,
                "global_mmlu_full_de_virology": 0,
                "global_mmlu_full_de_world_religions": 0
              },
              "higher_is_better": {
                "global_mmlu_full_de": {
                  "acc": true
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "acc": true
                },
                "global_mmlu_full_de_anatomy": {
                  "acc": true
                },
                "global_mmlu_full_de_astronomy": {
                  "acc": true
                },
                "global_mmlu_full_de_business_ethics": {
                  "acc": true
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "acc": true
                },
                "global_mmlu_full_de_college_biology": {
                  "acc": true
                },
                "global_mmlu_full_de_college_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_de_college_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_de_college_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_college_medicine": {
                  "acc": true
                },
                "global_mmlu_full_de_college_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_computer_security": {
                  "acc": true
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_econometrics": {
                  "acc": true
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "acc": true
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_formal_logic": {
                  "acc": true
                },
                "global_mmlu_full_de_global_facts": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_biology": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_geography": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_physics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "acc": true
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "acc": true
                },
                "global_mmlu_full_de_human_aging": {
                  "acc": true
                },
                "global_mmlu_full_de_human_sexuality": {
                  "acc": true
                },
                "global_mmlu_full_de_humanities": {
                  "acc": true
                },
                "global_mmlu_full_de_international_law": {
                  "acc": true
                },
                "global_mmlu_full_de_jurisprudence": {
                  "acc": true
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "acc": true
                },
                "global_mmlu_full_de_machine_learning": {
                  "acc": true
                },
                "global_mmlu_full_de_management": {
                  "acc": true
                },
                "global_mmlu_full_de_marketing": {
                  "acc": true
                },
                "global_mmlu_full_de_medical_genetics": {
                  "acc": true
                },
                "global_mmlu_full_de_miscellaneous": {
                  "acc": true
                },
                "global_mmlu_full_de_moral_disputes": {
                  "acc": true
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "acc": true
                },
                "global_mmlu_full_de_nutrition": {
                  "acc": true
                },
                "global_mmlu_full_de_other": {
                  "acc": true
                },
                "global_mmlu_full_de_philosophy": {
                  "acc": true
                },
                "global_mmlu_full_de_prehistory": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_accounting": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_law": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_medicine": {
                  "acc": true
                },
                "global_mmlu_full_de_professional_psychology": {
                  "acc": true
                },
                "global_mmlu_full_de_public_relations": {
                  "acc": true
                },
                "global_mmlu_full_de_security_studies": {
                  "acc": true
                },
                "global_mmlu_full_de_social_sciences": {
                  "acc": true
                },
                "global_mmlu_full_de_sociology": {
                  "acc": true
                },
                "global_mmlu_full_de_stem": {
                  "acc": true
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "acc": true
                },
                "global_mmlu_full_de_virology": {
                  "acc": true
                },
                "global_mmlu_full_de_world_religions": {
                  "acc": true
                }
              },
              "n-samples": {
                "global_mmlu_full_de_computer_security": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_college_biology": {
                  "original": 144,
                  "effective": 144
                },
                "global_mmlu_full_de_college_mathematics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_conceptual_physics": {
                  "original": 235,
                  "effective": 235
                },
                "global_mmlu_full_de_elementary_mathematics": {
                  "original": 378,
                  "effective": 378
                },
                "global_mmlu_full_de_astronomy": {
                  "original": 152,
                  "effective": 152
                },
                "global_mmlu_full_de_high_school_physics": {
                  "original": 151,
                  "effective": 151
                },
                "global_mmlu_full_de_machine_learning": {
                  "original": 112,
                  "effective": 112
                },
                "global_mmlu_full_de_college_chemistry": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_chemistry": {
                  "original": 203,
                  "effective": 203
                },
                "global_mmlu_full_de_college_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_computer_science": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_mathematics": {
                  "original": 270,
                  "effective": 270
                },
                "global_mmlu_full_de_abstract_algebra": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_high_school_biology": {
                  "original": 310,
                  "effective": 310
                },
                "global_mmlu_full_de_electrical_engineering": {
                  "original": 145,
                  "effective": 145
                },
                "global_mmlu_full_de_college_physics": {
                  "original": 102,
                  "effective": 102
                },
                "global_mmlu_full_de_anatomy": {
                  "original": 135,
                  "effective": 135
                },
                "global_mmlu_full_de_high_school_statistics": {
                  "original": 216,
                  "effective": 216
                },
                "global_mmlu_full_de_nutrition": {
                  "original": 306,
                  "effective": 306
                },
                "global_mmlu_full_de_professional_medicine": {
                  "original": 272,
                  "effective": 272
                },
                "global_mmlu_full_de_college_medicine": {
                  "original": 173,
                  "effective": 173
                },
                "global_mmlu_full_de_professional_accounting": {
                  "original": 282,
                  "effective": 282
                },
                "global_mmlu_full_de_marketing": {
                  "original": 234,
                  "effective": 234
                },
                "global_mmlu_full_de_miscellaneous": {
                  "original": 783,
                  "effective": 783
                },
                "global_mmlu_full_de_business_ethics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_virology": {
                  "original": 166,
                  "effective": 166
                },
                "global_mmlu_full_de_medical_genetics": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_human_aging": {
                  "original": 223,
                  "effective": 223
                },
                "global_mmlu_full_de_global_facts": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_clinical_knowledge": {
                  "original": 265,
                  "effective": 265
                },
                "global_mmlu_full_de_management": {
                  "original": 103,
                  "effective": 103
                },
                "global_mmlu_full_de_high_school_psychology": {
                  "original": 545,
                  "effective": 545
                },
                "global_mmlu_full_de_professional_psychology": {
                  "original": 612,
                  "effective": 612
                },
                "global_mmlu_full_de_high_school_government_and_politics": {
                  "original": 193,
                  "effective": 193
                },
                "global_mmlu_full_de_high_school_macroeconomics": {
                  "original": 390,
                  "effective": 390
                },
                "global_mmlu_full_de_high_school_microeconomics": {
                  "original": 238,
                  "effective": 238
                },
                "global_mmlu_full_de_econometrics": {
                  "original": 114,
                  "effective": 114
                },
                "global_mmlu_full_de_high_school_geography": {
                  "original": 198,
                  "effective": 198
                },
                "global_mmlu_full_de_security_studies": {
                  "original": 245,
                  "effective": 245
                },
                "global_mmlu_full_de_us_foreign_policy": {
                  "original": 100,
                  "effective": 100
                },
                "global_mmlu_full_de_public_relations": {
                  "original": 110,
                  "effective": 110
                },
                "global_mmlu_full_de_human_sexuality": {
                  "original": 131,
                  "effective": 131
                },
                "global_mmlu_full_de_sociology": {
                  "original": 201,
                  "effective": 201
                },
                "global_mmlu_full_de_professional_law": {
                  "original": 1534,
                  "effective": 1534
                },
                "global_mmlu_full_de_international_law": {
                  "original": 121,
                  "effective": 121
                },
                "global_mmlu_full_de_prehistory": {
                  "original": 324,
                  "effective": 324
                },
                "global_mmlu_full_de_world_religions": {
                  "original": 171,
                  "effective": 171
                },
                "global_mmlu_full_de_high_school_world_history": {
                  "original": 237,
                  "effective": 237
                },
                "global_mmlu_full_de_high_school_us_history": {
                  "original": 204,
                  "effective": 204
                },
                "global_mmlu_full_de_jurisprudence": {
                  "original": 108,
                  "effective": 108
                },
                "global_mmlu_full_de_formal_logic": {
                  "original": 126,
                  "effective": 126
                },
                "global_mmlu_full_de_logical_fallacies": {
                  "original": 163,
                  "effective": 163
                },
                "global_mmlu_full_de_high_school_european_history": {
                  "original": 165,
                  "effective": 165
                },
                "global_mmlu_full_de_philosophy": {
                  "original": 311,
                  "effective": 311
                },
                "global_mmlu_full_de_moral_scenarios": {
                  "original": 895,
                  "effective": 895
                },
                "global_mmlu_full_de_moral_disputes": {
                  "original": 346,
                  "effective": 346
                }
              },
              "config": {
                "model": "hf",
                "model_args": "pretrained=t5-base,trust_remote_code=True",
                "model_num_parameters": 222903552,
                "model_dtype": "torch.float32",
                "model_revision": "main",
                "model_sha": "a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1",
                "batch_size": "auto",
                "batch_sizes": [
                  64
                ],
                "device": "cuda",
                "use_cache": null,
                "limit": null,
                "bootstrap_iters": 100000,
                "gen_kwargs": null,
                "random_seed": 42,
                "numpy_seed": 42,
                "torch_seed": 42,
                "fewshot_seed": 42
              },
              "git_hash": "a4103c7",
              "date": 1758648742.9583387,
              "pretty_env_info": "PyTorch version: 2.7.1+cu126\nIs debug build: False\nCUDA used to build PyTorch: 12.6\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.2 LTS (x86_64)\nGCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\nClang version: Could not collect\nCMake version: version 3.24.1\nLibc version: glibc-2.35\n\nPython version: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] (64-bit runtime)\nPython platform: Linux-6.8.0-1036-nvidia-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H200\nNvidia driver version: 580.82.07\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               224\nOn-line CPU(s) list:                  0-223\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480C\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   56\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          3800.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            5.3 MiB (112 instances)\nL1i cache:                            3.5 MiB (112 instances)\nL2 cache:                             224 MiB (112 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-55,112-167\nNUMA node1 CPU(s):                    56-111,168-223\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pytorch-lightning==2.5.2\n[pip3] torch==2.7.1\n[pip3] torchmetrics==1.8.0\n[pip3] triton==3.3.1\n[conda] Could not collect",
              "transformers_version": "4.54.1",
              "lm_eval_version": "0.4.9",
              "upper_git_hash": null,
              "tokenizer_pad_token": [
                "<pad>",
                "0"
              ],
              "tokenizer_eos_token": [
                "</s>",
                "1"
              ],
              "tokenizer_bos_token": [
                null,
                "None"
              ],
              "eot_token_id": 1,
              "max_length": 512,
              "task_hashes": {},
              "model_source": "hf",
              "model_name": "t5-base",
              "model_name_sanitized": "t5-base",
              "system_instruction": null,
              "system_instruction_sha": null,
              "fewshot_as_multiturn": false,
              "chat_template": null,
              "chat_template_sha": null,
              "start_time": 634886.202944465,
              "end_time": 635239.160814773,
              "total_evaluation_time_seconds": "352.95787030796055"
            },
            "duration_seconds": 400.2540936470032,
            "duration_minutes": 6.670901560783387,
            "command": "python -m lm_eval --model hf --model_args pretrained=t5-base --tasks global_mmlu_full_de --device cuda --batch_size auto --output_path /netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/t5-base_global_mmlu_de_0shot_20250923_193047 --num_fewshot 0 --trust_remote_code --seed 42 --wandb_args project=BA-T5-CrossLingual,entity=nikolas-rauscher-dfki,group=crosslingual_transfer_eval_full_15k-final,name=t5-base_global_mmlu_de_0shot",
            "output_dir": "/netscratch/nrauscher/projects/BA-hydra/logs/eval_pipeline/runs/2025-09-23_19-00-53/evaluation/results/universal/scientific_crosslingual_transfer_eval_full_15k/t5-base_global_mmlu_de_0shot_20250923_193047",
            "status": "success"
          }
        }
      }
    }
  }
}