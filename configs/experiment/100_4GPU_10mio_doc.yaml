# @package _global_

# 4-GPU H100-80GB T5 Pre-training Configuration  
# 600k step production run with deterministic resume

defaults:
  - override /data: t5_pretraining_text_windows
  - override /model: t5_base
  - override /trainer: gpu
  - override /logger: wandb
  - override /callbacks: default

experiment_name: t5_4gpu_100k_val_optimized

tags: ["t5-base", "4gpu", "text-windows", "h100", "600k-steps", "100k-val", "validation-optimized"]

seed: 42

# Use separate directory for production training logs
hydra:
  run:
    dir: pretraining_logs/train/runs/${now:%Y-%m-%d_%H-%M-%S}

# Disable testing for pretraining
test: false
train: true

# 4-GPU training settings (memory optimized)
data:
  batch_size: 48  # per-GPU batch size (80GB)
  limit_files: -1  # Use all files
  limit_documents: -1  # Use all 260M docs for representative sampling
  train_val_split: [0.999615, 0.000385]  # 100k val docs (521 batches) for 600k step run
  num_workers: 3   # per-GPU workers (total 12 across 4 GPUs) - optimized for fscratch I/O
  prefetch_factor: 2          # modest; increase if input stalls
  persistent_workers: true    # avoid refork cost
  seed: 42                    # Seed for reproducible shuffling and resume

# 4-GPU distributed training settings (memory optimized)
trainer:
  max_epochs: -1  # unlimited epochs for continued pretraining
  max_steps: 600000  # ~7 days of training at current pace
  precision: bf16-mixed
  accumulate_grad_batches: 2  # Effective batch = 48 * 4 * 2 = 384 (same eff batch as 4GPUx2accum)
  devices: 4
  accelerator: gpu
  strategy: ddp
  gradient_clip_val: 0.5
  check_val_every_n_epoch: null  # disable epoch-based validation
  limit_val_batches: null  # ensure no validation limiting
  val_check_interval: 5000  # every 2.5k optimization steps (~35 points per 24h job, 41min interval)
  log_every_n_steps: 1
  use_distributed_sampler: true  # CRITICAL: Don't override our custom sampler (Lightning 2.0+)

model:
  t5_model:
    enable_gradient_checkpointing: true
  
  optimizer:
    _target_: transformers.optimization.Adafactor
    lr: 1e-4
    relative_step: false
    warmup_init: false
    scale_parameter: false
    _partial_: true
    # Override all AdamW parameters to avoid inheritance
    betas: null
    weight_decay: null
  
  scheduler:
    _target_: transformers.optimization.get_inverse_sqrt_schedule
    num_warmup_steps: 10000  # longer warmup for stability
    _partial_: true

callbacks:
  model_checkpoint:
    dirpath: "checkpoints/best/"
    filename: "step-{step:06d}-val_ppl-{val/perplexity:.3f}"
    save_top_k: 10
    monitor: "val/perplexity"
    mode: "min"
    save_last: true
    save_on_train_epoch_end: false
    save_weights_only: false
  step_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: "checkpoints/steps/"
    filename: "step-{step:06d}"
    every_n_train_steps: 5000  # Checkpoint every 2500 optimization steps (with accumulate_grad_batches: 2)
    save_last: true
    save_top_k: 10     # Keep last 10 step checkpoints (25k steps back)
    monitor: "step"    # Monitor step number  
    mode: "max"        # Keep highest step numbers (newest)
    save_weights_only: false
  timer:
    _target_: lightning.pytorch.callbacks.Timer
    duration: "00:23:55:00"  # Stop after 23h 55min (DD:HH:MM:SS format)
    interval: "step"         # Stop at step level for continuous training
  early_stopping:
    monitor: "val/loss"
    mode: "min"
    patience: 80  # Higher patience for 600k step run (80 * 2.5k = 200k steps patience)
    min_delta: 0.01  # More significant improvement required
    check_finite: false
    verbose: true

logger:
  wandb:
    group: "H100-4GPU-final"
    name: "final-600k-100k-val-2500steps-validation-260M-docs"
    tags: ["600k-steps", "100k-val-docs", "521-val-batches", "val-2500-interval", "260M-sampling", "optimized"]