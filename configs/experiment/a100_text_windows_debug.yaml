# @package _global_

# A100-80GB extensive test config 
# Utilize full 80GB memory with larger batch sizes and more epochs

defaults:
  - override /data: t5_pretraining_text_windows
  - override /model: t5_base
  - override /trainer: gpu
  - override /logger: wandb
  - override /callbacks: default

experiment_name: t5_debug_a100_text_windows

tags: ["t5-base", "debug", "text-windows", "a100", "lr-5e-4", "20k-docs", "val-check-0.25"]

seed: 42
test: false
train: true

data:
  batch_size: 48
  limit_files: -1
  limit_documents: 20000        # echtes Debug-Sample (vorher 2k)
  num_workers: 60

trainer:
  max_epochs: 11                # Multi-Epochen-Test
  precision: 16-mixed
  accumulate_grad_batches: 1 # war 4 
  devices: 1
  gradient_clip_val: 1.0
  check_val_every_n_epoch: 1
  max_steps: -1                  # kein Step-Cap
  val_check_interval: 0.25        # ~2x pro Epoche (statt 10x)
  log_every_n_steps: 1          # weniger Spam, noch fein

model:
  t5_model:
    enable_gradient_checkpointing: true
  
  optimizer:
    _target_: transformers.optimization.Adafactor
    lr: 5e-4
    relative_step: false
    warmup_init: false
    scale_parameter: false
    _partial_: true
    betas: null
    weight_decay: null
  
  scheduler:
    _target_: transformers.optimization.get_inverse_sqrt_schedule
    num_warmup_steps: 100        # ~1 Epoche (bleibt final-nah fürs Verhalten)
    _partial_: true

callbacks:
  model_checkpoint:
    save_top_k: 3
    monitor: "val/perplexity"
    mode: "min"
    save_last: true
    every_n_epochs: 1
    save_on_train_epoch_end: true

  early_stopping:
    monitor: val/perplexity
    mode: min
    patience: 20                 # genug Luft für 10 Epochen
    min_delta: 0.0
    verbose: true

logger:
  wandb:
    group: "debug-a100-text-windows"
    name: "t5-debug-a100-text-windows-lr-5e-4-20k-docs"
    tags: ["debug", "a100", "text-windows", "lr-5e-4", "20k-docs"]