[
  {
    "model_name": "t5-base-baseline",
    "run_type": "baseline",
    "steps": 0,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "acceptable",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "unacceptable",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": "acceptable",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "sst2",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8168903589248657
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "negative",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.999999463558197
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "equivalent",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "not_equivalent",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": "equivalent",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "not_duplicate",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "equivalent",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "not_duplicate",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "equivalent",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "2.8",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.5,
          "rouge_1": 0.5,
          "rouge_2": 0.0,
          "bert_score": 0.9684709310531616
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "0.0",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.5,
          "rouge_1": 0.5,
          "rouge_2": 0.0,
          "bert_score": 0.9454163312911987
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "neutral",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.9983687996864319
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "entailment",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "not_entailment",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": "entailment",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "not_entailment",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": "entailment",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "not_entailment",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": "entailment",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "the immune system protects the body against diseases by recognizing and attacking harmful substances . white blood cells are key components that identify and destroy pathogens like bacteria and viruses .",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.42105263157894735,
          "exact_match": 0.0,
          "bleu": 0.25806451612903225,
          "rouge_l": 0.3,
          "rouge_1": 0.45000000000000007,
          "rouge_2": 0.2631578947368421,
          "bert_score": 0.912310779094696
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s .",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.27586206896551724,
          "exact_match": 0.0,
          "bleu": 0.2222222222222222,
          "rouge_l": 0.20689655172413793,
          "rouge_1": 0.41379310344827586,
          "rouge_2": 0.14814814814814814,
          "bert_score": 0.877251923084259
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "Das Wetter ist heute sch\u00f6n.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8000000000000002,
          "exact_match": 0.0,
          "bleu": 0.8,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9912165403366089
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "Ich bin Student an der Universit\u00e4t.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.7692307692307692,
          "exact_match": 0.0,
          "bleu": 0.7054014374088451,
          "rouge_l": 0.9333333333333333,
          "rouge_1": 0.9333333333333333,
          "rouge_2": 0.7692307692307692,
          "bert_score": 0.9764138460159302
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "La biblioth\u00e8que est ferm\u00e9e le dimanche.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8333333333333334,
          "exact_match": 0.0,
          "bleu": 0.8333333333333334,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9939662218093872
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "Bonjour, comment \u00eates-vous?",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6666666666666666,
          "exact_match": 0.0,
          "bleu": 0.6666666666666666,
          "rouge_l": 0.75,
          "rouge_1": 0.75,
          "rouge_2": 0.3333333333333333,
          "bert_score": 0.9433156251907349
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Ich bin ein Student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.25,
          "rouge_1": 0.25,
          "rouge_2": 0.0,
          "bert_score": 0.9156805276870728
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "Bonjour, comment \u00e7a va?",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8247730731964111
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "Paris",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000005960464478
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "- (EN) This movie is absolutely fantastic and I loved every minute of",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "- (en) this movie is absolutely fantastic and i loved every minute of",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8345853686332703
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "a. This product is terrible and broke immediately.",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "negative",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.999999463558197
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "acceptable",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "cola: Was put the book by John on table the.",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": "cola: was put the book by john on table the.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8200767636299133
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "the immune system keeps a record of every germ it has defeated . it can destroy the germ quickly if it enters the body again .",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2,
          "exact_match": 0.0,
          "bleu": 0.11538461538461539,
          "rouge_l": 0.23529411764705882,
          "rouge_1": 0.23529411764705882,
          "rouge_2": 0.125,
          "bert_score": 0.8930082321166992
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "climate change refers to long-term shifts in global temperatures and weather patterns . human activities have been the main driver since the 1800s .",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.39999999999999997,
          "exact_match": 0.0,
          "bleu": 0.2916666666666667,
          "rouge_l": 0.4864864864864865,
          "rouge_1": 0.5405405405405405,
          "rouge_2": 0.1714285714285714,
          "bert_score": 0.9297895431518555
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "Das Wetter ist heute sch\u00f6n.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8000000000000002,
          "exact_match": 0.0,
          "bleu": 0.8,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9912165403366089
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Ich bin ein Student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.25,
          "rouge_1": 0.25,
          "rouge_2": 0.0,
          "bert_score": 0.9156805276870728
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "La biblioth\u00e8que est ferm\u00e9e le dimanche.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8333333333333334,
          "exact_match": 0.0,
          "bleu": 0.8333333333333334,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9939662218093872
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "False",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8373923897743225
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "True",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8247231841087341
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": ":: The capital of France is: The capital of France is",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8205389976501465
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "::: Photosynthesis is the process by which plants produce photo",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8177459239959717
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "grammar: I am going to the store tomorrow.",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.7999999999999999,
          "exact_match": 0.0,
          "bleu": 0.75,
          "rouge_l": 0.9333333333333333,
          "rouge_1": 0.9333333333333333,
          "rouge_2": 0.923076923076923,
          "bert_score": 0.9487582445144653
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "False",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8231762051582336
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "entailment",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7987209558486938
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "5.0",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.9386576414108276
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "not_entailment",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": "entailment",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "DNA is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses .",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.5161290322580645,
          "exact_match": 0.0,
          "bleu": 0.3333333333333333,
          "rouge_l": 0.5161290322580645,
          "rouge_1": 0.5161290322580645,
          "rouge_2": 0.27586206896551724,
          "bert_score": 0.8925724625587463
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "Das Experiment zeigte signifikante Ergebnisse.",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8000000000000002,
          "exact_match": 0.0,
          "bleu": 0.8,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9946615099906921
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "How does CRISPR gene editing work?",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.06896551724137931,
          "exact_match": 0.0,
          "bleu": 0.008297844727977323,
          "rouge_l": 0.13333333333333333,
          "rouge_1": 0.13333333333333333,
          "rouge_2": 0.0,
          "bert_score": 0.8543458580970764
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "quantum computing leverages quantum mechanical phenomena such as superposition and entanglement . quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers .",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.5714285714285714,
          "exact_match": 0.0,
          "bleu": 0.4117647058823529,
          "rouge_l": 0.5000000000000001,
          "rouge_1": 0.6153846153846154,
          "rouge_2": 0.36,
          "bert_score": 0.924479067325592
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "neutral",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.801758885383606
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "Describe steps to measure protein concentration using Bradford assay.",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3870967741935484,
          "exact_match": 0.0,
          "bleu": 0.12591706855837456,
          "rouge_l": 0.24242424242424243,
          "rouge_1": 0.36363636363636365,
          "rouge_2": 0.06451612903225806,
          "bert_score": 0.8618446588516235
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "enzymatic activity increased 5-fold at pH 8 compared to pH 6",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2222222222222222,
          "exact_match": 0.0,
          "bleu": 0.09299242178658203,
          "rouge_l": 0.2631578947368421,
          "rouge_1": 0.3684210526315789,
          "rouge_2": 0.05555555555555555,
          "bert_score": 0.8730493783950806
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.18750000000000003,
          "exact_match": 0.0,
          "bleu": 0.1637461506155964,
          "rouge_l": 0.1818181818181818,
          "rouge_1": 0.2424242424242424,
          "rouge_2": 0.06451612903225808,
          "bert_score": 0.873134434223175
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "key findings",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.08333333333333334,
          "exact_match": 0.0,
          "bleu": 2.2699964881242427e-05,
          "rouge_l": 0.16,
          "rouge_1": 0.16,
          "rouge_2": 0.08695652173913045,
          "bert_score": 0.8415790796279907
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": ": What is the difference between genotype and phenotype? define: What is the difference between genotype and phenotype? define: What is the difference between genotype and",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.17142857142857143,
          "exact_match": 0.0,
          "bleu": 0.13708051949162406,
          "rouge_l": 0.1851851851851852,
          "rouge_1": 0.1851851851851852,
          "rouge_2": 0.0,
          "bert_score": 0.8153545260429382
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "False",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8013363480567932
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": ": After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability.",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.20512820512820512,
          "exact_match": 0.0,
          "bleu": 0.2,
          "rouge_l": 0.3076923076923077,
          "rouge_1": 0.358974358974359,
          "rouge_2": 0.16216216216216214,
          "bert_score": 0.8819488883018494
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.46451269697040115,
      "exact_match": 0.2857142857142857,
      "bleu": 0.4383790631340257,
      "rouge_l": 0.5149122142852233,
      "rouge_1": 0.530353260930898,
      "rouge_2": 0.17505257511502623,
      "bert_score": 0.9065745941230229,
      "empty_ratio": 0.017857142857142856
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.9084449112415314,
        "count": 2
      },
      "mrpc": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0,
        "count": 2
      },
      "qqp": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.5,
        "bert_score": 0.9569436311721802,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.9991844594478607,
        "count": 2
      },
      "qnli": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 1
      },
      "rte": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 1
      },
      "wnli": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.32422867513611614,
        "exact_match": 0.0,
        "bleu": 0.22183450510063413,
        "rouge_l": 0.30716928896442086,
        "bert_score": 0.9030901193618774,
        "count": 4
      },
      "translation": {
        "token_f1": 0.5225071225071225,
        "exact_match": 0.0,
        "bleu": 0.5154149745269087,
        "rouge_l": 0.6870370370370371,
        "bert_score": 0.9495810137854682,
        "count": 9
      },
      "squad": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000002980232239,
        "count": 2
      },
      "classification": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.9172924160957336,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.9100384414196014,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8310577869415283,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8191424608230591,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.39999999999999997,
        "exact_match": 0.0,
        "bleu": 0.375,
        "rouge_l": 0.4666666666666667,
        "bert_score": 0.47437912225723267,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8109485805034637,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.9386576414108276,
        "count": 1
      },
      "entailment": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.543778801843318,
        "exact_match": 0.0,
        "bleu": 0.37254901960784315,
        "rouge_l": 0.5080645161290323,
        "bert_score": 0.9085257649421692,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.8000000000000002,
        "exact_match": 0.0,
        "bleu": 0.8,
        "rouge_l": 1.0,
        "bert_score": 0.9946615099906921,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.06896551724137931,
        "exact_match": 0.0,
        "bleu": 0.008297844727977323,
        "rouge_l": 0.13333333333333333,
        "bert_score": 0.8543458580970764,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.801758885383606,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.3870967741935484,
        "exact_match": 0.0,
        "bleu": 0.12591706855837456,
        "rouge_l": 0.24242424242424243,
        "bert_score": 0.8618446588516235,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.2222222222222222,
        "exact_match": 0.0,
        "bleu": 0.09299242178658203,
        "rouge_l": 0.2631578947368421,
        "bert_score": 0.8730493783950806,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.18750000000000003,
        "exact_match": 0.0,
        "bleu": 0.1637461506155964,
        "rouge_l": 0.1818181818181818,
        "bert_score": 0.873134434223175,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.08333333333333334,
        "exact_match": 0.0,
        "bleu": 2.2699964881242427e-05,
        "rouge_l": 0.16,
        "bert_score": 0.8415790796279907,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.17142857142857143,
        "exact_match": 0.0,
        "bleu": 0.13708051949162406,
        "rouge_l": 0.1851851851851852,
        "bert_score": 0.8153545260429382,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8013363480567932,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.20512820512820512,
        "exact_match": 0.0,
        "bleu": 0.2,
        "rouge_l": 0.3076923076923077,
        "bert_score": 0.8819488883018494,
        "count": 1
      }
    }
  },
  {
    "model_name": "red-run-t5-base-sci-cp-en-50k-steps-lr00001-clip0_5",
    "run_type": "red",
    "steps": 50000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "acceptable",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "acceptable: Book was by John put the on table.",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": "acceptable",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "sst2",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8168903589248657
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "this film is terrible and boring.",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": "negative",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.999999463558197
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "equivalent",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "not_equivalent",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": "equivalent",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "duplicate",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "equivalent",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "not_duplicate",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "equivalent",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "2.4",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.5,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.9855704307556152
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "0.0",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.5,
          "rouge_1": 0.5,
          "rouge_2": 0.0,
          "bert_score": 0.9454163312911987
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "neutral",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.9983687996864319
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "entailment",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "not_entailment",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": "entailment",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "not_entailment",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": "entailment",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "not_entailment",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": "entailment",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "white blood cells are key components that identify and destroy pathogens like bacteria and viruses .",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.23076923076923075,
          "exact_match": 0.0,
          "bleu": 0.1875,
          "rouge_l": 0.23076923076923075,
          "rouge_1": 0.23076923076923075,
          "rouge_2": 0.16666666666666666,
          "bert_score": 0.8814323544502258
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s .",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.27586206896551724,
          "exact_match": 0.0,
          "bleu": 0.2222222222222222,
          "rouge_l": 0.20689655172413793,
          "rouge_1": 0.41379310344827586,
          "rouge_2": 0.14814814814814814,
          "bert_score": 0.877251923084259
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "Das Wetter ist heute sch\u00f6n.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8000000000000002,
          "exact_match": 0.0,
          "bleu": 0.8,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9912165403366089
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "Ich bin Student an der Universit\u00e4t.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.7692307692307692,
          "exact_match": 0.0,
          "bleu": 0.7054014374088451,
          "rouge_l": 0.9333333333333333,
          "rouge_1": 0.9333333333333333,
          "rouge_2": 0.7692307692307692,
          "bert_score": 0.9764138460159302
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "La biblioth\u00e8que est ferm\u00e9e le dimanche.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8333333333333334,
          "exact_match": 0.0,
          "bleu": 0.8333333333333334,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9939662218093872
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "Bonjour, comment vous \u00eates-vous ?",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.5,
          "exact_match": 0.0,
          "bleu": 0.4,
          "rouge_l": 0.6666666666666665,
          "rouge_1": 0.6666666666666665,
          "rouge_2": 0.28571428571428575,
          "bert_score": 0.9200531244277954
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Ich bin ein Student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.25,
          "rouge_1": 0.25,
          "rouge_2": 0.0,
          "bert_score": 0.9156805276870728
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "Bonjour, comment \u00e7a va?",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8247730731964111
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "Paris",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000005960464478
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": ": This product is terrible and broke immediately.",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "negative",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.999999463558197
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "acceptable",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "cola: Was put the book by John on table the.",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": "cola: was put the book by john on table the.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8200767636299133
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "the immune system keeps a record of every germ it has defeated . it can destroy the germ quickly if it enters the body again .",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2,
          "exact_match": 0.0,
          "bleu": 0.11538461538461539,
          "rouge_l": 0.23529411764705882,
          "rouge_1": 0.23529411764705882,
          "rouge_2": 0.125,
          "bert_score": 0.8930082321166992
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "climate change refers to long-term shifts in global temperatures and weather patterns . human activities have been the main driver of climate change since the 1800s .",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3888888888888889,
          "exact_match": 0.0,
          "bleu": 0.25925925925925924,
          "rouge_l": 0.45,
          "rouge_1": 0.5,
          "rouge_2": 0.15789473684210525,
          "bert_score": 0.9262564182281494
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "Das Wetter ist heute sch\u00f6n.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8000000000000002,
          "exact_match": 0.0,
          "bleu": 0.8,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9912165403366089
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Ich bin ein Student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.25,
          "rouge_1": 0.25,
          "rouge_2": 0.0,
          "bert_score": 0.9156805276870728
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "La biblioth\u00e8que est ferm\u00e9e le dimanche.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8333333333333334,
          "exact_match": 0.0,
          "bleu": 0.8333333333333334,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9939662218093872
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "True",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8335915803909302
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "True",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8247231841087341
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "complete: The capital of France complete: complete complete: complete complete: complete complete",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8143891096115112
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": ":: Photosynthesis is the process by which plants produce and use the",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8090722560882568
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "grammar: grammar: grammar: grammar: grammar: grammar: grammar: grammar:",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7898530960083008
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "Grammar: She don't like pizza very much.",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6153846153846153,
          "exact_match": 0.0,
          "bleu": 0.5714285714285714,
          "rouge_l": 0.7999999999999999,
          "rouge_1": 0.7999999999999999,
          "rouge_2": 0.6153846153846153,
          "bert_score": 0.9385408163070679
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "Paris",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8185095191001892
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "True",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8282759189605713
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "4.0",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.5,
          "rouge_1": 0.5,
          "rouge_2": 0.0,
          "bert_score": 0.9411020278930664
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "not_entailment",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": "entailment",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses .",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.48484848484848486,
          "exact_match": 0.0,
          "bleu": 0.3076923076923077,
          "rouge_l": 0.48484848484848486,
          "rouge_1": 0.48484848484848486,
          "rouge_2": 0.25806451612903225,
          "bert_score": 0.8653842210769653
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "Das Experiment hat signifikante Ergebnisse gebracht.",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.7272727272727272,
          "exact_match": 0.0,
          "bleu": 0.6666666666666666,
          "rouge_l": 0.7272727272727272,
          "rouge_1": 0.7272727272727272,
          "rouge_2": 0.4444444444444445,
          "bert_score": 0.9470550417900085
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "Explain: How does CRISPR gene editing work?",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.06666666666666667,
          "exact_match": 0.0,
          "bleu": 0.012594669908908207,
          "rouge_l": 0.12903225806451613,
          "rouge_1": 0.12903225806451613,
          "rouge_2": 0.0,
          "bert_score": 0.8502947688102722
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers .",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.5,
          "exact_match": 0.0,
          "bleu": 0.45454545454545453,
          "rouge_l": 0.4390243902439024,
          "rouge_1": 0.6341463414634146,
          "rouge_2": 0.358974358974359,
          "bert_score": 0.9046576023101807
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "neutral",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.801758885383606
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": ": Describe steps to measure protein concentration using Bradford assay. Describe steps to measure protein concentration using Bradford assay. Describe steps to measure protein concentration using Bradford assay. Describe",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.37499999999999994,
          "exact_match": 0.0,
          "bleu": 0.20689655172413793,
          "rouge_l": 0.23076923076923075,
          "rouge_1": 0.23076923076923075,
          "rouge_2": 0.039999999999999994,
          "bert_score": 0.8318726420402527
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": ": Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.1818181818181818,
          "exact_match": 0.0,
          "bleu": 0.16546816923461166,
          "rouge_l": 0.1818181818181818,
          "rouge_1": 0.2424242424242424,
          "rouge_2": 0.06451612903225808,
          "bert_score": 0.8721596598625183
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "key findings",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.08333333333333334,
          "exact_match": 0.0,
          "bleu": 2.2699964881242427e-05,
          "rouge_l": 0.16,
          "rouge_1": 0.16,
          "rouge_2": 0.08695652173913045,
          "bert_score": 0.8415790796279907
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "define: What is the difference between genotype and phenotype? define: What is the difference between genotype and phenotype? define: What is the difference between genotype",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.1764705882352941,
          "exact_match": 0.0,
          "bleu": 0.13634300623459383,
          "rouge_l": 0.14814814814814817,
          "rouge_1": 0.1851851851851852,
          "rouge_2": 0.0,
          "bert_score": 0.8266611099243164
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.4436109325371496,
      "exact_match": 0.2857142857142857,
      "bleu": 0.42282307675610253,
      "rouge_l": 0.500426309309029,
      "rouge_1": 0.5191702664623637,
      "rouge_2": 0.1521606284340324,
      "bert_score": 0.8572628434215274,
      "empty_ratio": 0.07142857142857142
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.9084449112415314,
        "count": 2
      },
      "mrpc": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0,
        "count": 2
      },
      "qqp": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.5,
        "bert_score": 0.965493381023407,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.9991844594478607,
        "count": 2
      },
      "qnli": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 1
      },
      "rte": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 1
      },
      "wnli": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.2738800471559092,
        "exact_match": 0.0,
        "bleu": 0.1960915242165242,
        "rouge_l": 0.2807399750351069,
        "bert_score": 0.8944872319698334,
        "count": 4
      },
      "translation": {
        "token_f1": 0.5039886039886041,
        "exact_match": 0.0,
        "bleu": 0.48578534489727904,
        "rouge_l": 0.6777777777777777,
        "bert_score": 0.946996291478475,
        "count": 9
      },
      "squad": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000002980232239,
        "count": 2
      },
      "classification": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.4999997317790985,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.9100384414196014,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8291573822498322,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.811730682849884,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.30769230769230765,
        "exact_match": 0.0,
        "bleu": 0.2857142857142857,
        "rouge_l": 0.39999999999999997,
        "bert_score": 0.8641969561576843,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8233927190303802,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.5,
        "bert_score": 0.9411020278930664,
        "count": 1
      },
      "entailment": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.49242424242424243,
        "exact_match": 0.0,
        "bleu": 0.38111888111888115,
        "rouge_l": 0.4619364375461936,
        "bert_score": 0.885020911693573,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.7272727272727272,
        "exact_match": 0.0,
        "bleu": 0.6666666666666666,
        "rouge_l": 0.7272727272727272,
        "bert_score": 0.9470550417900085,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.06666666666666667,
        "exact_match": 0.0,
        "bleu": 0.012594669908908207,
        "rouge_l": 0.12903225806451613,
        "bert_score": 0.8502947688102722,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.801758885383606,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.37499999999999994,
        "exact_match": 0.0,
        "bleu": 0.20689655172413793,
        "rouge_l": 0.23076923076923075,
        "bert_score": 0.8318726420402527,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.1818181818181818,
        "exact_match": 0.0,
        "bleu": 0.16546816923461166,
        "rouge_l": 0.1818181818181818,
        "bert_score": 0.8721596598625183,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.08333333333333334,
        "exact_match": 0.0,
        "bleu": 2.2699964881242427e-05,
        "rouge_l": 0.16,
        "bert_score": 0.8415790796279907,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.1764705882352941,
        "exact_match": 0.0,
        "bleu": 0.13634300623459383,
        "rouge_l": 0.14814814814814817,
        "bert_score": 0.8266611099243164,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      }
    }
  },
  {
    "model_name": "red-run-t5-base-sci-cp-en-105k-steps-lr00001-clip0_5",
    "run_type": "red",
    "steps": 105000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "cola: cola: The book was put by John on the table. cola: The book was put by John on the table. cola: The book was put by John on the table",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": "cola: cola: the book was put by john on the table. cola: the book was put by john on the table. cola: the book was put by john on the table",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8044718503952026
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "cola: Book was by John put the on table. cola: Book was by John put the on table. cola: Book was by John put the on table. cola: Book",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": "cola: book was by john put the on table. cola: book was by john put the on table. cola: book was by john put the on table. cola: book",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7840340733528137
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "sst1: This movie is absolutely fantastic!",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": "sst1: this movie is absolutely fantastic!",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.812300980091095
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "equivalent",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "not_equivalent",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": "equivalent",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "duplicate",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "equivalent",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "not_duplicate",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "equivalent",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "2.2",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.5,
          "rouge_1": 0.5,
          "rouge_2": 0.0,
          "bert_score": 0.9998346567153931
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "0.4",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.5,
          "rouge_1": 0.5,
          "rouge_2": 0.0,
          "bert_score": 0.9708906412124634
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "neutral",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.9983687996864319
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "entailment",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "not_entailment",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": "entailment",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "not_entailment",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": "entailment",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "entailment",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "white blood cells are key components that identify and destroy pathogens like bacteria and viruses . the immune system recognizes and destroys harmful substances . a white blood cell is a key component that destroys a pathogen .",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3157894736842105,
          "exact_match": 0.0,
          "bleu": 0.15789473684210525,
          "rouge_l": 0.26086956521739124,
          "rouge_1": 0.26086956521739124,
          "rouge_2": 0.1818181818181818,
          "bert_score": 0.8781052231788635
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.28571428571428575,
          "exact_match": 0.0,
          "bleu": 0.23529411764705882,
          "rouge_l": 0.20689655172413793,
          "rouge_1": 0.41379310344827586,
          "rouge_2": 0.14814814814814814,
          "bert_score": 0.8803763389587402
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "Das Wetter ist heute sch\u00f6n.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8000000000000002,
          "exact_match": 0.0,
          "bleu": 0.8,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9912165403366089
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "Ich bin Student an der Universit\u00e4t.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.7692307692307692,
          "exact_match": 0.0,
          "bleu": 0.7054014374088451,
          "rouge_l": 0.9333333333333333,
          "rouge_1": 0.9333333333333333,
          "rouge_2": 0.7692307692307692,
          "bert_score": 0.9764138460159302
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "La biblioth\u00e8que est ferm\u00e9e le dimanche.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8333333333333334,
          "exact_match": 0.0,
          "bleu": 0.8333333333333334,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9939662218093872
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "Bonjour, comment \u00eates-vous?",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6666666666666666,
          "exact_match": 0.0,
          "bleu": 0.6666666666666666,
          "rouge_l": 0.75,
          "rouge_1": 0.75,
          "rouge_2": 0.3333333333333333,
          "bert_score": 0.9433156251907349
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Ich bin ein Student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.25,
          "rouge_1": 0.25,
          "rouge_2": 0.0,
          "bert_score": 0.9156805276870728
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "Bonjour, comment \u00e7a va?",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8247730731964111
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "Paris",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000005960464478
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "sentiment: this movie is absolutely fantastic and i loved every minute of it!",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8246665000915527
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "sentiment: This product is terrible and broke immediately.",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "negative",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.999999463558197
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "cola: cola: The book was put by John on the table. cola: The book was put by John on the table. cola: The book was put by John on the table",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": "cola: cola: the book was put by john on the table. cola: the book was put by john on the table. cola: the book was put by john on the table",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8044718503952026
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "cola: Was put the book by John on table the. cola: Was put the book by John on table the. cola: Was put the book by John on table the.",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": "cola: was put the book by john on table the. cola: was put the book by john on table the. cola: was put the book by john on table the.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7837038636207581
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "the immune system is a complex network of cells and proteins that defends the body . it keeps a record of every germ it has defeated so it can recognize and destroy the germ . if a germ infected the body, it can destroy it quickly .",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2,
          "exact_match": 0.0,
          "bleu": 0.0851063829787234,
          "rouge_l": 0.18518518518518517,
          "rouge_1": 0.22222222222222218,
          "rouge_2": 0.07692307692307693,
          "bert_score": 0.8707767128944397
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "climate change refers to long-term shifts in global temperatures and weather patterns . human activities have been the main driver of climate change since the 1800s . climate change is a natural phenomenon .",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.35000000000000003,
          "exact_match": 0.0,
          "bleu": 0.20588235294117646,
          "rouge_l": 0.391304347826087,
          "rouge_1": 0.43478260869565216,
          "rouge_2": 0.13636363636363635,
          "bert_score": 0.917296290397644
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "Das Wetter ist heute sch\u00f6n.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8000000000000002,
          "exact_match": 0.0,
          "bleu": 0.8,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9912165403366089
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Ich bin ein Student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.25,
          "rouge_1": 0.25,
          "rouge_2": 0.0,
          "bert_score": 0.9156805276870728
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "La biblioth\u00e8que est ferm\u00e9e le dimanche.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8333333333333334,
          "exact_match": 0.0,
          "bleu": 0.8333333333333334,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9939662218093872
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "Paraphrase: The scientist conducted an important experiment.",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.15384615384615383,
          "exact_match": 0.0,
          "bleu": 0.14285714285714285,
          "rouge_l": 0.15384615384615383,
          "rouge_1": 0.15384615384615383,
          "rouge_2": 0.0,
          "bert_score": 0.8894149661064148
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "Machine learning algorithms can analyze large datasets.",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.9239861965179443
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "complete: complete: complete: complete: complete: complete: complete: complete:",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7804483771324158
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "plants plants are the process by which plants Photosynthesis is the process",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8220930099487305
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "Grammar: I are going to the store tomorrow. grammar: I am going to",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.75,
          "exact_match": 0.0,
          "bleu": 0.46153846153846156,
          "rouge_l": 0.6,
          "rouge_1": 0.7000000000000001,
          "rouge_2": 0.6666666666666666,
          "bert_score": 0.9092602729797363
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "grammar: She don't like pizza very much. She don't like",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6153846153846153,
          "exact_match": 0.0,
          "bleu": 0.4,
          "rouge_l": 0.631578947368421,
          "rouge_1": 0.631578947368421,
          "rouge_2": 0.4705882352941177,
          "bert_score": 0.9309902191162109
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "Paris is the capital of France",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6666666666666666,
          "exact_match": 0.0,
          "bleu": 0.6666666666666666,
          "rouge_l": 0.8333333333333334,
          "rouge_1": 0.8333333333333334,
          "rouge_2": 0.8000000000000002,
          "bert_score": 0.9439532160758972
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "plants use photosynthesis to make food",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3636363636363636,
          "exact_match": 0.0,
          "bleu": 0.3333333333333333,
          "rouge_l": 0.5454545454545454,
          "rouge_1": 0.5454545454545454,
          "rouge_2": 0.22222222222222224,
          "bert_score": 0.8934370279312134
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "3.2",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.5,
          "rouge_1": 0.5,
          "rouge_2": 0.0,
          "bert_score": 0.9998125433921814
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "not_entailment",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": "entailment",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "deoxyribonucleic acid is a molecule that carries instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses .",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3870967741935483,
          "exact_match": 0.0,
          "bleu": 0.25,
          "rouge_l": 0.3870967741935483,
          "rouge_1": 0.3870967741935483,
          "rouge_2": 0.13793103448275862,
          "bert_score": 0.8590487837791443
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "Die Experimente haben signifikante Ergebnisse erbracht.",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3636363636363636,
          "exact_match": 0.0,
          "bleu": 0.3333333333333333,
          "rouge_l": 0.3636363636363636,
          "rouge_1": 0.3636363636363636,
          "rouge_2": 0.22222222222222224,
          "bert_score": 0.9378573298454285
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "Explain: How does CRISPR gene editing work?",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.06666666666666667,
          "exact_match": 0.0,
          "bleu": 0.012594669908908207,
          "rouge_l": 0.12903225806451613,
          "rouge_1": 0.12903225806451613,
          "rouge_2": 0.0,
          "bert_score": 0.8502947688102722
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "quantum computing leverages quantum mechanical phenomena such as superposition and entanglement . quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers .",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.5714285714285714,
          "exact_match": 0.0,
          "bleu": 0.4117647058823529,
          "rouge_l": 0.5000000000000001,
          "rouge_1": 0.6153846153846154,
          "rouge_2": 0.36,
          "bert_score": 0.924479067325592
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "neutral",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.801758885383606
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "Methodologie: Describe steps to measure protein concentration using Bradford assay. Describe steps to measure protein concentration using Bradford assay. Describe steps to measure protein concentration using Bradford assay.",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.37499999999999994,
          "exact_match": 0.0,
          "bleu": 0.21428571428571427,
          "rouge_l": 0.23076923076923075,
          "rouge_1": 0.23076923076923075,
          "rouge_2": 0.039999999999999994,
          "bert_score": 0.8361142873764038
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "analyze: analyze: analyze: analyze: Analyze: analyze: Analyze: Analyze: Analyze: Analyze: Analyze: Ana",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7474304437637329
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "Natural selection is the differential survival and reproduction of individuals due to differences in phenotype . Natural selection is the differential survival and reproduction of individuals due to differences in phenotype .",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.1818181818181818,
          "exact_match": 0.0,
          "bleu": 0.125,
          "rouge_l": 0.20833333333333334,
          "rouge_1": 0.20833333333333334,
          "rouge_2": 0.043478260869565216,
          "bert_score": 0.8504138588905334
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.5945945945945945,
          "exact_match": 0.0,
          "bleu": 0.45986532920024115,
          "rouge_l": 0.5641025641025642,
          "rouge_1": 0.6666666666666667,
          "rouge_2": 0.4864864864864865,
          "bert_score": 0.94572913646698
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": ": Define: Define: What is the difference between genotype and phenotype? Define: What is the difference between genotype and phenotype? Define:",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.17142857142857143,
          "exact_match": 0.0,
          "bleu": 0.13013531860475205,
          "rouge_l": 0.16326530612244897,
          "rouge_1": 0.20408163265306123,
          "rouge_2": 0.0,
          "bert_score": 0.8190300464630127
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs? predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reef",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2162162162162162,
          "exact_match": 0.0,
          "bleu": 0.16666666666666666,
          "rouge_l": 0.28571428571428575,
          "rouge_1": 0.32142857142857145,
          "rouge_2": 0.18518518518518517,
          "bert_score": 0.8866720199584961
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.4166337071692699,
      "exact_match": 0.21428571428571427,
      "bleu": 0.38269560184694307,
      "rouge_l": 0.47006700141473,
      "rouge_1": 0.48224362962587924,
      "rouge_2": 0.1835820974865423,
      "bert_score": 0.8826384288924081,
      "empty_ratio": 0.03571428571428571
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7942529618740082,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.4061504900455475,
        "count": 2
      },
      "mrpc": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0,
        "count": 2
      },
      "qqp": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.5,
        "bert_score": 0.9853626489639282,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.9991844594478607,
        "count": 2
      },
      "qnli": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 1
      },
      "rte": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 1
      },
      "wnli": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.28787593984962406,
        "exact_match": 0.0,
        "bleu": 0.17104439760226597,
        "rouge_l": 0.2610639124882003,
        "bert_score": 0.8866386413574219,
        "count": 4
      },
      "translation": {
        "token_f1": 0.5225071225071225,
        "exact_match": 0.0,
        "bleu": 0.5154149745269087,
        "rouge_l": 0.6870370370370371,
        "bert_score": 0.9495810137854682,
        "count": 9
      },
      "squad": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000002980232239,
        "count": 2
      },
      "classification": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.9123329818248749,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7940878570079803,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.07692307692307691,
        "exact_match": 0.0,
        "bleu": 0.07142857142857142,
        "rouge_l": 0.07692307692307691,
        "bert_score": 0.9067005813121796,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8012706935405731,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.6826923076923077,
        "exact_match": 0.0,
        "bleu": 0.4307692307692308,
        "rouge_l": 0.6157894736842104,
        "bert_score": 0.9201252460479736,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.5151515151515151,
        "exact_match": 0.0,
        "bleu": 0.5,
        "rouge_l": 0.6893939393939394,
        "bert_score": 0.9186951220035553,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.5,
        "bert_score": 0.9998125433921814,
        "count": 1
      },
      "entailment": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.47926267281105983,
        "exact_match": 0.0,
        "bleu": 0.33088235294117646,
        "rouge_l": 0.44354838709677424,
        "bert_score": 0.8917639255523682,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.3636363636363636,
        "exact_match": 0.0,
        "bleu": 0.3333333333333333,
        "rouge_l": 0.3636363636363636,
        "bert_score": 0.9378573298454285,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.06666666666666667,
        "exact_match": 0.0,
        "bleu": 0.012594669908908207,
        "rouge_l": 0.12903225806451613,
        "bert_score": 0.8502947688102722,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.801758885383606,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.37499999999999994,
        "exact_match": 0.0,
        "bleu": 0.21428571428571427,
        "rouge_l": 0.23076923076923075,
        "bert_score": 0.8361142873764038,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7474304437637329,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.1818181818181818,
        "exact_match": 0.0,
        "bleu": 0.125,
        "rouge_l": 0.20833333333333334,
        "bert_score": 0.8504138588905334,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.5945945945945945,
        "exact_match": 0.0,
        "bleu": 0.45986532920024115,
        "rouge_l": 0.5641025641025642,
        "bert_score": 0.94572913646698,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.17142857142857143,
        "exact_match": 0.0,
        "bleu": 0.13013531860475205,
        "rouge_l": 0.16326530612244897,
        "bert_score": 0.8190300464630127,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.2162162162162162,
        "exact_match": 0.0,
        "bleu": 0.16666666666666666,
        "rouge_l": 0.28571428571428575,
        "bert_score": 0.8866720199584961,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      }
    }
  },
  {
    "model_name": "red-run-t5-base-sci-cp-en-200k-steps-lr00001-clip0_5",
    "run_type": "red",
    "steps": 200000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "John",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": "john",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.9977560639381409
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "book was by John put the on table. cola: Book was by John put the on table. cola: Book was by John put the on table. cola: Book was by John",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": "book was by john put the on table. cola: book was by john put the on table. cola: book was by john put the on table. cola: book was by john",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7965514063835144
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "sst2: This movie is absolutely fantastic!",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": "sst2: this movie is absolutely fantastic!",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8137630820274353
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "sst2: This film is terrible and boring.",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": "negative",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.999999463558197
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "sentence2: A feline is resting on the sofa. sentence3: A feline is resting on the couch. sentence4: A feline is resting on the couch. sentence",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": "sentence2: a feline is resting on the sofa. sentence3: a feline is resting on the couch. sentence4: a feline is resting on the couch. sentence",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8121612071990967
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "sentence1: The weather is nice today. sentence2: The weather is nice today. sentence3: The weather is nice today. sentence4: The weather is nice today. sentence5: The",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": "sentence1: the weather is nice today. sentence2: the weather is nice today. sentence3: the weather is nice today. sentence4: the weather is nice today. sentence5: the",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8038942217826843
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "duplicate",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "equivalent",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "not_duplicate",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "equivalent",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "sentence1: A canine is jogging in the garden. sentence2: A canine is running in the garden. sentence3: A canine is running in the garden. sentence",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8480953574180603
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "sentence1: I love reading books. sentence2: I love reading books. sentence1: I love reading books. sentence2: I love reading books. sentence1: I love reading books.",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.846878170967102
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "contradiction",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "entailment",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "not_entailment",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": "entailment",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "entailment",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy didn't fit in the suitcase because it was too big. sentence3: The",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": "sentence1: the trophy didn't fit in the suitcase because it was too big. sentence2: the trophy didn't fit in the suitcase because it was too big. sentence3: the",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7776293754577637
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "white blood cells are key components that identify and destroy pathogens like bacteria and viruses . the immune system protects the body against diseases by recognizing and attacking harmful substances . .",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.42105263157894735,
          "exact_match": 0.0,
          "bleu": 0.25,
          "rouge_l": 0.3,
          "rouge_1": 0.45000000000000007,
          "rouge_2": 0.2631578947368421,
          "bert_score": 0.8966670632362366
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s . fossil fuel use has led to changes in global temperatures and weather patterns . human activities include burning coal, mining, and agriculture . .",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2608695652173913,
          "exact_match": 0.0,
          "bleu": 0.14285714285714285,
          "rouge_l": 0.19999999999999998,
          "rouge_1": 0.32,
          "rouge_2": 0.125,
          "bert_score": 0.877681314945221
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "Das Wetter ist heute sch\u00f6n.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8000000000000002,
          "exact_match": 0.0,
          "bleu": 0.8,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9912165403366089
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "Ich bin Student an der Universit\u00e4t.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.7692307692307692,
          "exact_match": 0.0,
          "bleu": 0.7054014374088451,
          "rouge_l": 0.9333333333333333,
          "rouge_1": 0.9333333333333333,
          "rouge_2": 0.7692307692307692,
          "bert_score": 0.9764138460159302
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "La biblioth\u00e8que est ferm\u00e9e le dimanche.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8333333333333334,
          "exact_match": 0.0,
          "bleu": 0.8333333333333334,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9939662218093872
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "Bonjour, comment \u00eates-vous?",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6666666666666666,
          "exact_match": 0.0,
          "bleu": 0.6666666666666666,
          "rouge_l": 0.75,
          "rouge_1": 0.75,
          "rouge_2": 0.3333333333333333,
          "bert_score": 0.9433156251907349
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Ich bin ein Student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.25,
          "rouge_1": 0.25,
          "rouge_2": 0.0,
          "bert_score": 0.9156805276870728
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "Bonjour, comment \u00e7a va?",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8247730731964111
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "Paris is the capital of France. It is located in northern France.",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.18181818181818182,
          "exact_match": 0.0,
          "bleu": 0.08333333333333333,
          "rouge_l": 0.15384615384615385,
          "rouge_1": 0.15384615384615385,
          "rouge_2": 0.0,
          "bert_score": 0.807623565196991
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "sentiment: This product is terrible and broke immediately. I don't know what",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "negative",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.999999463558197
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "John",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": "john",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.9977560639381409
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "the immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again . if a germ enters a body again, it can destroy it quickly .",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.23529411764705882,
          "exact_match": 0.0,
          "bleu": 0.0975609756097561,
          "rouge_l": 0.2040816326530612,
          "rouge_1": 0.2040816326530612,
          "rouge_2": 0.0851063829787234,
          "bert_score": 0.8758484125137329
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "climate change refers to long-term shifts in global temperatures and weather patterns . climate change is a natural phenomenon, but human activities have been the main driver since the 1800s . .",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.35000000000000003,
          "exact_match": 0.0,
          "bleu": 0.21875,
          "rouge_l": 0.4090909090909091,
          "rouge_1": 0.4545454545454545,
          "rouge_2": 0.14285714285714288,
          "bert_score": 0.9219205379486084
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "Das Wetter ist heute sch\u00f6n.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8000000000000002,
          "exact_match": 0.0,
          "bleu": 0.8,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9912165403366089
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Ich bin ein Student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.25,
          "rouge_1": 0.25,
          "rouge_2": 0.0,
          "bert_score": 0.9156805276870728
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "La biblioth\u00e8que est ferm\u00e9e le dimanche.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8333333333333334,
          "exact_match": 0.0,
          "bleu": 0.8333333333333334,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9939662218093872
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "paraphrase: The scientist conducted an important experiment. Paraphrase: 'Paraphrase: the scientist conducted a major experiment.",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.25,
          "exact_match": 0.0,
          "bleu": 0.13333333333333333,
          "rouge_l": 0.19047619047619044,
          "rouge_1": 0.19047619047619044,
          "rouge_2": 0.0,
          "bert_score": 0.8623791337013245
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "Machine learning algorithms can analyze large datasets. Paraphrase: Machine learning Algorithms can Analyze Large datasets .",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.867136538028717
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "complete: complete: complete: complete: complete: complete: complete: complete:",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7804483771324158
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "photosynthesis plants complete complete: photosynthesis is plants complete complete: photosynthesis plants",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8009762167930603
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "grammar: She don't like pizza very much.",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6153846153846153,
          "exact_match": 0.0,
          "bleu": 0.5714285714285714,
          "rouge_l": 0.7999999999999999,
          "rouge_1": 0.7999999999999999,
          "rouge_2": 0.6153846153846153,
          "bert_score": 0.9523647427558899
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii rosii",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7402603030204773
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "sentence1: The cat is sleeping. sentence2: A feline is resting. sentence3: A feline is sleeping. sentence4: A feline is sleeping. sentence5: A",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8344765901565552
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "contradiction",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": "not_entailment",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.6666666666666666,
          "rouge_1": 0.6666666666666666,
          "rouge_2": 0.0,
          "bert_score": 0.8495611548423767
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "DNA is a molecule that carries instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses . . deoxyribonucleic acid) . DNA .: deoxyrinucleo acid) is",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.411764705882353,
          "exact_match": 0.0,
          "bleu": 0.21875,
          "rouge_l": 0.3888888888888889,
          "rouge_1": 0.3888888888888889,
          "rouge_2": 0.11764705882352941,
          "bert_score": 0.8417958617210388
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "Die Experimente haben signifikante Ergebnisse gezeigt.",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3636363636363636,
          "exact_match": 0.0,
          "bleu": 0.3333333333333333,
          "rouge_l": 0.3636363636363636,
          "rouge_1": 0.3636363636363636,
          "rouge_2": 0.22222222222222224,
          "bert_score": 0.9417720437049866
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "quantum mechanical phenomena such as superposition and entanglement to process information . quantum computers can perform certain calculations exponentially faster than classical computers . enabling quantum computers to perform certain . calculations .",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.5238095238095238,
          "exact_match": 0.0,
          "bleu": 0.3333333333333333,
          "rouge_l": 0.4489795918367347,
          "rouge_1": 0.5306122448979592,
          "rouge_2": 0.2978723404255319,
          "bert_score": 0.8884297609329224
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "enzyme activity increased 5-fold at pH 8 compared to pH 6 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.34782608695652173,
          "exact_match": 0.0,
          "bleu": 0.30668147154310776,
          "rouge_l": 0.23076923076923078,
          "rouge_1": 0.5,
          "rouge_2": 0.12,
          "bert_score": 0.8859909176826477
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "Paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype. Paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.1818181818181818,
          "exact_match": 0.0,
          "bleu": 0.12903225806451613,
          "rouge_l": 0.20408163265306123,
          "rouge_1": 0.20408163265306123,
          "rouge_2": 0.0425531914893617,
          "bert_score": 0.8311967849731445
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7233624458312988
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": ": What is the difference between genotype and phenotype? What is the difference between genotype and phenotype? What is the difference between genotype and pheno",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.17142857142857143,
          "exact_match": 0.0,
          "bleu": 0.13634300623459383,
          "rouge_l": 0.18867924528301888,
          "rouge_1": 0.18867924528301888,
          "rouge_2": 0.0,
          "bert_score": 0.8215193748474121
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.3038797615668181,
      "exact_match": 0.14285714285714285,
      "bleu": 0.27845484874666426,
      "rouge_l": 0.33808088998452884,
      "rouge_1": 0.34997942512285984,
      "rouge_2": 0.1273993741336084,
      "bert_score": 0.7364665110196386,
      "empty_ratio": 0.17857142857142858
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8971537351608276,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.9068812727928162,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8080277144908905,
        "count": 2
      },
      "qqp": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8474867641925812,
        "count": 2
      },
      "mnli": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 2
      },
      "qnli": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 1
      },
      "rte": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7776293754577637,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.3168040786108494,
        "exact_match": 0.0,
        "bleu": 0.17729202961672474,
        "rouge_l": 0.2782931354359926,
        "bert_score": 0.8930293321609497,
        "count": 4
      },
      "translation": {
        "token_f1": 0.5225071225071225,
        "exact_match": 0.0,
        "bleu": 0.5154149745269087,
        "rouge_l": 0.6870370370370371,
        "bert_score": 0.9495810137854682,
        "count": 9
      },
      "squad": {
        "token_f1": 0.09090909090909091,
        "exact_match": 0.0,
        "bleu": 0.041666666666666664,
        "rouge_l": 0.07692307692307693,
        "bert_score": 0.4038117825984955,
        "count": 2
      },
      "classification": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.4999997317790985,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.49887803196907043,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.125,
        "exact_match": 0.0,
        "bleu": 0.06666666666666667,
        "rouge_l": 0.09523809523809522,
        "bert_score": 0.8647578358650208,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.790712296962738,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.30769230769230765,
        "exact_match": 0.0,
        "bleu": 0.2857142857142857,
        "rouge_l": 0.39999999999999997,
        "bert_score": 0.47618237137794495,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.37013015151023865,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8344765901565552,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.6666666666666666,
        "bert_score": 0.8495611548423767,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.4677871148459384,
        "exact_match": 0.0,
        "bleu": 0.27604166666666663,
        "rouge_l": 0.4189342403628118,
        "bert_score": 0.8651128113269806,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.3636363636363636,
        "exact_match": 0.0,
        "bleu": 0.3333333333333333,
        "rouge_l": 0.3636363636363636,
        "bert_score": 0.9417720437049866,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.34782608695652173,
        "exact_match": 0.0,
        "bleu": 0.30668147154310776,
        "rouge_l": 0.23076923076923078,
        "bert_score": 0.8859909176826477,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.1818181818181818,
        "exact_match": 0.0,
        "bleu": 0.12903225806451613,
        "rouge_l": 0.20408163265306123,
        "bert_score": 0.8311967849731445,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7233624458312988,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.17142857142857143,
        "exact_match": 0.0,
        "bleu": 0.13634300623459383,
        "rouge_l": 0.18867924528301888,
        "bert_score": 0.8215193748474121,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      }
    }
  },
  {
    "model_name": "flan-t5-base-baseline",
    "run_type": "baseline",
    "steps": 0,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "cola:",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8656719923019409
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "cola: book was by John put the on table.",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": "cola: book was by john put the on table.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8322303295135498
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "sst2: This movie is absolutely fantastic! sst2: This movie is absolutely fantastic! sst2: This movie is absolutely fantastic! sst",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": "sst2: this movie is absolutely fantastic! sst2: this movie is absolutely fantastic! sst2: this movie is absolutely fantastic! sst",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7758504152297974
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "sst2: This film is terrible and boring.",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": "negative",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.999999463558197
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "sentence1: The cat is sleeping on the couch sentence2: A feline is resting on the sofa",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": "sentence1: the cat is sleeping on the couch sentence2: a feline is resting on the sofa",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8298357725143433
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "sentence1: The weather is nice sentence2: I like pizza very much",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": "sentence1: the weather is nice sentence2: i like pizza very much",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8294284343719482
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "Cook it in a pot of boiling water.",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "cook it in a pot of boiling water.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8029814958572388
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "brew it in a pot of boiling water",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8063400387763977
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "sentence1: A dog is running in the park sentence2: A canine is jogging in the garden",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8630992770195007
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "sentence1: I love reading books sentence2: Cars are expensive nowadays",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8646531701087952
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "it is not possible to tell",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8427388072013855
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "it is not possible to tell",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8119936585426331
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "Berlin",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": "berlin",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.816590428352356
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "yes",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": "entailment",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "sentence1: The trophy didn't fit in the suitcase because it was too big sentence2: The trophy was too big",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": "sentence1: the trophy didn't fit in the suitcase because it was too big sentence2: the trophy was too big",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7923059463500977
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "Understand the role of the immune system.",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.23529411764705885,
          "exact_match": 0.0,
          "bleu": 0.16134803485935978,
          "rouge_l": 0.33333333333333326,
          "rouge_1": 0.33333333333333326,
          "rouge_2": 0.25,
          "bert_score": 0.8967316746711731
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "Climate change is the result of human activities, such as burning fossil fuels.",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.32,
          "exact_match": 0.0,
          "bleu": 0.3076923076923077,
          "rouge_l": 0.4,
          "rouge_1": 0.4,
          "rouge_2": 0.17391304347826086,
          "bert_score": 0.9225484728813171
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "The weather is beautiful today.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8388550877571106
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "Ich bin ein Student in der Universit\u00e4t.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.7142857142857143,
          "exact_match": 0.0,
          "bleu": 0.7142857142857143,
          "rouge_l": 0.875,
          "rouge_1": 0.875,
          "rouge_2": 0.7142857142857143,
          "bert_score": 0.983232855796814
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "La biblioth\u00e8que est clos\u00e9e sur les dimanches.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.4615384615384615,
          "exact_match": 0.0,
          "bleu": 0.42857142857142855,
          "rouge_l": 0.7058823529411765,
          "rouge_1": 0.7058823529411765,
          "rouge_2": 0.39999999999999997,
          "bert_score": 0.9598149061203003
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "Good morning, how are you?",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8158929347991943
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "I am a student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.75,
          "exact_match": 0.0,
          "bleu": 0.75,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9724712371826172
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "Hello, how is it going?",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.4444444444444445,
          "exact_match": 0.0,
          "bleu": 0.4,
          "rouge_l": 0.4444444444444445,
          "rouge_1": 0.4444444444444445,
          "rouge_2": 0.28571428571428575,
          "bert_score": 0.9424610733985901
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.9655172413793104,
          "exact_match": 0.0,
          "bleu": 0.9355069850316178,
          "rouge_l": 0.967741935483871,
          "rouge_1": 0.967741935483871,
          "rouge_2": 0.9655172413793104,
          "bert_score": 0.9790750741958618
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "Paris",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000005960464478
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "positive",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000005960464478
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "negative",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.999999463558197
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "cola:",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8656719923019409
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "cola:",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8656719923019409
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "Learn about the immune system.",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.26666666666666666,
          "exact_match": 0.0,
          "bleu": 0.14715177646857694,
          "rouge_l": 0.4,
          "rouge_1": 0.4,
          "rouge_2": 0.30769230769230765,
          "bert_score": 0.8854559063911438
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "Understand climate change.",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.125,
          "exact_match": 0.0,
          "bleu": 0.011891331115750803,
          "rouge_l": 0.23529411764705882,
          "rouge_1": 0.23529411764705882,
          "rouge_2": 0.13333333333333336,
          "bert_score": 0.8809983134269714
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "The weather is beautiful today.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8388550877571106
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "I am a student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.75,
          "exact_match": 0.0,
          "bleu": 0.75,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9724712371826172
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "La biblioth\u00e8que est clos\u00e9e sur les dimanches.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.4615384615384615,
          "exact_match": 0.0,
          "bleu": 0.42857142857142855,
          "rouge_l": 0.7058823529411765,
          "rouge_1": 0.7058823529411765,
          "rouge_2": 0.39999999999999997,
          "bert_score": 0.9598149061203003
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "The scientist conducted an important experiment.",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.16666666666666666,
          "exact_match": 0.0,
          "bleu": 0.16666666666666666,
          "rouge_l": 0.16666666666666666,
          "rouge_1": 0.16666666666666666,
          "rouge_2": 0.0,
          "bert_score": 0.9416234493255615
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "Machine learning algorithms can analyze large datasets.",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.9239861965179443
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "Paris",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000005960464478
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "are able to survive without sunlight.",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.2,
          "rouge_1": 0.2,
          "rouge_2": 0.0,
          "bert_score": 0.861822247505188
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "I am going to the store tomorrow.",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8571428571428571,
          "exact_match": 0.0,
          "bleu": 0.8571428571428571,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9904565215110779
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "She doesn't like pizza very much.",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8333333333333334,
          "exact_match": 0.0,
          "bleu": 0.8333333333333334,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9954075813293457
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "italy",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8044275045394897
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "photosynthesis is the process by which a plant converts light energy into light energy.",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.10526315789473682,
          "rouge_1": 0.10526315789473682,
          "rouge_2": 0.0,
          "bert_score": 0.8548874855041504
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "sentence1: The cat is sleeping sentence2: A feline is resting.",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8633778095245361
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "it is not possible to tell",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": "not_entailment",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.6666666666666666,
          "rouge_1": 0.6666666666666666,
          "rouge_2": 0.0,
          "bert_score": 0.8495611548423767
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.5,
          "exact_match": 0.0,
          "bleu": 0.32,
          "rouge_l": 0.48484848484848486,
          "rouge_1": 0.48484848484848486,
          "rouge_2": 0.25806451612903225,
          "bert_score": 0.8690133690834045
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "Die Experiment zeigte wichtigen Ergebnisse.",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.4000000000000001,
          "exact_match": 0.0,
          "bleu": 0.4,
          "rouge_l": 0.6,
          "rouge_1": 0.6,
          "rouge_2": 0.25,
          "bert_score": 0.9395869374275208
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "CRISPR gene editing is a gene editing technique that uses a chromosome as a template for a specific gene to be edited.",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.20512820512820512,
          "exact_match": 0.0,
          "bleu": 0.16601831205132045,
          "rouge_l": 0.17391304347826086,
          "rouge_1": 0.21739130434782608,
          "rouge_2": 0.0,
          "bert_score": 0.8942122459411621
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "Quantum computing utilizes quantum mechanical phenomena such as superposition and entanglement to process information.",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3636363636363637,
          "exact_match": 0.0,
          "bleu": 0.2791881675133095,
          "rouge_l": 0.4117647058823529,
          "rouge_1": 0.4117647058823529,
          "rouge_2": 0.18749999999999997,
          "bert_score": 0.9197977781295776
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "We would be able to grow new plants.",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.06896551724137931,
          "exact_match": 0.0,
          "bleu": 0.021721742931305642,
          "rouge_l": 0.13333333333333333,
          "rouge_1": 0.13333333333333333,
          "rouge_2": 0.0,
          "bert_score": 0.8661708831787109
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "The Bradford assay is used to measure the concentration of protein in the blood. The Bradford assay is used to measure the concentration of protein in the blood. The Bradford assay is used to measure",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.29411764705882354,
          "exact_match": 0.0,
          "bleu": 0.14285714285714285,
          "rouge_l": 0.2033898305084746,
          "rouge_1": 0.2033898305084746,
          "rouge_2": 0.0,
          "bert_score": 0.8252652287483215
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "The enzyme has a higher pH value.",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.30303030303030304,
          "exact_match": 0.0,
          "bleu": 0.04732304225161548,
          "rouge_l": 0.30303030303030304,
          "rouge_1": 0.30303030303030304,
          "rouge_2": 0.1935483870967742,
          "bert_score": 0.8602449893951416
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.18750000000000003,
          "exact_match": 0.0,
          "bleu": 0.1637461506155964,
          "rouge_l": 0.1818181818181818,
          "rouge_1": 0.2424242424242424,
          "rouge_2": 0.06451612903225808,
          "bert_score": 0.873134434223175
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "Machine learning algorithms predict drug-protein interactions with 87% accuracy.",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3870967741935484,
          "exact_match": 0.0,
          "bleu": 0.15725138865713328,
          "rouge_l": 0.42424242424242425,
          "rouge_1": 0.5454545454545454,
          "rouge_2": 0.3870967741935484,
          "bert_score": 0.9324066042900085
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "phenotype is a phenotype of a trait.",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.13333333333333333,
          "exact_match": 0.0,
          "bleu": 0.01233123121755788,
          "rouge_l": 0.05555555555555555,
          "rouge_1": 0.1111111111111111,
          "rouge_2": 0.0,
          "bert_score": 0.8588254451751709
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "increase the risk of coral extinction",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.14814814814814814,
          "exact_match": 0.0,
          "bleu": 0.019605490547476626,
          "rouge_l": 0.2,
          "rouge_1": 0.2,
          "rouge_2": 0.0,
          "bert_score": 0.8719321489334106
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "Cell viability decreases after treatment with compound X.",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.14285714285714288,
          "exact_match": 0.0,
          "bleu": 0.055782540037107455,
          "rouge_l": 0.14285714285714288,
          "rouge_1": 0.28571428571428575,
          "rouge_2": 0.07692307692307693,
          "bert_score": 0.8784195184707642
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.29437931070125395,
      "exact_match": 0.10714285714285714,
      "bleu": 0.2621069120074751,
      "rouge_l": 0.3485880005995294,
      "rouge_1": 0.3561542352620373,
      "rouge_2": 0.16157330016531968,
      "bert_score": 0.8926482658301081,
      "empty_ratio": 0.0
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8489511609077454,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.8879249393939972,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8296321034431458,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8046607673168182,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.863876223564148,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8273662328720093,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.816590428352356,
        "count": 1
      },
      "rte": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7923059463500977,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.2367401960784314,
        "exact_match": 0.0,
        "bleu": 0.1570208625339988,
        "rouge_l": 0.34215686274509804,
        "bert_score": 0.8964335918426514,
        "count": 4
      },
      "translation": {
        "token_f1": 0.39797856464523135,
        "exact_match": 0.0,
        "bleu": 0.3857142857142857,
        "rouge_l": 0.5256899055918665,
        "bert_score": 0.9204299251238505,
        "count": 9
      },
      "squad": {
        "token_f1": 0.9827586206896552,
        "exact_match": 0.5,
        "bleu": 0.9677534925158089,
        "rouge_l": 0.9838709677419355,
        "bert_score": 0.9895378351211548,
        "count": 2
      },
      "classification": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000000298023224,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8656719923019409,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.08333333333333333,
        "exact_match": 0.0,
        "bleu": 0.08333333333333333,
        "rouge_l": 0.08333333333333333,
        "bert_score": 0.9328048229217529,
        "count": 2
      },
      "completion": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.6,
        "bert_score": 0.9309114217758179,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.8452380952380952,
        "exact_match": 0.0,
        "bleu": 0.8452380952380952,
        "rouge_l": 1.0,
        "bert_score": 0.9929320514202118,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.05263157894736841,
        "bert_score": 0.8296574950218201,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8633778095245361,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.6666666666666666,
        "bert_score": 0.8495611548423767,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.4318181818181819,
        "exact_match": 0.0,
        "bleu": 0.29959408375665475,
        "rouge_l": 0.4483065953654189,
        "bert_score": 0.8944055736064911,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.4000000000000001,
        "exact_match": 0.0,
        "bleu": 0.4,
        "rouge_l": 0.6,
        "bert_score": 0.9395869374275208,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.20512820512820512,
        "exact_match": 0.0,
        "bleu": 0.16601831205132045,
        "rouge_l": 0.17391304347826086,
        "bert_score": 0.8942122459411621,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.06896551724137931,
        "exact_match": 0.0,
        "bleu": 0.021721742931305642,
        "rouge_l": 0.13333333333333333,
        "bert_score": 0.8661708831787109,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.29411764705882354,
        "exact_match": 0.0,
        "bleu": 0.14285714285714285,
        "rouge_l": 0.2033898305084746,
        "bert_score": 0.8252652287483215,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.30303030303030304,
        "exact_match": 0.0,
        "bleu": 0.04732304225161548,
        "rouge_l": 0.30303030303030304,
        "bert_score": 0.8602449893951416,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.18750000000000003,
        "exact_match": 0.0,
        "bleu": 0.1637461506155964,
        "rouge_l": 0.1818181818181818,
        "bert_score": 0.873134434223175,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.3870967741935484,
        "exact_match": 0.0,
        "bleu": 0.15725138865713328,
        "rouge_l": 0.42424242424242425,
        "bert_score": 0.9324066042900085,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.13333333333333333,
        "exact_match": 0.0,
        "bleu": 0.01233123121755788,
        "rouge_l": 0.05555555555555555,
        "bert_score": 0.8588254451751709,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.14814814814814814,
        "exact_match": 0.0,
        "bleu": 0.019605490547476626,
        "rouge_l": 0.2,
        "bert_score": 0.8719321489334106,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.14285714285714288,
        "exact_match": 0.0,
        "bleu": 0.055782540037107455,
        "rouge_l": 0.14285714285714288,
        "bert_score": 0.8784195184707642,
        "count": 1
      }
    }
  },
  {
    "model_name": "red-run-t5-base-sci-cp-en-300k-steps-lr00001-clip0_5",
    "run_type": "red",
    "steps": 300000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "sst2: This film is terrible and boring. This film is terrible and sst2: This film is terrible and sst2: This film is terrible and",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": "negative",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.999999463558197
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "sentence1: . sentence2: . sentence1: . sentence1: . sentence1: . sentence1: . sentence2: . sentence1:",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7872365713119507
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "mrpc sentence2: The weather is nice today. sentence1: The weather is nice today. sentence1: The weather is nice today. sentence2: The weather is nice",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": "mrpc sentence2: the weather is nice today. sentence1: the weather is nice today. sentence1: the weather is nice today. sentence2: the weather is nice",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8116710186004639
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "not_duplicate",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "equivalent",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "sentence1: The dog is running in the park. sentence2: The dog is running in the park. sentence3: The dog is running in the park. sentence4: The dog is",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8359937071800232
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "sentence1: I love reading books. sentence1: I love reading books. sentence2: sentence1: sentence1: sentence2: sentence1: sentence2: sentence3: sentence4",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8339067101478577
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "contradiction",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "library library is mnli  mnli mnli  mnli mnli mnli m",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7396718859672546
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "not_entailment",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": "entailment",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 1.0000001192092896
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "sentence1: The suitcase was too big. sentence3: sentence3: The trophy was too big. sentence3: The trophy was too big. sentence3: The trophy was too big. sentence",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": "sentence1: the suitcase was too big. sentence3: sentence3: the trophy was too big. sentence3: the trophy was too big. sentence3: the trophy was too big. sentence",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7816645503044128
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "white blood cells are key components that identify and destroy pathogens like bacteria and viruses . white blood . .1 .2 .3 .4 .5 .6. The immune system protects the body against diseases by recognizing and attacking harmful substances",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.36363636363636365,
          "exact_match": 0.0,
          "bleu": 0.20512820512820512,
          "rouge_l": 0.25,
          "rouge_1": 0.375,
          "rouge_2": 0.2173913043478261,
          "bert_score": 0.8740854859352112
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "long-term changes in global temperatures and weather patterns. Human activities, primarily burning fossil fuels, have been the main cause of climate change since the 1800s ad hoc era . Climate change refers to longterm changes -term trends in global temperature and weather",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2173913043478261,
          "exact_match": 0.0,
          "bleu": 0.11904761904761904,
          "rouge_l": 0.18518518518518517,
          "rouge_1": 0.25925925925925924,
          "rouge_2": 0.07692307692307693,
          "bert_score": 0.8670068979263306
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "Das Wetter ist sch\u00f6n heute.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8000000000000002,
          "exact_match": 0.0,
          "bleu": 0.8,
          "rouge_l": 0.8333333333333334,
          "rouge_1": 1.0,
          "rouge_2": 0.6,
          "bert_score": 0.9803104400634766
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "Ich bin Student an der Universit\u00e4t.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.7692307692307692,
          "exact_match": 0.0,
          "bleu": 0.7054014374088451,
          "rouge_l": 0.9333333333333333,
          "rouge_1": 0.9333333333333333,
          "rouge_2": 0.7692307692307692,
          "bert_score": 0.9764138460159302
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "La biblioth\u00e8que est ferm\u00e9e le dimanche.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8333333333333334,
          "exact_match": 0.0,
          "bleu": 0.8333333333333334,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9939662218093872
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "Good morning, how are you?",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8158929347991943
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Ich bin ein Student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.25,
          "rouge_1": 0.25,
          "rouge_2": 0.0,
          "bert_score": 0.9156805276870728
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "Bonjour, comment \u00e7a va?",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8247730731964111
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen. Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8749999999999999,
          "exact_match": 0.0,
          "bleu": 0.45454545454545453,
          "rouge_l": 0.653061224489796,
          "rouge_1": 0.653061224489796,
          "rouge_2": 0.6382978723404256,
          "bert_score": 0.9217454791069031
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": ":: Paris is the capital of France:: context: Paris is the capital and most populous city of France.: Paris is the capital and most populous",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.14285714285714288,
          "exact_match": 0.0,
          "bleu": 0.04,
          "rouge_l": 0.07999999999999999,
          "rouge_1": 0.07999999999999999,
          "rouge_2": 0.0,
          "bert_score": 0.8085291385650635
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "product is terrible and broke immediately. This product is terrible and broke immediately.",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "negative",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.999999463558197
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "the immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again if . The immune system .. adica adica adica continest clientii clientii clientiioileaoileaoilea",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.21052631578947364,
          "exact_match": 0.0,
          "bleu": 0.1,
          "rouge_l": 0.20833333333333334,
          "rouge_1": 0.20833333333333334,
          "rouge_2": 0.08695652173913043,
          "bert_score": 0.8452727794647217
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "Climate change.. Climate change............   .. .....",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.1111111111111111,
          "exact_match": 0.0,
          "bleu": 0.05190053731909962,
          "rouge_l": 0.22222222222222224,
          "rouge_1": 0.22222222222222224,
          "rouge_2": 0.125,
          "bert_score": 0.806782603263855
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "Das Wetter ist sch\u00f6n heute.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8000000000000002,
          "exact_match": 0.0,
          "bleu": 0.8,
          "rouge_l": 0.8333333333333334,
          "rouge_1": 1.0,
          "rouge_2": 0.6,
          "bert_score": 0.9803104400634766
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Ich bin ein Student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.25,
          "rouge_1": 0.25,
          "rouge_2": 0.0,
          "bert_score": 0.9156805276870728
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "La biblioth\u00e8que est ferm\u00e9e le dimanche.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.8333333333333334,
          "exact_match": 0.0,
          "bleu": 0.8333333333333334,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 1.0,
          "bert_score": 0.9939662218093872
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "Paraphrase: The scientist conducted an important experiment. The researcher conducted an experiment.",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.28571428571428575,
          "exact_match": 0.0,
          "bleu": 0.16666666666666666,
          "rouge_l": 0.2222222222222222,
          "rouge_1": 0.2222222222222222,
          "rouge_2": 0.12500000000000003,
          "bert_score": 0.8819735050201416
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "Machine learning algorithms can analyze large datasets. Machine Learning algorithms can Anaphrase: Machine learning can analyze Large datasets can analyze Anaphrase Paraphrase: Array Paraphrase , Machine learning analyzing large dataset Anaphrase Machine learning",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8388295769691467
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "complete: complete: complete: complete: complete: complete: complete complete: complete",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7834146618843079
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "Photosynthesis is plants Photosynthesis is plants Photosynthesis is plants Photosynthesis is plants",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8012357354164124
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "grammar: She don't like pizza very much. She don't like",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6153846153846153,
          "exact_match": 0.0,
          "bleu": 0.4,
          "rouge_l": 0.631578947368421,
          "rouge_1": 0.631578947368421,
          "rouge_2": 0.4705882352941177,
          "bert_score": 0.9309902191162109
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "Paris is the capital of France.",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6666666666666666,
          "exact_match": 0.0,
          "bleu": 0.6666666666666666,
          "rouge_l": 0.8333333333333334,
          "rouge_1": 0.8333333333333334,
          "rouge_2": 0.8000000000000002,
          "bert_score": 0.9209302067756653
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "Photosynthesis to make food Plants use photosynthesis to make food. Plants use photosynthesis to make food. Plants use photosynthesis",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3333333333333333,
          "exact_match": 0.0,
          "bleu": 0.10526315789473684,
          "rouge_l": 0.25,
          "rouge_1": 0.25,
          "rouge_2": 0.0909090909090909,
          "bert_score": 0.8662741184234619
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "sentence1: The cat is sleeping. sentence2: A feline is sleeping. sentence1: A feline is sleeping. sentence2: A feline is sleeping. sentence3: A",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8329108953475952
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "neutral",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8598328828811646
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "deoxyribonucleic acid) is a molecule that carries instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses . . viruses and many . bacteria, . yeast, yeast .",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3529411764705882,
          "exact_match": 0.0,
          "bleu": 0.17647058823529413,
          "rouge_l": 0.32432432432432434,
          "rouge_1": 0.32432432432432434,
          "rouge_2": 0.1142857142857143,
          "bert_score": 0.8402470350265503
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "Das Experiment hat bedeutende Ergebnisse gezeigt.",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.5454545454545454,
          "exact_match": 0.0,
          "bleu": 0.5,
          "rouge_l": 0.5454545454545454,
          "rouge_1": 0.5454545454545454,
          "rouge_2": 0.22222222222222224,
          "bert_score": 0.9107682108879089
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "CRISPR gene editing explain: How does CRISPR explain: explain: explain: How does CRISPR gene editing explain: explain: explain: How does CRISPR explain: explain:",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.06896551724137931,
          "exact_match": 0.0,
          "bleu": 0.041628407307320904,
          "rouge_l": 0.0851063829787234,
          "rouge_1": 0.0851063829787234,
          "rouge_2": 0.0,
          "bert_score": 0.8148403763771057
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "quantum mechanical phenomena such as superposition and entanglement to process information Quantum computing.  quantum quantum. . Unlike classical computers, quantum computers can perform certain calculations exponentially faster than classical computers that exist in definite states, quantum",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.43999999999999995,
          "exact_match": 0.0,
          "bleu": 0.3055555555555556,
          "rouge_l": 0.43636363636363634,
          "rouge_1": 0.509090909090909,
          "rouge_2": 0.22641509433962262,
          "bert_score": 0.8880593776702881
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions? The enzyme activity increased 5-fold at pH 8 compared to pH",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.35555555555555557,
          "exact_match": 0.0,
          "bleu": 0.25,
          "rouge_l": 0.32786885245901637,
          "rouge_1": 0.45901639344262296,
          "rouge_2": 0.10169491525423728,
          "bert_score": 0.8852787613868713
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.26107920302607723,
      "exact_match": 0.08928571428571429,
      "bleu": 0.22419537432932377,
      "rouge_l": 0.27419739660240644,
      "rouge_1": 0.2873452934080901,
      "rouge_2": 0.1297306217301113,
      "bert_score": 0.631538317671844,
      "empty_ratio": 0.2857142857142857
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.4999997317790985,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7994537949562073,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.5,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8349502086639404,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.8698360025882721,
        "count": 2
      },
      "qnli": {
        "token_f1": 1.0,
        "exact_match": 1.0,
        "bleu": 1.0,
        "rouge_l": 1.0,
        "bert_score": 1.0000001192092896,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7816645503044128,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.22566627372119363,
        "exact_match": 0.0,
        "bleu": 0.11901909037373093,
        "rouge_l": 0.21643518518518517,
        "bert_score": 0.8482869416475296,
        "count": 4
      },
      "translation": {
        "token_f1": 0.44843304843304854,
        "exact_match": 0.0,
        "bleu": 0.4413409004528347,
        "rouge_l": 0.5666666666666667,
        "bert_score": 0.9329993592368232,
        "count": 9
      },
      "squad": {
        "token_f1": 0.5089285714285714,
        "exact_match": 0.0,
        "bleu": 0.24727272727272726,
        "rouge_l": 0.36653061224489797,
        "bert_score": 0.8651373088359833,
        "count": 2
      },
      "classification": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.4999997317790985,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.14285714285714288,
        "exact_match": 0.0,
        "bleu": 0.08333333333333333,
        "rouge_l": 0.1111111111111111,
        "bert_score": 0.8604015409946442,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7923251986503601,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.30769230769230765,
        "exact_match": 0.0,
        "bleu": 0.2,
        "rouge_l": 0.3157894736842105,
        "bert_score": 0.46549510955810547,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.5,
        "exact_match": 0.0,
        "bleu": 0.38596491228070173,
        "rouge_l": 0.5416666666666667,
        "bert_score": 0.8936021625995636,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8329108953475952,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8598328828811646,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.3964705882352941,
        "exact_match": 0.0,
        "bleu": 0.24101307189542487,
        "rouge_l": 0.3803439803439803,
        "bert_score": 0.8641532063484192,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.5454545454545454,
        "exact_match": 0.0,
        "bleu": 0.5,
        "rouge_l": 0.5454545454545454,
        "bert_score": 0.9107682108879089,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.06896551724137931,
        "exact_match": 0.0,
        "bleu": 0.041628407307320904,
        "rouge_l": 0.0851063829787234,
        "bert_score": 0.8148403763771057,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.35555555555555557,
        "exact_match": 0.0,
        "bleu": 0.25,
        "rouge_l": 0.32786885245901637,
        "bert_score": 0.8852787613868713,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      }
    }
  },
  {
    "model_name": "green-run-t5-base-sci-cp-en-200k-steps-lr0001-clip1",
    "run_type": "green",
    "steps": 200000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "biciclet",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8013299703598022
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "ridt\u00e9 prevazut prevazut",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7810778021812439
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "is absolutely fantastic! tava",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8190922141075134
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "film is terrible and boring. sst1: This film is terrible and boring.",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": "negative",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.999999463558197
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "t\u00e9",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8598184585571289
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "\u00e4mtlichemetodats",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7623773217201233
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "t\u00e9\",harmonie",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7868700623512268
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "t\u00e9",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8598184585571289
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "t\u00e9t\u00e9 etaj",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8104743361473083
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "t\u00e9",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8308988213539124
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "rid",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8620840907096863
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "rid",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8620840907096863
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "biciclet tavat\u00e9 superioara zahar",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.757509708404541
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "carora cunoaste rafina cunoaste iesitINCLUD bucuracaroracarora",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7629679441452026
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "caroraINCLUD $150 ingrijire3-5...\"DAY9001carora",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.778286337852478
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": ": weather is beautiful today. Translate English to German: TranslateEnglish translate translate translate English translate translate German translate translate weather today is beautiful Today. Translat translate English into German: translate English: Translat German:",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.780485987663269
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "translate English to German: I am a student at the university.tile liber 2011, carne\u00e9tit\u00e9",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.09523809523809523,
          "exact_match": 0.0,
          "bleu": 0.07142857142857142,
          "rouge_l": 0.08333333333333333,
          "rouge_1": 0.08333333333333333,
          "rouge_2": 0.0,
          "bert_score": 0.8305150270462036
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "ceapacaroracarorarach## $150###### quarterlycosting $69.## scrie",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7560806274414062
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": ": Good morning, how are you? Translate English to French: Translate French to English: Translation English to Fran\u00e7ais: Translation: Translat French to French, French: Translation from French: English: Translat English: French: French to",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7549986243247986
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "to English: Translat German to German translate German to English translate German into English: Ich bin ein Student.Ich bin ein Sch\u00fcler.Itas Translate German to translated German into German: Ich sprachsprachsprach translate German translate",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.049999999999999996,
          "rouge_1": 0.049999999999999996,
          "rouge_2": 0.0,
          "bert_score": 0.8036863207817078
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "French to French: Bonjour Bonjour Bonjour, comment Bonjour, Bonjour, Comment Bonjour, traducre fran\u00e7ais en fran\u00e7ais: Bonjour, en French to English: Bonjour. Bonjour, Quel Quel traduc traducre Fran\u00e7ais en traducre Bonjour Bonjour",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7661327719688416
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "context: What is photosynthesis? squad question: What is photosynthesis? squad question: What is photosynthesis? context: What is context",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8138002753257751
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "ionat masiniharmonieharmonieharmonieharmonie",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7635363340377808
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "25,000",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8693462610244751
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "product: This product is terrible and broke immediately.",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "negative",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.999999463558197
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "biciclet",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8013299703598022
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "Dumitru 1/4 1/4thighspetridden Dumitru ingrijirehoppingcarorathighshoppinghei\u00dftthighs termic termic iesit Dumitru senzatithighs",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7537110447883606
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "iesit gradini Dumitru Dumitru (\" Dumitru 1/4 Dumitru iesit Dumitru articole termic termic termic termic termic buccaroram\u00e2nt iesit",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7651923894882202
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": ": weather is beautiful today. Translate English to German: TranslateEnglish translate translate translate English translate translate German translate translate weather today is beautiful Today. Translat translate English into German: translate English: Translat German:",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.780485987663269
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "to English: Translat German to German translate German to English translate German into English: Ich bin ein Student.Ich bin ein Sch\u00fcler.Itas Translate German to translated German into German: Ich sprachsprachsprach translate German translate",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.049999999999999996,
          "rouge_1": 0.049999999999999996,
          "rouge_2": 0.0,
          "bert_score": 0.8036863207817078
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "ceapacaroracarorarach## $150###### quarterlycosting $69.## scrie",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7560806274414062
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "paraphrase: The scientist conducted an important experiment. paraphrase paraphrase -paraphrase: Paraphrase: paraphrase, paraphrase and paraphrase. Paraphrase e paraphrase Paraphrase paraphra: paraphra paraphrase par",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.09090909090909091,
          "exact_match": 0.0,
          "bleu": 0.043478260869565216,
          "rouge_l": 0.06896551724137931,
          "rouge_1": 0.06896551724137931,
          "rouge_2": 0.0,
          "bert_score": 0.8314948678016663
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "rid\u201d).caroranumarulrachmarim\u00e2nt cunoaste iesit",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7760818600654602
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "France France France France France France France France France France France France France France France",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7920630574226379
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": ": :: grammar: I are going to the store tomorrow. Grammar",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.5555555555555556,
          "exact_match": 0.0,
          "bleu": 0.45454545454545453,
          "rouge_l": 0.75,
          "rouge_1": 0.75,
          "rouge_2": 0.5714285714285715,
          "bert_score": 0.9114775061607361
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "Pizza Pizza Pizza Pizza Pizza Pizza Pizza Pizza Pizza Pizza Pizza Pizza Pizza Pizza Pizza",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2857142857142857,
          "exact_match": 0.0,
          "bleu": 0.06666666666666667,
          "rouge_l": 0.09090909090909091,
          "rouge_1": 0.09090909090909091,
          "rouge_2": 0.0,
          "bert_score": 0.8163202404975891
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "Paris is the capital of France Paris is the capital of France. Paris is the capital of France. Paris is the capital of France",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6153846153846153,
          "exact_match": 0.0,
          "bleu": 0.16666666666666666,
          "rouge_l": 0.33333333333333337,
          "rouge_1": 0.33333333333333337,
          "rouge_2": 0.2857142857142857,
          "bert_score": 0.8896517753601074
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": ": Plants use photosynthesis to make food question: Plants use photosynthesis to make food question: Plants use photosynthesis to",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3076923076923077,
          "exact_match": 0.0,
          "bleu": 0.10526315789473684,
          "rouge_l": 0.2608695652173913,
          "rouge_1": 0.2608695652173913,
          "rouge_2": 0.09523809523809523,
          "bert_score": 0.8591165542602539
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "sentence1: A cat is sleeping.",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8932799696922302
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "DNA tavaharmonie _____ senzatifuz aluat organizatori scrie",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.13333333333333333,
          "exact_match": 0.0,
          "bleu": 0.12383969996431167,
          "rouge_l": 0.14285714285714288,
          "rouge_1": 0.14285714285714288,
          "rouge_2": 0.0,
          "bert_score": 0.7699365019798279
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "German to German: The experiment showed significant results. The experiment showed significant results. Translat English to German: The experiment showed significant results. Translat English to German: The experiment showed significant results.",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.13333333333333333,
          "exact_match": 0.0,
          "bleu": 0.03225806451612903,
          "rouge_l": 0.05555555555555555,
          "rouge_1": 0.05555555555555555,
          "rouge_2": 0.0,
          "bert_score": 0.7917243838310242
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "Dumitru vitezethighs\".t\u00e9taux Dumitru iesitcarora iesitm\u00e2ntrizm\u00e2nt 2014-11-m\u00e2nt Asociatim\u00e2ntm\u00e2ntm\u00e2ntm\u00e2nt identificat iesit",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.743008017539978
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "t\u00e9",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7853201031684875
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "biciclett\u00e9hiko",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7761988639831543
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "natural selection natural selection natural selection natural selection natural selection natural selection natural selection natural natural selection natural natural natural",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7858205437660217
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "willingness to tavaharmonieharmonieharmonie",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.77084881067276
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "t\u00e9 etaj",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7815313935279846
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.07530643959215388,
      "exact_match": 0.03571428571428571,
      "bleu": 0.05471690254557325,
      "rouge_l": 0.06938970604370047,
      "rouge_1": 0.06938970604370047,
      "rouge_2": 0.017006802721088437,
      "bert_score": 0.68049342078822,
      "empty_ratio": 0.16071428571428573
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7912038862705231,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.9095458388328552,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8110978901386261,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8233442604541779,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8206865787506104,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8620840907096863,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8620840907096863,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.757509708404541,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7650394290685654,
        "count": 4
      },
      "translation": {
        "token_f1": 0.010582010582010581,
        "exact_match": 0.0,
        "bleu": 0.007936507936507936,
        "rouge_l": 0.02037037037037037,
        "bert_score": 0.7813502550125122,
        "count": 9
      },
      "squad": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.788668304681778,
        "count": 2
      },
      "classification": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.9346728622913361,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.4006649851799011,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.045454545454545456,
        "exact_match": 0.0,
        "bleu": 0.021739130434782608,
        "rouge_l": 0.034482758620689655,
        "bert_score": 0.8037883639335632,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.39603152871131897,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.42063492063492064,
        "exact_match": 0.0,
        "bleu": 0.2606060606060606,
        "rouge_l": 0.42045454545454547,
        "bert_score": 0.8638988733291626,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.4615384615384615,
        "exact_match": 0.0,
        "bleu": 0.13596491228070173,
        "rouge_l": 0.2971014492753623,
        "bert_score": 0.8743841648101807,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8932799696922302,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.06666666666666667,
        "exact_match": 0.0,
        "bleu": 0.061919849982155835,
        "rouge_l": 0.07142857142857144,
        "bert_score": 0.756472259759903,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.13333333333333333,
        "exact_match": 0.0,
        "bleu": 0.03225806451612903,
        "rouge_l": 0.05555555555555555,
        "bert_score": 0.7917243838310242,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7853201031684875,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7761988639831543,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7858205437660217,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.77084881067276,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7815313935279846,
        "count": 1
      }
    }
  },
  {
    "model_name": "yellow-run-flan-t5-base-sci-cp-en-50k-steps-lr0001-clip1",
    "run_type": "yellow",
    "steps": 50000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "on the  on the  on the  on the  on the  on the  on the  on the  on the on the cola",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7590559720993042
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "cola: Book was by John by John put the on  cola: Book was by John book was by John book was by John book was by John book was",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": "cola: book was by john by john put the on  cola: book was by john book was by john book was by john book was by john book was",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7666107416152954
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "sst1: This movie is absolutely fantastic!sst1: This movie is absolutely fantastic! I absolutely love this movie.sst1:",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": "sst1: this movie is absolutely fantastic!sst1: this movie is absolutely fantastic! i absolutely love this movie.sst1:",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7942578792572021
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "s can fly. hypothesis: Penguins can fly. hypotheses: All birds can fly. hypotheses: All birds can fly. hypotheses: All birds can fly. hypothesis",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": "s can fly. hypothesis: penguins can fly. hypotheses: all birds can fly. hypotheses: all birds can fly. hypotheses: all birds can fly. hypothesis",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8141331672668457
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "English to German: [URL] : ::::: Today, the weather is beautiful today. : today: today: today: today: today.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.790944516658783
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "::: The library is closed on Sundays:: : : : : : : : : : :",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7699413299560547
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "English to French: Good morning, how are you?: Hello you?, how about you? you? you?: Hello!: Hello you, you you, you?,",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7570235133171082
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": ": Ich bin ein Student. .. :  Student. Student.:  Student. Student. Student. Student. Student. Student. Student. Student. student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.10526315789473685,
          "rouge_1": 0.10526315789473685,
          "rouge_2": 0.0,
          "bert_score": 0.8472208976745605
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "to English: Bonjour, comment \u00e7a va? Bonjour, Bonjour: Bonjour: Bonjour: Bonjour: Bonjour: Bonjour: Bonjour you?: Bonjour: Bonjour?: Bonjour: Bonjour:",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.08695652173913043,
          "rouge_1": 0.08695652173913043,
          "rouge_2": 0.0,
          "bert_score": 0.7746644616127014
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": ": What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen, respectively. context: :",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.7567567567567568,
          "exact_match": 0.0,
          "bleu": 0.5769230769230769,
          "rouge_l": 0.8,
          "rouge_1": 0.8,
          "rouge_2": 0.7894736842105263,
          "bert_score": 0.9280275702476501
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": ": context: Paris is located in northern France. context: Paris is located in northern France. context: Paris is located in northern France. context: Paris is located in northern France. context:",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2222222222222222,
          "exact_match": 0.0,
          "bleu": 0.03333333333333333,
          "rouge_l": 0.06666666666666667,
          "rouge_1": 0.06666666666666667,
          "rouge_2": 0.0,
          "bert_score": 0.8330855965614319
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "sentiment: I loved every minute of it! I loved every minute of",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "sentiment: i loved every minute of it! i loved every minute of",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8190935850143433
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": ": This product is terrible and broke immediately. This product is terrible and broke",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "negative",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.999999463558197
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "on the  on the  on the  on the  on the  on the  on the  on the  on the on the cola",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7590559720993042
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "on  on  on  on  on cola: was put the book by John on  on  on  on",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": "on  on  on  on  on cola: was put the book by john on  on  on  on",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7736971974372864
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": ":",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7789499163627625
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": ":",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7881237268447876
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "English to German: [URL] : ::::: Today, the weather is beautiful today. : today: today: today: today: today.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.790944516658783
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": ": Ich bin ein Student. .. :  Student. Student.:  Student. Student. Student. Student. Student. Student. Student. Student. student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.10526315789473685,
          "rouge_1": 0.10526315789473685,
          "rouge_2": 0.0,
          "bert_score": 0.8472208976745605
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "::: The library is closed on Sundays:: : : : : : : : : : :",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7699413299560547
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "paraphrase: The scientist conducted an important experiment. The scientist performed a significant experiment. paraphrases: the scientist conducted a important experiment the experiment was important experiment the experiment experiment: experiment:",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.4,
          "exact_match": 0.0,
          "bleu": 0.13793103448275862,
          "rouge_l": 0.2285714285714286,
          "rouge_1": 0.2285714285714286,
          "rouge_2": 0.12121212121212122,
          "bert_score": 0.8457095623016357
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": ".",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8020904660224915
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": ": The capital of France is France: The capital of France is France:",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8057892322540283
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": ":::: Photosynthesis is the process by which plants",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8191658854484558
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "capital of France: Paris is the capital of France: Paris is the capital of France is the capital of France is the",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6153846153846153,
          "exact_match": 0.0,
          "bleu": 0.18181818181818182,
          "rouge_l": 0.35714285714285715,
          "rouge_1": 0.35714285714285715,
          "rouge_2": 0.3076923076923077,
          "bert_score": 0.8693021535873413
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "to answer the: Plants use photosynthesis to make food to make photosynthesis make photosynthesis make photosynthesis make food is",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2857142857142857,
          "exact_match": 0.0,
          "bleu": 0.10526315789473684,
          "rouge_l": 0.25,
          "rouge_1": 0.25,
          "rouge_2": 0.0909090909090909,
          "bert_score": 0.8591154217720032
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": ": Penguins have wings.",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": ": penguins have wings.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7980362772941589
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": ": The experiment showed significant results: The experiment showed significant results: The experiment showed significant results: The experiment showed significant results: The experiment showed significant results: The experiment showed",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.1818181818181818,
          "exact_match": 0.0,
          "bleu": 0.034482758620689655,
          "rouge_l": 0.06060606060606061,
          "rouge_1": 0.06060606060606061,
          "rouge_2": 0.0,
          "bert_score": 0.7667845487594604
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": ":::: explain: explain: explain: explain: explain: explain: explain: explain: explain: explain: explain: explain: explain: explain: explain: explain:",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7532548904418945
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": ": Quantum computing",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.1739130434782609,
          "exact_match": 0.0,
          "bleu": 0.002306251557643172,
          "rouge_l": 0.18181818181818182,
          "rouge_1": 0.18181818181818182,
          "rouge_2": 0.1,
          "bert_score": 0.8470689058303833
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": ": Describe steps to measure protein concentration using Bradford assay: Describe steps to measure protein concentration using ELISA: Describe steps to measure protein concentration using ELISA: Describe",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3636363636363636,
          "exact_match": 0.0,
          "bleu": 0.2222222222222222,
          "rouge_l": 0.24000000000000002,
          "rouge_1": 0.24000000000000002,
          "rouge_2": 0.041666666666666664,
          "bert_score": 0.831577718257904
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "2\u00b0C???: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?: If ocean temperatures rise by 2\u00b0",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2162162162162162,
          "exact_match": 0.0,
          "bleu": 0.17317265578604057,
          "rouge_l": 0.3404255319148936,
          "rouge_1": 0.3404255319148936,
          "rouge_2": 0.22222222222222224,
          "bert_score": 0.88655686378479
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.0752796729504804,
      "exact_match": 0.017857142857142856,
      "bleu": 0.04406165486854791,
      "rouge_l": 0.06826274221872665,
      "rouge_1": 0.06826274221872665,
      "rouge_2": 0.02987814451630241,
      "bert_score": 0.46511507460049223,
      "empty_ratio": 0.42857142857142855
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7628333568572998,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.3971289396286011,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.40706658363342285,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.3917684108018875,
        "count": 4
      },
      "translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.033053648614289347,
        "bert_score": 0.7053223848342896,
        "count": 9
      },
      "squad": {
        "token_f1": 0.4894894894894895,
        "exact_match": 0.0,
        "bleu": 0.3051282051282051,
        "rouge_l": 0.43333333333333335,
        "bert_score": 0.880556583404541,
        "count": 2
      },
      "classification": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.9095465242862701,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7663765847682953,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.2,
        "exact_match": 0.0,
        "bleu": 0.06896551724137931,
        "rouge_l": 0.1142857142857143,
        "bert_score": 0.8239000141620636,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8124775588512421,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.4505494505494505,
        "exact_match": 0.0,
        "bleu": 0.14354066985645933,
        "rouge_l": 0.3035714285714286,
        "bert_score": 0.8642087876796722,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7980362772941589,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.08695652173913045,
        "exact_match": 0.0,
        "bleu": 0.001153125778821586,
        "rouge_l": 0.09090909090909091,
        "bert_score": 0.42353445291519165,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.1818181818181818,
        "exact_match": 0.0,
        "bleu": 0.034482758620689655,
        "rouge_l": 0.06060606060606061,
        "bert_score": 0.7667845487594604,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7532548904418945,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.3636363636363636,
        "exact_match": 0.0,
        "bleu": 0.2222222222222222,
        "rouge_l": 0.24000000000000002,
        "bert_score": 0.831577718257904,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.2162162162162162,
        "exact_match": 0.0,
        "bleu": 0.17317265578604057,
        "rouge_l": 0.3404255319148936,
        "bert_score": 0.88655686378479,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      }
    }
  },
  {
    "model_name": "green-run-t5-base-sci-cp-en-655k-steps-lr0001-clip1",
    "run_type": "green",
    "steps": 655000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "cola cola: The book was put by John on the table",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": "cola cola: the book was put by john on the table",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7960007786750793
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "). cola: Book was by John put on table.",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": "). cola: book was by john put on table.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.844865620136261
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "sst2: sst2: sst2: sst2:",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7790058255195618
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "mother.......",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.860638439655304
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "The cat is sleeping on the couch.",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": "the cat is sleeping on the couch.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8514364361763
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": ". sentence2: I like pizza.",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": ". sentence2: i like pizza.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8230940699577332
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": ", , , , , , , , , , , , , , , , ,",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7427614331245422
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "venant venant Jacksonville",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "venant venant jacksonville",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8205053210258484
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "later  Men Emp Emp Mold",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8266030550003052
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "hypothesis: All birds can fly. mnli mnli mnli mnli mnli mnli",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": "hypothesis: all birds can fly. mnli mnli mnli mnli mnli mnli",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8063011765480042
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "The library is open today.",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": "the library is open today.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8109080195426941
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "101ytonytongleuckn",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7701963186264038
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "Jacksonville nearby accordingmakers #Fig.stor>now",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": "not_entailment",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.6666666666666666,
          "rouge_1": 0.6666666666666666,
          "rouge_2": 0.0,
          "bert_score": 0.8495611548423767
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "inteleger wnli sentence1: The trophy was too big. sentence2: The trophy was too big. sentence3:",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": "inteleger wnli sentence1: the trophy was too big. sentence2: the trophy was too big. sentence3:",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7885189056396484
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "During theighawayivitate usoaraIONS usoaraIONSivitateIONS",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7573577165603638
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "gotn-DonuwelynIONAL\u0103riARYBerourocarul zaharenkopet today\u20ac ly\u00f6\u00df veryrich and and Alb fertigo",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7516953349113464
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "weather is beautiful today. Translat German translate translate German translate German: German: The weather is warm today. German translation translation Translation translate German Translat Translat German Translat German German translation Translation Translation Translation German: . TranslatGerman:",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7771632075309753
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "When translation together- of the. When or Kindergarten, German, and2 . Kindergarten",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7911495566368103
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "-The library is closed on Sundays.rizable. Falrizably. , By",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8097779750823975
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "Translate English to French: Translation Good Good Good Morning, Good morning, how are you? Translate French to French Translate GoodGoodGood GoodGoodGoodGood a a Translation Translation Bonjour,",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.1111111111111111,
          "exact_match": 0.0,
          "bleu": 0.038461538461538464,
          "rouge_l": 0.06666666666666668,
          "rouge_1": 0.06666666666666668,
          "rouge_2": 0.0,
          "bert_score": 0.7852816581726074
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Translat German to English Translat German into English: translate German to German: Ich bin Student. Ich bin Deutsch Student.Deutsch. \u00dcbersetzung \u00dcbersetzung ich Deutsch, German Ich ich bin Student Student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.1,
          "exact_match": 0.0,
          "bleu": 0.034482758620689655,
          "rouge_l": 0.058823529411764705,
          "rouge_1": 0.058823529411764705,
          "rouge_2": 0.0,
          "bert_score": 0.8117058873176575
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "Translate French to English to English: Bonjour, comment Bonjour Bonjour Bonjour, Bonjour, Bonjour Bonjour a Bonjour Bonjour. Translate English into English: Bonjour : Bonjour Bonjour Bonjour? Bonjour Bonjour? ( Bonjour",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7761152386665344
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "hillbuckrenumitetitiangustegr\u00e9gr\u00e9ea",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7523131966590881
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "Sfantivitaterophy andnature Lacktell",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8098114132881165
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "cola cola: The book was put by John on the table",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": "cola cola: the book was put by john on the table",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7960007786750793
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "cola The book by John. Sfant de cola cola",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": "cola the book by john. sfant de cola cola",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.788280725479126
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": ".nahAlb.IONAL resurseoarabe arie imediatPre andturesHiggiarar\u00e8s Everruption Regulament usoaralyIONS whether",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.766491174697876
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "Mean Climate Change Gesichtlection",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.23529411764705882,
          "exact_match": 0.0,
          "bleu": 0.052699612280932166,
          "rouge_l": 0.22222222222222224,
          "rouge_1": 0.22222222222222224,
          "rouge_2": 0.125,
          "bert_score": 0.8124491572380066
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "weather is beautiful today. Translat German translate translate German translate German: German: The weather is warm today. German translation translation Translation translate German Translat Translat German Translat German German translation Translation Translation Translation German: . TranslatGerman:",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7771632075309753
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Translat German to English Translat German into English: translate German to German: Ich bin Student. Ich bin Deutsch Student.Deutsch. \u00dcbersetzung \u00dcbersetzung ich Deutsch, German Ich ich bin Student Student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.1,
          "exact_match": 0.0,
          "bleu": 0.034482758620689655,
          "rouge_l": 0.058823529411764705,
          "rouge_1": 0.058823529411764705,
          "rouge_2": 0.0,
          "bert_score": 0.8117058873176575
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "-The library is closed on Sundays.rizable. Falrizably. , By",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8097779750823975
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "The scientist conducted an important experiment, Scientist conducted the scientist conducted a important experiment. , , scientist conducted the scientist  scientist Scientist conducted an Scientist scientist adicas Scientist conducted",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.25,
          "exact_match": 0.0,
          "bleu": 0.06896551724137931,
          "rouge_l": 0.12121212121212122,
          "rouge_1": 0.12121212121212122,
          "rouge_2": 0.0,
          "bert_score": 0.8224555253982544
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": ". Kindergarten   , . Paraphrase: Machine learning algorithms can analyze large datasets.ingham and and , and can analyze",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8467584848403931
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "culori France is France complete: Paris complete: Paris complete: Paris complete: Paris",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.33333333333333337,
          "exact_match": 0.0,
          "bleu": 0.08333333333333333,
          "rouge_l": 0.15384615384615385,
          "rouge_1": 0.15384615384615385,
          "rouge_2": 0.0,
          "bert_score": 0.8163667917251587
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "complete photosynthesis Complete: Photosynthesis",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8387231230735779
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "I am going to the store tomorrow. I am going to the store tomorrow",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.9333333333333333,
          "exact_match": 0.0,
          "bleu": 0.5,
          "rouge_l": 0.6666666666666666,
          "rouge_1": 0.6666666666666666,
          "rouge_2": 0.631578947368421,
          "bert_score": 0.9344149231910706
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "Pizza: Pizza: Pizza: Pizza:",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.18181818181818182,
          "rouge_1": 0.18181818181818182,
          "rouge_2": 0.0,
          "bert_score": 0.8289176821708679
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "question question: Paris is the capital of France? question: Paris is the capital of France? question: Paris is the capital of Paris?",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6666666666666667,
          "exact_match": 0.0,
          "bleu": 0.22727272727272727,
          "rouge_l": 0.35714285714285715,
          "rouge_1": 0.35714285714285715,
          "rouge_2": 0.3076923076923077,
          "bert_score": 0.8908482789993286
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "question: question: question: question: question: question: question: question: question",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.763100266456604
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "wings. wings. wings. wings. wings. wings. wings. wings. wings. wings. wings. wings. wings. wings. wings. wings. wings. wings",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.764675498008728
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "goterieIONSivitateoplastyoplasty Nat localitate localitate",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.75998455286026
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "Translat English to German: Translat English to German: translate German: translate German:testedtestedtestedtestedtestedtestedtestedtested",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.741860032081604
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "Answer Answer: How does CRISPR gene editing explain: Explain: How does CRISPR gene editing? Explain: How does CRISPR gene editing? Explain: How CRISPR gene editing",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0625,
          "exact_match": 0.0,
          "bleu": 0.04,
          "rouge_l": 0.08163265306122448,
          "rouge_1": 0.08163265306122448,
          "rouge_2": 0.0,
          "bert_score": 0.8066005110740662
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "# . law beneficence a way of engin ture Raum",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.06666666666666667,
          "exact_match": 0.0,
          "bleu": 0.036787944117144235,
          "rouge_l": 0.07142857142857144,
          "rouge_1": 0.07142857142857144,
          "rouge_2": 0.0,
          "bert_score": 0.8055394887924194
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "- - - - ----------------------------------------------------------------------------------",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7302826046943665
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "Describe steps to measure protein concentration using Bradford assay: Describe steps to measure protein concentration using Bradford assay: Describe steps to measure protein concentration using Bradford assay: conduc",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.37499999999999994,
          "exact_match": 0.0,
          "bleu": 0.21428571428571427,
          "rouge_l": 0.23076923076923075,
          "rouge_1": 0.23076923076923075,
          "rouge_2": 0.039999999999999994,
          "bert_score": 0.8347648978233337
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": ", later Jacksonvillestab 10-15works >>areiediedHi]Hi",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7842333316802979
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "around NW] SP nearby >>ordnung >>",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7672895789146423
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "What are the key findings? White,''What are the key findings? Question: What is the key question?",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.1935483870967742,
          "exact_match": 0.0,
          "bleu": 0.1288667397733073,
          "rouge_l": 0.19999999999999998,
          "rouge_1": 0.19999999999999998,
          "rouge_2": 0.10526315789473685,
          "bert_score": 0.8342295289039612
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "as  Gam Gam Gam Gammacher",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7814037799835205
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "restaurants fizic --------- -- - - - - - - -",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7586444616317749
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "# Queenston,",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7892193794250488
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.061204528854552576,
      "exact_match": 0.0,
      "bleu": 0.026064975785847426,
      "rouge_l": 0.05603069732721593,
      "rouge_1": 0.05603069732721593,
      "rouge_2": 0.02159882880277617,
      "bert_score": 0.7438383815543992,
      "empty_ratio": 0.07142857142857142
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8204331994056702,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8198221325874329,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8372652530670166,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7816333770751953,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.4133015275001526,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8086045980453491,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7701963186264038,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.6666666666666666,
        "bert_score": 0.8495611548423767,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7885189056396484,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.058823529411764705,
        "exact_match": 0.0,
        "bleu": 0.013174903070233042,
        "rouge_l": 0.05555555555555556,
        "bert_score": 0.7719983458518982,
        "count": 4
      },
      "translation": {
        "token_f1": 0.034567901234567905,
        "exact_match": 0.0,
        "bleu": 0.01193633952254642,
        "rouge_l": 0.02047930283224401,
        "bert_score": 0.7944267325931125,
        "count": 9
      },
      "squad": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7810623049736023,
        "count": 2
      },
      "classification": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7921407520771027,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.125,
        "exact_match": 0.0,
        "bleu": 0.034482758620689655,
        "rouge_l": 0.06060606060606061,
        "bert_score": 0.8346070051193237,
        "count": 2
      },
      "completion": {
        "token_f1": 0.16666666666666669,
        "exact_match": 0.0,
        "bleu": 0.041666666666666664,
        "rouge_l": 0.07692307692307693,
        "bert_score": 0.8275449573993683,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.4666666666666667,
        "exact_match": 0.0,
        "bleu": 0.25,
        "rouge_l": 0.4242424242424242,
        "bert_score": 0.8816663026809692,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.33333333333333337,
        "exact_match": 0.0,
        "bleu": 0.11363636363636363,
        "rouge_l": 0.17857142857142858,
        "bert_score": 0.8269742727279663,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.764675498008728,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.03333333333333333,
        "exact_match": 0.0,
        "bleu": 0.018393972058572117,
        "rouge_l": 0.03571428571428572,
        "bert_score": 0.7827620208263397,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.741860032081604,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.0625,
        "exact_match": 0.0,
        "bleu": 0.04,
        "rouge_l": 0.08163265306122448,
        "bert_score": 0.8066005110740662,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7302826046943665,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.37499999999999994,
        "exact_match": 0.0,
        "bleu": 0.21428571428571427,
        "rouge_l": 0.23076923076923075,
        "bert_score": 0.8347648978233337,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7842333316802979,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7672895789146423,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.1935483870967742,
        "exact_match": 0.0,
        "bleu": 0.1288667397733073,
        "rouge_l": 0.19999999999999998,
        "bert_score": 0.8342295289039612,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7814037799835205,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7586444616317749,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7892193794250488,
        "count": 1
      }
    }
  },
  {
    "model_name": "yellow-run-flan-t5-base-sci-cp-en-90k-steps-lr0001-clip1",
    "run_type": "yellow",
    "steps": 90000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": ": The table was put on the table: : : The table was put on the s: s: s:",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": ": the table was put on the table: : : the table was put on the s: s: s:",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7856840491294861
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": ":  was by John put the the on   was by John put the on :   was by John put the on  was",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": ":  was by john put the the on   was by john put the on :   was by john put the on  was",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7678669095039368
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": ", I absolutely love this movie!by, this movie is absolutely fantastic!, this movie is absolutely fantastic! This movie is absolutely fantastic! This movie is absolutely fantastic!",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": ", i absolutely love this movie!by, this movie is absolutely fantastic!, this movie is absolutely fantastic! this movie is absolutely fantastic! this movie is absolutely fantastic!",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7919571399688721
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": ". The film is boring and boring. This film is boring and boring. This film is boring and boring. This film is boring and boring. This film is boring and boring. This film is",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": ". the film is boring and boring. this film is boring and boring. this film is boring and boring. this film is boring and boring. this film is boring and boring. this film is",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7992605566978455
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "mrpc: : mrpc: mrpc: mrpc: mrpc: mrpc",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7665854096412659
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "stsb",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.850100576877594
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "sb sentence2: I love reading books. sentence1: I love reading books. sentence2: I love reading books. sentence1: I love reading books. sentence2: I love",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8228076100349426
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": ". hypothesis: Penguins can fly. hypothesis: All birds can fly. hypothesis: All birds can fly can fly can fly can fly can fly can fly. hypothesis:",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": ". hypothesis: penguins can fly. hypothesis: all birds can fly. hypothesis: all birds can fly can fly can fly can fly can fly can fly. hypothesis:",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8268428444862366
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "mnli premise: Today. hypothesis: You can borrow books today. hypothesis: You can borrow books today. hypothesis: You can borrow books today. premise: Today.",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": "mnli premise: today. hypothesis: you can borrow books today. hypothesis: you can borrow books today. hypothesis: you can borrow books today. premise: today.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7891001105308533
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "te",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8532163500785828
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "summarize: : : : : : : : : : : : : : . .. : - - = -: :",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7264572978019714
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": ".",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7789787650108337
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "::::::::::::::::::::::::",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7240346074104309
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "ny: I am a student at the university.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.13333333333333333,
          "exact_match": 0.0,
          "bleu": 0.125,
          "rouge_l": 0.125,
          "rouge_1": 0.125,
          "rouge_2": 0.0,
          "bert_score": 0.8467832803726196
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "s:: : : : The library is closed on Sundays: : : : : : : : :",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7721419930458069
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "::::: Good morning, how are you? Good morning? have you? good morning? Good afternoon, how about you?Good morning, what are you doing??good morning??Good",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7729623913764954
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "German: I am Student, Student, und Student.:: : :: : : : : : : ich am Student:",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.28571428571428575,
          "exact_match": 0.0,
          "bleu": 0.1111111111111111,
          "rouge_l": 0.4285714285714285,
          "rouge_1": 0.4285714285714285,
          "rouge_2": 0.16666666666666666,
          "bert_score": 0.8400079607963562
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "s: in French: French to English: to French: French: in French in: in English: in French in French in French in English in French in French",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7655494809150696
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": ": context: context: context: context: context: context: context: context: context: context: context: context: context: context: context: context: context: context: context:",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7537128925323486
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": ": context: context: context: context: context: context: context: context: context: context: context: context: context: context: context: context: context: context: context:",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7485693097114563
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "I absolutely loved this movie. I love this movie. I love",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "i absolutely loved this movie. i love this movie. i love",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8034428954124451
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "and immediately. This. This is. This is. This",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "and immediately. this. this is. this is. this",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8335186243057251
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": ": The table was put on the table: : : The table was put on the s: s: s:",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": ": the table was put on the table: : : the table was put on the s: s: s:",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7856840491294861
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": ": John on: John on s.,.,.,.,.,.,.,.,.,.,.,.,.",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": ": john on: john on s.,.,.,.,.,.,.,.,.,.,.,.,.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7892239093780518
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": ".",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7864752411842346
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "::::::::::::::::::::::::",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7240346074104309
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "German: I am Student, Student, und Student.:: : :: : : : : : : ich am Student:",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.28571428571428575,
          "exact_match": 0.0,
          "bleu": 0.1111111111111111,
          "rouge_l": 0.4285714285714285,
          "rouge_1": 0.4285714285714285,
          "rouge_2": 0.16666666666666666,
          "bert_score": 0.8400079607963562
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "s:: : : : The library is closed on Sundays: : : : : : : : :",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7721419930458069
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "paraphrase: The scientist conducted an important experiment. the scientist conducted a significant experiment. The scientist scientist conducted the important experiment: The: The: The: The: The experiment: The researcher conducted",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.4444444444444444,
          "exact_match": 0.0,
          "bleu": 0.13793103448275862,
          "rouge_l": 0.17142857142857143,
          "rouge_1": 0.2285714285714286,
          "rouge_2": 0.12121212121212122,
          "bert_score": 0.8370299339294434
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": ":",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8070512413978577
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": ": : : : :",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7859500646591187
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "s: Photosynthesis: Photosynthesis: Photosynthesis: Photosynthesis: Photo",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7904062867164612
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": ". the store tomorrow. I will have to do the store tomorrow.",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.5,
          "exact_match": 0.0,
          "bleu": 0.3333333333333333,
          "rouge_l": 0.5555555555555556,
          "rouge_1": 0.5555555555555556,
          "rouge_2": 0.25,
          "bert_score": 0.9054338335990906
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": ". Paris is the capital of France. Paris is the capital of France. Paris is the capital of France. Paris is the capital of",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6153846153846153,
          "exact_match": 0.0,
          "bleu": 0.16666666666666666,
          "rouge_l": 0.3448275862068966,
          "rouge_1": 0.3448275862068966,
          "rouge_2": 0.2962962962962963,
          "bert_score": 0.8773249387741089
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "s. Plants. Plants. Plants. Plants. Plants. Plants. Plants. Plants. Plant",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.13333333333333333,
          "rouge_1": 0.13333333333333333,
          "rouge_2": 0.0,
          "bert_score": 0.85013747215271
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": ": : : : : : : : : : : : : : : : : : :",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7288556694984436
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "r",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8621853590011597
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": ".",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7830473184585571
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": ": The experiment showed significant results.:::: The experiment showed significant results.: The experiment showed significant results.: The experiment showed significant results. The experiment",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.15384615384615385,
          "exact_match": 0.0,
          "bleu": 0.043478260869565216,
          "rouge_l": 0.07407407407407407,
          "rouge_1": 0.07407407407407407,
          "rouge_2": 0.0,
          "bert_score": 0.7942472100257874
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": ": CRISPR gene editing works.: CRISPR gene editing works.: CRISPR gene editing works... Gene editing works.. Gene editing works",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.06451612903225806,
          "exact_match": 0.0,
          "bleu": 0.04045371192598609,
          "rouge_l": 0.09523809523809525,
          "rouge_1": 0.09523809523809525,
          "rouge_2": 0.0,
          "bert_score": 0.8258775472640991
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "summary:",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8018168807029724
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": ": Describe steps to measure protein concentration using Bradford assay: Describe steps to measure protein concentration using Bradford assay: Describe steps to measure protein concentration using Bradford assay:",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.37499999999999994,
          "exact_match": 0.0,
          "bleu": 0.21428571428571427,
          "rouge_l": 0.23529411764705882,
          "rouge_1": 0.23529411764705882,
          "rouge_2": 0.04081632653061224,
          "bert_score": 0.8314998149871826
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": ": What does genotype   : g :  :  :  :  :  :",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.06666666666666667,
          "exact_match": 0.0,
          "bleu": 0.020210089552970725,
          "rouge_l": 0.0606060606060606,
          "rouge_1": 0.0606060606060606,
          "rouge_2": 0.0,
          "bert_score": 0.7675148844718933
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": ": Identify: Identify: Describe: Describe: Describe: Describe: Describe: Describe: Describe: Describe: Describe: Describe: Describe",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7608836889266968
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.0522253556095722,
      "exact_match": 0.0,
      "bleu": 0.02327823273820031,
      "rouge_l": 0.04736607591486613,
      "rouge_1": 0.04838648407813143,
      "rouge_2": 0.018601037095935056,
      "bert_score": 0.6272573385919843,
      "empty_ratio": 0.21428571428571427
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7767754793167114,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7956088483333588,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.38329270482063293,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8364540934562683,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8079714775085449,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8532163500785828,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.57297782599926,
        "count": 4
      },
      "translation": {
        "token_f1": 0.07830687830687831,
        "exact_match": 0.0,
        "bleu": 0.038580246913580245,
        "rouge_l": 0.10912698412698411,
        "bert_score": 0.7841849194632636,
        "count": 9
      },
      "squad": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7511411011219025,
        "count": 2
      },
      "classification": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8184807598590851,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7874539792537689,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.2222222222222222,
        "exact_match": 0.0,
        "bleu": 0.06896551724137931,
        "rouge_l": 0.08571428571428572,
        "bert_score": 0.8220405876636505,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7881781756877899,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.25,
        "exact_match": 0.0,
        "bleu": 0.16666666666666666,
        "rouge_l": 0.2777777777777778,
        "bert_score": 0.4527169167995453,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.30769230769230765,
        "exact_match": 0.0,
        "bleu": 0.08333333333333333,
        "rouge_l": 0.23908045977011494,
        "bert_score": 0.8637312054634094,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7288556694984436,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8621853590011597,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7924320995807648,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.15384615384615385,
        "exact_match": 0.0,
        "bleu": 0.043478260869565216,
        "rouge_l": 0.07407407407407407,
        "bert_score": 0.7942472100257874,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.06451612903225806,
        "exact_match": 0.0,
        "bleu": 0.04045371192598609,
        "rouge_l": 0.09523809523809525,
        "bert_score": 0.8258775472640991,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.37499999999999994,
        "exact_match": 0.0,
        "bleu": 0.21428571428571427,
        "rouge_l": 0.23529411764705882,
        "bert_score": 0.8314998149871826,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.06666666666666667,
        "exact_match": 0.0,
        "bleu": 0.020210089552970725,
        "rouge_l": 0.0606060606060606,
        "bert_score": 0.7675148844718933,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7608836889266968,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      }
    }
  },
  {
    "model_name": "red-run-t5-base-sci-cp-en-600k-steps-lr00001-clip0_5",
    "run_type": "red",
    "steps": 600000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "table. table. table. cola: table table table. table. cola: The table table. cola: The table table. cola: The table table. cola:",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": "table. table. table. cola: table table table. table. cola: the table table. cola: the table table. cola: the table table. cola:",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8077130913734436
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7495517134666443
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7739610075950623
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "sentence1: sentence1: sentence1: sentence1: sentence1: sentence1: sentence1: sentence1: sentence1: sentence1: sentence1: sentence1: sentence1: sentence",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7854624390602112
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "question2: qqqp question2: how to prepare spaghetti? question2: how to prepare spaghetti? question2: how to prepare spaghetti? question2: how to prepare spaghetti",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.805120587348938
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "quesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesques",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7395762801170349
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "sentence2 sentence2 sentence2 sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7716935873031616
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7658263444900513
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "the turn climate help 180 Warm Introduction advance global major 1. 1 ( progress standard \" each responsible",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.06666666666666667,
          "exact_match": 0.0,
          "bleu": 0.05555555555555555,
          "rouge_l": 0.07142857142857144,
          "rouge_1": 0.07142857142857144,
          "rouge_2": 0.0,
          "bert_score": 0.7969709634780884
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "the weather is beautiful today. The weather is lovely today.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8248533606529236
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "a student student at a university. I am trying to find a solution to the problem I am studying at the university.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.1,
          "exact_match": 0.0,
          "bleu": 0.045454545454545456,
          "rouge_l": 0.06666666666666667,
          "rouge_1": 0.06666666666666667,
          "rouge_2": 0.0,
          "bert_score": 0.8362906575202942
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "can on on Sundays on Sundayn Sundays. Sundays, Sundays to Sundays are Sundays can Sundays Sundays , on Sunday s on Mondays  on Sunday Sundays and Sundayson Sunday",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7428052425384521
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": ": How morning: The French: Good morning: French morning:???? How morning, how do you? The phrase phrase: A tip tip: The phrase: French: The tip tip tip. French morning,",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7709490060806274
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": ". , , . . . . . . . . . . . . .",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7478331327438354
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "Bonjour Bonjour: Bonjour: Comment Bonjour Bonjour, comment commenter les mots: Bonjour, Comment Bonjour, les mots : Comment Comment Bonjour pour comment Comment Bonjour les mots Bonjour Bonjour Bonjour Comment Bonjour Comment Comment Comment les mots par Comment Bonjour",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7628881335258484
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "product product product product product product product product product product product product product product product product",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7845262885093689
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "table. table. table. cola: table table table. table. cola: The table table. cola: The table table. cola: The table table. cola:",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": "table. table. table. cola: table table table. table. cola: the table table. cola: the table table. cola: the table table. cola:",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8077130913734436
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "table table table table table table table table table table the table table table table table table table table table table table table table table table table table table table table table table table table table table table table table",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7554914951324463
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "Climate change is meeting activities activities Meeting activities are meeting activities on the meeting meeting activities in activities activities activities on this category of activities activities, office Scientist Organis Change Accel Klima It This 2020",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.18181818181818185,
          "exact_match": 0.0,
          "bleu": 0.08571428571428572,
          "rouge_l": 0.12244897959183673,
          "rouge_1": 0.12244897959183673,
          "rouge_2": 0.0425531914893617,
          "bert_score": 0.7984378933906555
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "the weather is beautiful today. The weather is lovely today.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8248533606529236
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": ". , , . . . . . . . . . . . . .",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7478331327438354
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "can on on Sundays on Sundayn Sundays. Sundays, Sundays to Sundays are Sundays can Sundays Sundays , on Sunday s on Mondays  on Sunday Sundays and Sundayson Sunday",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7428052425384521
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "scientists conducted conducted an important important experiment conducted a scientist conducted . Paraphrase: The scientist conducted conducted the scientist conducted the important research conducted conducted a important paraphrase paraphrase: Paraphrase paraphrased: The scientists conducted",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.21052631578947367,
          "exact_match": 0.0,
          "bleu": 0.058823529411764705,
          "rouge_l": 0.15384615384615385,
          "rouge_1": 0.15384615384615385,
          "rouge_2": 0.0,
          "bert_score": 0.8166669607162476
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "Machine learning algorithms can analyze Machine learning machines can analyze large datasets can analyze machine learning algorithms analyze large algorithms can . Machine learning Machine learning algorithm can analyze Large algorithms can can analyze machines can analyzing Machine learning can analyze algorithms can. Machine",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8148109316825867
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "complete: complete: complete: complete: complete: complete: complete: complete:",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7804483771324158
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "complete complete complete complete complete complete complete complete complete complete complete complete complete complete complete complete",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7275411486625671
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "pizza very much. She don't like pizza very much. She don'",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6153846153846153,
          "exact_match": 0.0,
          "bleu": 0.36363636363636365,
          "rouge_l": 0.631578947368421,
          "rouge_1": 0.631578947368421,
          "rouge_2": 0.4705882352941177,
          "bert_score": 0.9136773943901062
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "Paris is the capital of Paris. Paris is the capital of Paris. Paris is the capital of Paris. Paris is the capital of Paris.",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6666666666666666,
          "exact_match": 0.0,
          "bleu": 0.16666666666666666,
          "rouge_l": 0.26666666666666666,
          "rouge_1": 0.26666666666666666,
          "rouge_2": 0.21428571428571427,
          "bert_score": 0.8736374974250793
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "planta plant use photosynthesis plant use photosynthesis planta plant use photosynthesis plant use photosynthesis plant use photosynthesis planta plant",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.08333333333333333,
          "rouge_1": 0.08333333333333333,
          "rouge_2": 0.0,
          "bert_score": 0.8206729292869568
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "sentence2: sentence2: sentence2: sentence2: sentence2: A sentence2: A sentence2: A sentence2: A cat is sleeping sentence2: A sentence sentence2: A",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7983334064483643
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "gene explain: CRISPR explain: explain explain: explain explain: explain: explain: explain: explain: explain: explain: explain: explain: explain: explain: explain: explain: explain:",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.07407407407407408,
          "exact_match": 0.0,
          "bleu": 0.041279899988103885,
          "rouge_l": 0.044444444444444446,
          "rouge_1": 0.08888888888888889,
          "rouge_2": 0.0,
          "bert_score": 0.7718602418899536
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "entanglement to process quantum computers that can exist in quantum computers entaillement -number of quantum computers to perform quantum computers can exist as quantum computers in a quantum computers' enanumnum computers that exist in numnum computer a enum",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3,
          "exact_match": 0.0,
          "bleu": 0.15789473684210525,
          "rouge_l": 0.20689655172413793,
          "rouge_1": 0.27586206896551724,
          "rouge_2": 0.07142857142857144,
          "bert_score": 0.814929723739624
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "The enzyme activity at pH 8 compared to pH 8 Compared to pH 8 compared to pH 8 . What does this suggest about the enzyme's optimal conditions? ? ?",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3636363636363637,
          "exact_match": 0.0,
          "bleu": 0.26666666666666666,
          "rouge_l": 0.2592592592592593,
          "rouge_1": 0.4444444444444445,
          "rouge_2": 0.11538461538461538,
          "bert_score": 0.8725059628486633
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "ocean temperatures rise by 2\u00b0C, ocean temperatures rise by ocean temperatures rise ocean temperatures predict: : ocean temperatures rise by ocean temperatures predict: : Ocean temperatures predict: Ocean temperatures predict:",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.14285714285714285,
          "exact_match": 0.0,
          "bleu": 0.06666666666666667,
          "rouge_l": 0.18867924528301888,
          "rouge_1": 0.18867924528301888,
          "rouge_2": 0.1568627450980392,
          "bert_score": 0.8442224860191345
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.048600536194521156,
      "exact_match": 0.0,
      "bleu": 0.023363552082191503,
      "rouge_l": 0.03741515749308054,
      "rouge_1": 0.0427472136872057,
      "rouge_2": 0.019126840588936066,
      "bert_score": 0.4820082698549543,
      "empty_ratio": 0.39285714285714285
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.778632402420044,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7797117233276367,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7723484337329865,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.3858467936515808,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7658263444900513,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.06212121212121213,
        "exact_match": 0.0,
        "bleu": 0.03531746031746032,
        "rouge_l": 0.04846938775510204,
        "bert_score": 0.398852214217186,
        "count": 4
      },
      "translation": {
        "token_f1": 0.011111111111111112,
        "exact_match": 0.0,
        "bleu": 0.005050505050505051,
        "rouge_l": 0.007407407407407408,
        "bert_score": 0.7779012521107992,
        "count": 9
      },
      "squad": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "classification": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.39226314425468445,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.781602293252945,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.10526315789473684,
        "exact_match": 0.0,
        "bleu": 0.029411764705882353,
        "rouge_l": 0.07692307692307693,
        "bert_score": 0.8157389461994171,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7539947628974915,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.30769230769230765,
        "exact_match": 0.0,
        "bleu": 0.18181818181818182,
        "rouge_l": 0.3157894736842105,
        "bert_score": 0.4568386971950531,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.3333333333333333,
        "exact_match": 0.0,
        "bleu": 0.08333333333333333,
        "rouge_l": 0.175,
        "bert_score": 0.8471552133560181,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7983334064483643,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.15,
        "exact_match": 0.0,
        "bleu": 0.07894736842105263,
        "rouge_l": 0.10344827586206896,
        "bert_score": 0.407464861869812,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.07407407407407408,
        "exact_match": 0.0,
        "bleu": 0.041279899988103885,
        "rouge_l": 0.044444444444444446,
        "bert_score": 0.7718602418899536,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.3636363636363637,
        "exact_match": 0.0,
        "bleu": 0.26666666666666666,
        "rouge_l": 0.2592592592592593,
        "bert_score": 0.8725059628486633,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.14285714285714285,
        "exact_match": 0.0,
        "bleu": 0.06666666666666667,
        "rouge_l": 0.18867924528301888,
        "bert_score": 0.8442224860191345,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      }
    }
  },
  {
    "model_name": "red-run-t5-base-sci-cp-en-500k-steps-lr00001-clip0_5",
    "run_type": "red",
    "steps": 500000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "table. cola: cola: cola: cola: cola: cola: cola: cola: cola: cola: cola: cola: cola",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7779468297958374
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7495517134666443
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": ". sentence1: A feline is resting on the couch. sentence2: A feline is resting on the couch. sentence3: A cat is sleeping on the couch. sentence",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": ". sentence1: a feline is resting on the couch. sentence2: a feline is resting on the couch. sentence3: a cat is sleeping on the couch. sentence",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8348197937011719
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.732715368270874
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "question2? question2: how question2? how question2: how to prepare spaghetti? question2: How to prepare spaghetti? question2: how to prepare question2: how to prepare question",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "question2? question2: how question2? how question2: how to prepare spaghetti? question2: how to prepare spaghetti? question2: how to prepare question2: how to prepare question",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7617461085319519
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "dimineata dimineata dimineata dimineata premisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremisepremise",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.6901555061340332
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "Scientist discovered Scientists Scientist discovered Scientists Scientist discovered Scientists Scientist discovered",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": "scientist discovered scientists scientist discovered scientists scientist discovered scientists scientist discovered",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7725896835327148
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "sentencennli nlli sentence1: Thennli sentence1: the nlli sentence1: the nlli sentence1: the",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": "sentencennli nlli sentence1: thennli sentence1: the nlli sentence1: the nlli sentence1: the",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7682387828826904
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "climat changes in global temperatures and weather patterns and weather trends in climate change since 1800s or climat-climate since the main cause of climate change or climate-change since the 1800s -term temperatures and temperatures or weather patterns or weather trends and weather conditions and weather temperatures and",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.11764705882352942,
          "exact_match": 0.0,
          "bleu": 0.0425531914893617,
          "rouge_l": 0.13114754098360654,
          "rouge_1": 0.13114754098360654,
          "rouge_2": 0.03389830508474576,
          "bert_score": 0.81325763463974
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "the weather is beautiful today. The weather is lovely today.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8248533606529236
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": ".--I am a student at the university. . --Joint students at the University.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.11764705882352941,
          "exact_match": 0.0,
          "bleu": 0.07692307692307693,
          "rouge_l": 0.1,
          "rouge_1": 0.1,
          "rouge_2": 0.0,
          "bert_score": 0.8423362374305725
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "library is closed on Sundays. Sundays, Sundays and Sundays .",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7942669987678528
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": ". Good morning, how can you get up early, how are you able to find a job? Good morning can you find out how to get started. .Good morning, if you can get a",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7782485485076904
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "\"Ich bin bin etliches Wissen und Wissenswertes zu versichern. Ich bin bin ein Profi, senzati senzati senzati gradini gradinite gradinitelle miron gradini",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7783217430114746
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "Bonjour Bonjour, comment \u00e7a va va va! Bonjour, Comment \u00e7a va : Bonjour, va va? Comment commenter \u00e7a vava va va ? Bonjour, par comment comment comment va va \u00e7a va? comment comment :",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7714734077453613
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "table. cola: cola: cola: cola: cola: cola: cola: cola: cola: cola: cola: cola: cola",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7779468297958374
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "table the table the table the table the table the table the table the table the table the table the table the table the table the table the table the table the table the table the table the table the",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7496427297592163
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "climate change is a natural activities are activities have been the main driver since the main activities in activities have a main driver for the main driving activities in the main drivers' activities in human activities on activities activities have the main activity in activities activities on human activities activities -the main driver",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2941176470588235,
          "exact_match": 0.0,
          "bleu": 0.09433962264150944,
          "rouge_l": 0.11940298507462686,
          "rouge_1": 0.1492537313432836,
          "rouge_2": 0.06153846153846154,
          "bert_score": 0.8174403309822083
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "the weather is beautiful today. The weather is lovely today.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8248533606529236
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "\"Ich bin bin etliches Wissen und Wissenswertes zu versichern. Ich bin bin ein Profi, senzati senzati senzati gradini gradinite gradinitelle miron gradini",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7783217430114746
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "library is closed on Sundays. Sundays, Sundays and Sundays .",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7942669987678528
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": ". paraphrase: Paraphrase: The scientist conducted an important experiment. Paraphrased: The paraphrasephrase: paraphrases: The scientists conducted a important paraphrase : phraphrase: the scientist conducted",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.18181818181818182,
          "exact_match": 0.0,
          "bleu": 0.08333333333333333,
          "rouge_l": 0.14285714285714288,
          "rouge_1": 0.14285714285714288,
          "rouge_2": 0.0,
          "bert_score": 0.8091378808021545
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "Machine learning algorithms can analyze machine learning algorithms algorithms can machine learning can analyze large datasets can analyze Large algorithms can Ana Machine learning learning algorithms machine can analyze Machine learning models can analyze machines can analyze. Machine learning algorithm can analyze algorithms can Machine learning",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8106563091278076
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "photosynthesis is the process plant photosynthesis is the plants complete: Photosynthesis is",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.816651463508606
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "store tomorrow. grammar: grammar: grammar: grammar: grammar: grammar: grammar",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.18181818181818182,
          "exact_match": 0.0,
          "bleu": 0.1111111111111111,
          "rouge_l": 0.25,
          "rouge_1": 0.25,
          "rouge_2": 0.14285714285714288,
          "bert_score": 0.8328160047531128
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "grammar: Grammar: Grammar: Grammar: Grammar: Grammar: Grammar: Grammar:",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7831252813339233
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "Paris is the capital of Paris is the capital of Paris is the capital of Paris is the capital of Paris is the capital of Paris is the",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.7272727272727272,
          "exact_match": 0.0,
          "bleu": 0.14285714285714285,
          "rouge_l": 0.23529411764705882,
          "rouge_1": 0.23529411764705882,
          "rouge_2": 0.18750000000000003,
          "bert_score": 0.8626794219017029
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "Plants use photosynthesis use photosynthesis plant use photosynthesis plant use photosynthesis plant use photosynthesis plant use photosynthesis plant use photosynthesis",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.22222222222222224,
          "exact_match": 0.0,
          "bleu": 0.05,
          "rouge_l": 0.08000000000000002,
          "rouge_1": 0.08000000000000002,
          "rouge_2": 0.0,
          "bert_score": 0.8200397491455078
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7982213497161865
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "gene editing explain: how explain: CRISPR gene explain: how explain: CRISPR gene explain: how explain: CRISPR gene explain: how explain: CRISPR gene explain:",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.07142857142857142,
          "exact_match": 0.0,
          "bleu": 0.041628407307320904,
          "rouge_l": 0.0851063829787234,
          "rouge_1": 0.0851063829787234,
          "rouge_2": 0.0,
          "bert_score": 0.8063730001449585
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "quantum mechanical phenomena that entanglement in quantum computers that can exist in numnum computers that exist in quantum mechanical concepts that exist as quantum computers -numerical computers that enumnum -functions quantum computers ( quantum computers) can exist as a quantum computers",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.15789473684210525,
          "exact_match": 0.0,
          "bleu": 0.07317073170731707,
          "rouge_l": 0.1,
          "rouge_1": 0.13333333333333333,
          "rouge_2": 0.03448275862068966,
          "bert_score": 0.8104811310768127
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "Describe steps to concentration concentration steps to concentration concentration steps to concentration concentration steps to do steps to steps to steps to concentration concentration steps to steps Describe steps Describe steps Describe steps Describe steps",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.14814814814814814,
          "exact_match": 0.0,
          "bleu": 0.05714285714285714,
          "rouge_l": 0.06779661016949153,
          "rouge_1": 0.06779661016949153,
          "rouge_2": 0.0,
          "bert_score": 0.7857135534286499
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "compared to pH 6. analyzed: . What does this suggest about the enzyme activity? . . . . . . . .",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.25,
          "exact_match": 0.0,
          "bleu": 0.189489299562541,
          "rouge_l": 0.2564102564102564,
          "rouge_1": 0.3589743589743589,
          "rouge_2": 0.10810810810810811,
          "bert_score": 0.842176616191864
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.044107402397428934,
      "exact_match": 0.0,
      "bleu": 0.0171883709656352,
      "rouge_l": 0.02800026850215905,
      "rouge_1": 0.030960057469410694,
      "rouge_2": 0.010149728146591928,
      "bert_score": 0.4680547405566488,
      "empty_ratio": 0.4107142857142857
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7637492716312408,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.783767580986023,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.38087305426597595,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.3450777530670166,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7725896835327148,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7682387828826904,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.10294117647058823,
        "exact_match": 0.0,
        "bleu": 0.03422320353271779,
        "rouge_l": 0.06263763151455835,
        "bert_score": 0.40767449140548706,
        "count": 4
      },
      "translation": {
        "token_f1": 0.013071895424836602,
        "exact_match": 0.0,
        "bleu": 0.008547008547008548,
        "rouge_l": 0.011111111111111112,
        "bert_score": 0.7985491553942362,
        "count": 9
      },
      "squad": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "classification": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7637947797775269,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.09090909090909091,
        "exact_match": 0.0,
        "bleu": 0.041666666666666664,
        "rouge_l": 0.07142857142857144,
        "bert_score": 0.8098970949649811,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.408325731754303,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.09090909090909091,
        "exact_match": 0.0,
        "bleu": 0.05555555555555555,
        "rouge_l": 0.125,
        "bert_score": 0.8079706430435181,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.4747474747474747,
        "exact_match": 0.0,
        "bleu": 0.09642857142857142,
        "rouge_l": 0.15764705882352942,
        "bert_score": 0.8413595855236053,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7982213497161865,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.07894736842105263,
        "exact_match": 0.0,
        "bleu": 0.036585365853658534,
        "rouge_l": 0.05,
        "bert_score": 0.40524056553840637,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.07142857142857142,
        "exact_match": 0.0,
        "bleu": 0.041628407307320904,
        "rouge_l": 0.0851063829787234,
        "bert_score": 0.8063730001449585,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.14814814814814814,
        "exact_match": 0.0,
        "bleu": 0.05714285714285714,
        "rouge_l": 0.06779661016949153,
        "bert_score": 0.7857135534286499,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.25,
        "exact_match": 0.0,
        "bleu": 0.189489299562541,
        "rouge_l": 0.2564102564102564,
        "bert_score": 0.842176616191864,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      }
    }
  },
  {
    "model_name": "green-run-t5-base-sci-cp-en-50k-steps-lr0001-clip1",
    "run_type": "green",
    "steps": 50000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "mrpc",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8431903719902039
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "sentence1: The dog is running in the park. stsb sentence2: A canine is jogging in",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8559248447418213
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "st",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8891782760620117
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": ":",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8010773062705994
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": ": The weather is beautiful today. It is beautiful",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8299315571784973
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": ": I am a student at the university",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.13333333333333333,
          "exact_match": 0.0,
          "bleu": 0.125,
          "rouge_l": 0.13333333333333333,
          "rouge_1": 0.13333333333333333,
          "rouge_2": 0.0,
          "bert_score": 0.8659972548484802
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "English to French: French: The library is closed on Sundays. Translate French to English: Translate English into French: French: This library is open on Saturdays.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7901413440704346
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "from English to French: Translated from English into French: Good morning, how are you? Translate French into French into English: Bonjour Bonjour Bonjour! Bonjour Bonjour, Bonjour Bonjour. Bonjour Bonjour \u00e0 l'heure Bonjour",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.08695652173913045,
          "exact_match": 0.0,
          "bleu": 0.03125,
          "rouge_l": 0.05555555555555555,
          "rouge_1": 0.05555555555555555,
          "rouge_2": 0.0,
          "bert_score": 0.7832625508308411
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": ": Ich bin ein Student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.25,
          "rouge_1": 0.25,
          "rouge_2": 0.0,
          "bert_score": 0.8905700445175171
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "from French to English: Bonjour Bonjour, comment \u00e7a va? Bonjour, Comment \u00e7a va! Bonjour Bonjour Bonjour! Comment \u00e7a \u00e7a va Bonjour Bonjour? Bonjour Bonjour Comment \u00e7a vous avez-vous d\u00e9j\u00e0? Bonjour! Bonjour, vous pouvez",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7672343850135803
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "Product: This product is terrible and broke immediately. Product: This product is",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "negative",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.999999463558197
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": ": The weather is beautiful today. It is beautiful",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8299315571784973
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": ": Ich bin ein Student.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.25,
          "rouge_1": 0.25,
          "rouge_2": 0.0,
          "bert_score": 0.8905700445175171
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "English to French: French: The library is closed on Sundays. Translate French to English: Translate English into French: French: This library is open on Saturdays.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7901413440704346
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "Paraphrase: The scientist conducted an important experiment. In the paraphrase: In the scientist conducted a significant experiment.",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.37499999999999994,
          "exact_match": 0.0,
          "bleu": 0.17647058823529413,
          "rouge_l": 0.2608695652173913,
          "rouge_1": 0.2608695652173913,
          "rouge_2": 0.09523809523809523,
          "bert_score": 0.8723636865615845
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "paraphrase: Machine learning algorithms can analyze large datasets. Paraphrase: Parphrase: In paraphrasing: In parphrase:",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8576269745826721
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "France is France is France is",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8244792222976685
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "Paris is the capital of France",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6666666666666666,
          "exact_match": 0.0,
          "bleu": 0.6666666666666666,
          "rouge_l": 0.8333333333333334,
          "rouge_1": 0.8333333333333334,
          "rouge_2": 0.8000000000000002,
          "bert_score": 0.9439532160758972
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": ":",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8072636723518372
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "sb",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8527038097381592
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "to",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7689762711524963
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati senzati",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7223576307296753
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "predict:  What do coral reefs?",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.07692307692307693,
          "exact_match": 0.0,
          "bleu": 0.005464744489458514,
          "rouge_l": 0.13793103448275862,
          "rouge_1": 0.13793103448275862,
          "rouge_2": 0.07407407407407408,
          "bert_score": 0.817489743232727
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.04176570711896799,
      "exact_match": 0.017857142857142856,
      "bleu": 0.03580092856056106,
      "rouge_l": 0.052161121820042365,
      "rouge_1": 0.052161121820042365,
      "rouge_2": 0.017309145880574455,
      "bert_score": 0.34454222449234556,
      "empty_ratio": 0.5892857142857143
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.42159518599510193,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8725515604019165,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8010773062705994,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 4
      },
      "translation": {
        "token_f1": 0.024476650563607087,
        "exact_match": 0.0,
        "bleu": 0.017361111111111112,
        "rouge_l": 0.07654320987654321,
        "bert_score": 0.8264200091362,
        "count": 9
      },
      "squad": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "classification": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.4999997317790985,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.18749999999999997,
        "exact_match": 0.0,
        "bleu": 0.08823529411764706,
        "rouge_l": 0.13043478260869565,
        "bert_score": 0.8649953305721283,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.41223961114883423,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.3333333333333333,
        "exact_match": 0.0,
        "bleu": 0.3333333333333333,
        "rouge_l": 0.4166666666666667,
        "bert_score": 0.8756084442138672,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8527038097381592,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7689762711524963,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7223576307296753,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.07692307692307693,
        "exact_match": 0.0,
        "bleu": 0.005464744489458514,
        "rouge_l": 0.13793103448275862,
        "bert_score": 0.817489743232727,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      }
    }
  },
  {
    "model_name": "green-run-t5-base-sci-cp-en-640k-steps-lr0001-clip1",
    "run_type": "green",
    "steps": 640000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "ture - cola: cola: cola: cola: cola: cola: cola: cola: cola:",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7850769758224487
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "cola: Book was by John put the on table. Starting",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": "cola: book was by john put the on table. starting",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8415201306343079
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "movie",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.9983752965927124
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "-  sentence1: Sfant",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": "-  sentence1: sfant",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.787452757358551
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "mrpc sentence1: Sfant de including according\u0163ine parte which",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": "mrpc sentence1: sfant de including according\u0163ine parte which",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7833828926086426
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "Part  Sfant Anchoring the",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "part  sfant anchoring the",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8311456441879272
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "Sunshine  .",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "sunshine  .",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8847207427024841
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "hypothesis: All birds can fly. mnli mnli mnli mnli mnli mnli m",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": "hypothesis: all birds can fly. mnli mnli mnli mnli mnli mnli m",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8013612031936646
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "library. library. library. library. library. library. library. library.",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8134397864341736
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "Outlet of: Scientists discovered a new species.",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": "outlet of: scientists discovered a new species.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8123125433921814
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "-  The -",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": "-  the -",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7510907649993896
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "Woman Jennywornup SP   rejected?ttes '   proaspat stripped-up' ''",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7860593199729919
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "packed along",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.792809009552002
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "weather is beautiful today. Translat Translation Translation Translation translate German: weather is the weather is warm today. German: The weather is Today. Translat Translat German: -The weather is pleasant today. Translation German: This weather",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7859938740730286
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "Dubai By including closer submarine submarinekraft Retrieved Bell'' ----------- himself",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7617173790931702
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "through through",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7736291885375977
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "translate English to French: Good morning, how are you? (Good morning, Good Morning, Good morning? Good morning. How are you today? ( Good Morning. Translation Good morning: Good Morning: Good Good Good morning",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7729983925819397
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Translat German to English translate German to German Ich bin ein Student Ich ich bin Student. Ich Ich bin Student, Student, Ich habe ich ich Ich Ich habe ein Student Student, ich habe",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.125,
          "exact_match": 0.0,
          "bleu": 0.030303030303030304,
          "rouge_l": 0.054054054054054064,
          "rouge_1": 0.054054054054054064,
          "rouge_2": 0.0,
          "bert_score": 0.8164122700691223
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "Bonjour, Bonjour Bonjour Bonjour, Bonjour Bonjour. Translation: Bonjour Bonjour? (translate French to English: Bonjour,  translate French to Translate French: Bonjour? Bonjour, translate: Bonjour. ( Translated to English to English",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7706729173660278
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "photosynthesis",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7849928140640259
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "incadr\u021bi",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7558565735816956
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "I loved every movie I loved every movie I loved every movie I loved every",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "i loved every movie i loved every movie i loved every movie i loved every",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7907233834266663
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "sentiment: sentiment: sentiment: sentiment: sentiment: sentiment: sentiment: sentiment",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.754317581653595
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "ture - cola: cola: cola: cola: cola: cola: cola: cola: cola:",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7850769758224487
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "The book by John White By",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": "the book by john white by",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8292741179466248
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": ". which is Femalegle farther farther SP farther toward \u00a3$ PM at\u015fi  ?orial closer",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7754970192909241
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": ". which Man-- Navy Klo Klo Klo Ladyunlesspton adica Meghan proaspat CBD Giurgiu Gran",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7646746635437012
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "weather is beautiful today. Translat Translation Translation Translation translate German: weather is the weather is warm today. German: The weather is Today. Translat Translat German: -The weather is pleasant today. Translation German: This weather",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7859938740730286
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Translat German to English translate German to German Ich bin ein Student Ich ich bin Student. Ich Ich bin Student, Student, Ich habe ich ich Ich Ich habe ein Student Student, ich habe",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.125,
          "exact_match": 0.0,
          "bleu": 0.030303030303030304,
          "rouge_l": 0.054054054054054064,
          "rouge_1": 0.054054054054054064,
          "rouge_2": 0.0,
          "bert_score": 0.8164122700691223
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "through through",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7736291885375977
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "The scientist conducted an important experiment. scientist conducted the scientist conducted a important experiment: , ,,, Scientist conducted . , The scientist , the scientist scientist conducted, ., -The scientist conducted",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2,
          "exact_match": 0.0,
          "bleu": 0.06451612903225806,
          "rouge_l": 0.12903225806451613,
          "rouge_1": 0.12903225806451613,
          "rouge_2": 0.0,
          "bert_score": 0.8284251093864441
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "-  Elizabeth Demographics Women By haus-",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7956035733222961
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "France is",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.9038552641868591
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "Photos Photossynthesis is the process by which Photosynthesis is the process by which Photo",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.792611300945282
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "I are going to the store tomorrow. I are going to the store tomorrow",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.7999999999999999,
          "exact_match": 0.0,
          "bleu": 0.42857142857142855,
          "rouge_l": 0.5714285714285714,
          "rouge_1": 0.5714285714285714,
          "rouge_2": 0.42105263157894735,
          "bert_score": 0.9345709085464478
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "Iulia: Pizza: Pizza: Pizza: Pizza: Pizza: Pizza: Pizza",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2222222222222222,
          "exact_match": 0.0,
          "bleu": 0.125,
          "rouge_l": 0.13333333333333333,
          "rouge_1": 0.13333333333333333,
          "rouge_2": 0.0,
          "bert_score": 0.8012646436691284
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "Question: Question: question: question: question: question: question: question: question: question: question: question: question: question:",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7566104531288147
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "question: question: question: question: question: question: question",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7706586122512817
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "sentence",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8944631218910217
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "wings. wings. wing wings. wing wings. wing wings. wing wings. wing wings. wing wings. wing wings. wing wings. wing",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7718183994293213
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "& localitate real[/ime-30-30-30ARYAnn adica outfitconfa produseSpreniischeidsummeAnnwer alongrophy",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7591477632522583
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "Translat English to German: Translat English to German: Translat English to German: experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.20000000000000004,
          "exact_match": 0.0,
          "bleu": 0.041666666666666664,
          "rouge_l": 0.06896551724137931,
          "rouge_1": 0.06896551724137931,
          "rouge_2": 0.0,
          "bert_score": 0.7448824048042297
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "Answer Explain: How does CRISPR gene: Explain: How does CRISPR: Explain: How does CRISPR explain: Explain: How does CRISPR: Explain: How does C",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.06451612903225806,
          "exact_match": 0.0,
          "bleu": 0.041628407307320904,
          "rouge_l": 0.0851063829787234,
          "rouge_1": 0.0851063829787234,
          "rouge_2": 0.0,
          "bert_score": 0.7895562648773193
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "The quantum-like nature of quantum bits (qubits).\u00e4chtigity White closer through whil 'stably",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.125,
          "exact_match": 0.0,
          "bleu": 0.08556951983876533,
          "rouge_l": 0.1764705882352941,
          "rouge_1": 0.1764705882352941,
          "rouge_2": 0.0,
          "bert_score": 0.8168232440948486
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "bacteria, bacteria, bacteria bacteria, bacteria bacteria bacteria bacteria bacteria bacteria bacteria bacteria bacteria bacteria bacteria bacteria bacteria bacteria bacteria bacteria bacteria bacteria bacteria bacteria bacteria bacteria",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.08695652173913042,
          "exact_match": 0.0,
          "bleu": 0.038461538461538464,
          "rouge_l": 0.04166666666666667,
          "rouge_1": 0.04166666666666667,
          "rouge_2": 0.0,
          "bert_score": 0.7636074423789978
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "Woman Klo man----- By Lady Sofia",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7963731288909912
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "- - means or natural- uns - -otroquit, i.",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8045294880867004
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "'What are the key findings?' Klo",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.14285714285714288,
          "exact_match": 0.0,
          "bleu": 0.023161150407600514,
          "rouge_l": 0.20689655172413793,
          "rouge_1": 0.20689655172413793,
          "rouge_2": 0.14814814814814814,
          "bert_score": 0.8399680256843567
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "- Sfant ie Sfant ie:v\u00e2r. and a. , , , ,",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0625,
          "exact_match": 0.0,
          "bleu": 0.017698791666500926,
          "rouge_l": 0.05405405405405405,
          "rouge_1": 0.05405405405405405,
          "rouge_2": 0.0,
          "bert_score": 0.7920483946800232
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "White  When",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7879221439361572
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "now Formulature",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7881937623023987
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.03846521456876346,
      "exact_match": 0.0,
      "bleu": 0.016551423081395355,
      "rouge_l": 0.02812610771133544,
      "rouge_1": 0.02812610771133544,
      "rouge_2": 0.010164299637983848,
      "bert_score": 0.7303491607308388,
      "empty_ratio": 0.08928571428571429
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8132985532283783,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.4991876482963562,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7854178249835968,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8579331934452057,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8074004948139191,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8123125433921814,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7510907649993896,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7797600030899048,
        "count": 4
      },
      "translation": {
        "token_f1": 0.027777777777777776,
        "exact_match": 0.0,
        "bleu": 0.006734006734006735,
        "rouge_l": 0.012012012012012014,
        "bert_score": 0.7841621504889594,
        "count": 9
      },
      "squad": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7704246938228607,
        "count": 2
      },
      "classification": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7725204825401306,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8071755468845367,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.1,
        "exact_match": 0.0,
        "bleu": 0.03225806451612903,
        "rouge_l": 0.06451612903225806,
        "bert_score": 0.8120143413543701,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8482332825660706,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.5111111111111111,
        "exact_match": 0.0,
        "bleu": 0.2767857142857143,
        "rouge_l": 0.35238095238095235,
        "bert_score": 0.8679177761077881,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7636345326900482,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8944631218910217,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7718183994293213,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.0625,
        "exact_match": 0.0,
        "bleu": 0.042784759919382666,
        "rouge_l": 0.08823529411764705,
        "bert_score": 0.7879855036735535,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.20000000000000004,
        "exact_match": 0.0,
        "bleu": 0.041666666666666664,
        "rouge_l": 0.06896551724137931,
        "bert_score": 0.7448824048042297,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.06451612903225806,
        "exact_match": 0.0,
        "bleu": 0.041628407307320904,
        "rouge_l": 0.0851063829787234,
        "bert_score": 0.7895562648773193,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.08695652173913042,
        "exact_match": 0.0,
        "bleu": 0.038461538461538464,
        "rouge_l": 0.04166666666666667,
        "bert_score": 0.7636074423789978,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7963731288909912,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8045294880867004,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.14285714285714288,
        "exact_match": 0.0,
        "bleu": 0.023161150407600514,
        "rouge_l": 0.20689655172413793,
        "bert_score": 0.8399680256843567,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0625,
        "exact_match": 0.0,
        "bleu": 0.017698791666500926,
        "rouge_l": 0.05405405405405405,
        "bert_score": 0.7920483946800232,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7879221439361572,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7881937623023987,
        "count": 1
      }
    }
  },
  {
    "model_name": "yellow-run-flan-t5-base-sci-cp-en-200k-steps-lr0001-clip1",
    "run_type": "yellow",
    "steps": 200000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": ".   .  .  .  . . . . . . . . . . .",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7389745116233826
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "a   a cola   a te ree  a te ree",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7968465089797974
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "s ares are absolutely fantastic! s ares are absolutely fantastic! s are absolutely fantastic! s are absolutely fantastic!s are absolutely fantastic!",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7732887268066406
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "is one of thes is one of the is one of the",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7786656022071838
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "rrrrrrrrrr",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7556369304656982
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "qqp qqp qqp qqp qqp qqp qqp",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7447489500045776
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "qqqp qqqp qqqp  qqqp qqqp qqq",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7633491158485413
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "stb stc stc stc stc stc stc stc stc st",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7926195859909058
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "i",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8589982986450195
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "e",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7985639572143555
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "to German: [URL] to to to to to Germany:: The Weather is beautiful.: The Weather Is beautiful. The The Climate is lovely. The The Climate I",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7864119410514832
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "a",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.786311149597168
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "French: French: closed on Sundays. The library is closed the library isle closes Sundays and:le closed Sundays library French: The library French is library is",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.058823529411764705,
          "rouge_1": 0.058823529411764705,
          "rouge_2": 0.0,
          "bert_score": 0.7883695960044861
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": ", how are you?, How is it???, how is this translated?????, how am I??, how??,, how's that??",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7678379416465759
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "ed. I was Student Student. I had to be Student Student, Student, and Student. len Student. Student Student: Ich was Studentn Studentn Student Student to English to English.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2,
          "exact_match": 0.0,
          "bleu": 0.07142857142857142,
          "rouge_l": 0.125,
          "rouge_1": 0.125,
          "rouge_2": 0.0,
          "bert_score": 0.8480373620986938
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "French into English. How do you translate French to English? How to translate a French to an English translation of French? How is it to be translated in English? And how to translate English to French? And to be English?",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.08,
          "exact_match": 0.0,
          "bleu": 0.025,
          "rouge_l": 0.09090909090909091,
          "rouge_1": 0.09090909090909091,
          "rouge_2": 0.0,
          "bert_score": 0.825678825378418
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "squad question: What is photosynthesis? context: What is photosynthesis squad question: What is photosynthesis? context: What is photosynthesis? context: What is photosynthesis? context:",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8103165626525879
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "squad squad squad question: Paris is the capital of France. It is located in northern France. It is located in northern France. context: Paris is the capital of France. It is located in",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.14285714285714288,
          "exact_match": 0.0,
          "bleu": 0.030303030303030304,
          "rouge_l": 0.05882352941176471,
          "rouge_1": 0.05882352941176471,
          "rouge_2": 0.0,
          "bert_score": 0.8104994893074036
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": ": is absolutely fantastic and I absolutely and I absolutely love it!",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": ": is absolutely fantastic and i absolutely and i absolutely love it!",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7997402548789978
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "to the end and.,.. the ...",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8146762847900391
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": ".   .  .  .  . . . . . . . . . . .",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7389745116233826
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": ": John Wiley & Sons, Ltd. : , , , , ,",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": ": john wiley & sons, ltd. : , , , , ,",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7876339554786682
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "to German: [URL] to to to to to Germany:: The Weather is beautiful.: The Weather Is beautiful. The The Climate is lovely. The The Climate I",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7864119410514832
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "ed. I was Student Student. I had to be Student Student, Student, and Student. len Student. Student Student: Ich was Studentn Studentn Student Student to English to English.",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2,
          "exact_match": 0.0,
          "bleu": 0.07142857142857142,
          "rouge_l": 0.125,
          "rouge_1": 0.125,
          "rouge_2": 0.0,
          "bert_score": 0.8480373620986938
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "French: French: closed on Sundays. The library is closed the library isle closes Sundays and:le closed Sundays library French: The library French is library is",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.058823529411764705,
          "rouge_1": 0.058823529411764705,
          "rouge_2": 0.0,
          "bert_score": 0.7883695960044861
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "sequencing: edited from This is the author's translation The author(s) unedited This is not to be faulted:thus: The",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.1,
          "exact_match": 0.0,
          "bleu": 0.05555555555555555,
          "rouge_l": 0.07407407407407407,
          "rouge_1": 0.07407407407407407,
          "rouge_2": 0.0,
          "bert_score": 0.8168257474899292
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "no human intervention.",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8668422102928162
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "illustration study abroad ing a complete and correct translation",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8155480027198792
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": ": the following : have a hard time",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8234329223632812
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "n. (n.s.s.): [URL] on",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8286605477333069
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "'t like pizza very much. 't like pizza very much",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6666666666666666,
          "exact_match": 0.0,
          "bleu": 0.4,
          "rouge_l": 0.588235294117647,
          "rouge_1": 0.588235294117647,
          "rouge_2": 0.5333333333333333,
          "bert_score": 0.9002031683921814
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "regardless, ofsignificance of this study.e.e.de. of this study",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.16666666666666666,
          "exact_match": 0.0,
          "bleu": 0.125,
          "rouge_l": 0.11764705882352942,
          "rouge_1": 0.11764705882352942,
          "rouge_2": 0.0,
          "bert_score": 0.8288351893424988
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": ": Plants: Plantss: Plants to Figure 1. Plants to Figure 1. Plants to Figure 1. Plant",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.15384615384615385,
          "exact_match": 0.0,
          "bleu": 0.0625,
          "rouge_l": 0.1,
          "rouge_1": 0.1,
          "rouge_2": 0.0,
          "bert_score": 0.8146803379058838
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": ".",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.815805196762085
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "to German: German to German to translate English to German: The experiment showed significant results. to to to to translate English to German to to to translate English",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.13333333333333333,
          "exact_match": 0.0,
          "bleu": 0.03571428571428571,
          "rouge_l": 0.06060606060606061,
          "rouge_1": 0.06060606060606061,
          "rouge_2": 0.0,
          "bert_score": 0.7835463881492615
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "r work? work? work? explain: How does CRISPR gene editing work? explain: How does CRISPR gene editing explain: How does CRISPR work?",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.06451612903225806,
          "exact_match": 0.0,
          "bleu": 0.04150457801283011,
          "rouge_l": 0.08695652173913043,
          "rouge_1": 0.08695652173913043,
          "rouge_2": 0.0,
          "bert_score": 0.823492705821991
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": ".  . . . . . . . . . . . . . .",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7281133532524109
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "r",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8045011162757874
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "reefs?sss?s, what impact might this have on coral reefs?'s's, what impact might this have on coral reefs",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.12903225806451613,
          "exact_match": 0.0,
          "bleu": 0.12399150399935213,
          "rouge_l": 0.17777777777777778,
          "rouge_1": 0.17777777777777778,
          "rouge_2": 0.09302325581395349,
          "bert_score": 0.8278376460075378
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.03637354197262031,
      "exact_match": 0.0,
      "bleu": 0.018614751722182084,
      "rouge_l": 0.030762079755046506,
      "rouge_1": 0.030762079755046506,
      "rouge_2": 0.011184939091915837,
      "bert_score": 0.5583450623920986,
      "empty_ratio": 0.30357142857142855
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.76791051030159,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7759771645069122,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.3778184652328491,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7540490329265594,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.3963097929954529,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.42949914932250977,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7985639572143555,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 4
      },
      "translation": {
        "token_f1": 0.05333333333333334,
        "exact_match": 0.0,
        "bleu": 0.01865079365079365,
        "rouge_l": 0.05095068330362448,
        "bert_score": 0.8028295238812765,
        "count": 9
      },
      "squad": {
        "token_f1": 0.07142857142857144,
        "exact_match": 0.0,
        "bleu": 0.015151515151515152,
        "rouge_l": 0.029411764705882356,
        "bert_score": 0.8104080259799957,
        "count": 2
      },
      "classification": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8072082698345184,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7633042335510254,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.05,
        "exact_match": 0.0,
        "bleu": 0.027777777777777776,
        "rouge_l": 0.037037037037037035,
        "bert_score": 0.8418339788913727,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8194904625415802,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.3333333333333333,
        "exact_match": 0.0,
        "bleu": 0.2,
        "rouge_l": 0.2941176470588235,
        "bert_score": 0.8644318580627441,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.16025641025641024,
        "exact_match": 0.0,
        "bleu": 0.09375,
        "rouge_l": 0.10882352941176471,
        "bert_score": 0.8217577636241913,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.815805196762085,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.13333333333333333,
        "exact_match": 0.0,
        "bleu": 0.03571428571428571,
        "rouge_l": 0.06060606060606061,
        "bert_score": 0.7835463881492615,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.06451612903225806,
        "exact_match": 0.0,
        "bleu": 0.04150457801283011,
        "rouge_l": 0.08695652173913043,
        "bert_score": 0.823492705821991,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7281133532524109,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8045011162757874,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.12903225806451613,
        "exact_match": 0.0,
        "bleu": 0.12399150399935213,
        "rouge_l": 0.17777777777777778,
        "bert_score": 0.8278376460075378,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      }
    }
  },
  {
    "model_name": "green-run-t5-base-sci-cp-en-250k-steps-lr0001-clip1",
    "run_type": "green",
    "steps": 250000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "endra\u00f6pfe\u00f6pfe prevazutgoutte",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.777925431728363
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "Suceavaionist cunoaste",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": "suceavaionist cunoaste",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7825254201889038
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "substanti imunitar prevazut imunitar",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7883760333061218
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "itude",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8511666655540466
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "Exista\u00f6\u00df sentence1: A feline is",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": "exista\u00f6\u00df sentence1: a feline is",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8098362684249878
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "substantia prevazut prevazut obtine prevazut obtine",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7789674997329712
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "culorisitude",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8402153849601746
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "imunitar imunitar imunitar",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8007059097290039
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "explanationCGcaroracarora",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8164025545120239
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "k\u00f6rperk\u00f6rpereille culori",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8049535155296326
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "k\u00f6rperuploads",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8151717185974121
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "substanti prevazut imunitar prevazut",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7854943871498108
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "prevazute:bl\u00e9 pahar",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7792763113975525
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "substanti prevazut prevazut",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7856801748275757
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "k\u00f6rperuploads",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7898539900779724
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "cablucea senzati cablu senzatigelangenpendingawaitingm\u00e2ntSpitalultroph cabluncyol\u00e9QA senzati**#### senzati senzati senzati",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7493622899055481
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "substanti span viteze cabluneuve[/Camere cablum\u00e2ntSpitalulmpl\u0103 prevazutmpl\u0103ztlichEinenEinenSpitalul",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.753013551235199
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "to German: Translate English to German Translate German to English: Translation from German to German. TranslateEnglish to German Translation translate English into German: The weather is beautiful today. German translation from German: Translation translate translate English to",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.778070330619812
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "translate English to German: I am a student at the university.kabelkabel licensed Bentley",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.1,
          "exact_match": 0.0,
          "bleu": 0.07692307692307693,
          "rouge_l": 0.09090909090909091,
          "rouge_1": 0.09090909090909091,
          "rouge_2": 0.0,
          "bert_score": 0.8212798833847046
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "substantiationmpl\u0103 prevazutumfangcaroracaroraseidME",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7488942742347717
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": ": Translate French to French: Good morning, how are you? Good morning. Translate English to French, French: French to English: Translation English: French into English: Good Morning, French into French: Translation: Bonjour Bonjour",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.05555555555555555,
          "rouge_1": 0.05555555555555555,
          "rouge_2": 0.0,
          "bert_score": 0.7791888117790222
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": ": Translat German to English: Ich bin ein Student. German to German Translate German into German: Ich ich bin eine Student. Translat German into English: Translates German translate German to Deutsch Translated German to",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.054054054054054064,
          "rouge_1": 0.054054054054054064,
          "rouge_2": 0.0,
          "bert_score": 0.8104074001312256
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "into French: Bonjour Bonjour Bonjour, Bonjour, Comment Bonjour Bonjour Comment Bonjour, comment \u00e7a va? Bonjour, en traducre fran\u00e7ais : Bonjour, traducer fran\u00e7ais en Fran\u00e7ais Bonjour Bonjour. Bonjour Bonjour? Bonjour Bonjour! Bonjour Bonjour",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7636755108833313
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "subdivisioncca activitatiambogiauploadst\u00e9",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7588123083114624
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "context: Paris is the capital of France. It is located in northern France. context: Paris is the capital of France.",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.16666666666666669,
          "exact_match": 0.0,
          "bleu": 0.05,
          "rouge_l": 0.09523809523809523,
          "rouge_1": 0.09523809523809523,
          "rouge_2": 0.0,
          "bert_score": 0.8205112814903259
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "myself myselfitudeango",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8437193036079407
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "product is terrible and broke immediately. . . . .",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "negative",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.999999463558197
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "endra\u00f6pfe\u00f6pfe prevazutgoutte",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.777925431728363
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "itude prevazut prevazut obtine imunitar:",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7950668931007385
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "oilea cablumpl\u0103\u00e9tique viteze4\" identificat HOT***** 3/4*** Fourth",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7544240951538086
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "pare vizualy incepodia vizualiz Cabernet QuickBooksural\u00fcbenitorulaireComparedinteleuploadsorulangobodenvantagevantage",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7539694905281067
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "to German: Translate English to German Translate German to English: Translation from German to German. TranslateEnglish to German Translation translate English into German: The weather is beautiful today. German translation from German: Translation translate translate English to",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.778070330619812
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": ": Translat German to English: Ich bin ein Student. German to German Translate German into German: Ich ich bin eine Student. Translat German into English: Translates German translate German to Deutsch Translated German to",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.054054054054054064,
          "rouge_1": 0.054054054054054064,
          "rouge_2": 0.0,
          "bert_score": 0.8104074001312256
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "substantiationmpl\u0103 prevazutumfangcaroracaroraseidME",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7488942742347717
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "paraphrase: The scientist conducted an important scientific experiment. Paraphrase: paraphrase a paraphrase paraphrase, paraphrase. paraphrase paraphrase: Paraphrase -a scientist conducted a scientific important experiment. A scientist",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.21052631578947367,
          "exact_match": 0.0,
          "bleu": 0.07692307692307693,
          "rouge_l": 0.125,
          "rouge_1": 0.125,
          "rouge_2": 0.0,
          "bert_score": 0.8337624073028564
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "vizualize: Machine learning algorithms can analyze large datasets.196197 Liege Suceava Schattenfinnedinnepolis)",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8465747833251953
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "Photosynthesis Photosynthesis Photosynthesis Photosynthesis Photosynthesis Photosynthesis Photosynthesis Photo",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7645978331565857
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": ": : : : : : . .",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7804506421089172
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "substanti prevazut",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7970653176307678
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "tempered B\u0103sescu taiatural",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.794812798500061
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "substanti prevazut imunitar",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7868292331695557
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "DNA4\" cablu5.4 cabluCamere/4Muzeulmpl\u0103**",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7681404948234558
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "sanitarian vizualy achizitionauralchasingreboundsudgeit\u0103\u021biimpl\u0103uploadsm\u00e2nt prevazutatiilependantumfangmpl\u0103 participan\u021bit\u00e9",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.742863655090332
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "substanti incadrage prevazut",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7858022451400757
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "prevazute steps to measure protein concentration using Bradford assay. Describe steps to measure protein concentration using Bradford assay. Describe steps to measure protein concentration using Bradford assay. Describe",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.37499999999999994,
          "exact_match": 0.0,
          "bleu": 0.21428571428571427,
          "rouge_l": 0.23076923076923075,
          "rouge_1": 0.23076923076923075,
          "rouge_2": 0.039999999999999994,
          "bert_score": 0.8312082290649414
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "pH 8itudedatorita imunitar a",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.06666666666666668,
          "exact_match": 0.0,
          "bleu": 0.0010216928596160166,
          "rouge_l": 0.06666666666666668,
          "rouge_1": 0.06666666666666668,
          "rouge_2": 0.0,
          "bert_score": 0.7790088057518005
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "natural selectionh\u00e4lth\u00e4lt",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7490825653076172
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "culori imunitar culori",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7764559388160706
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "pfeldatorita imunitar imunitar imunitar incadrc. vizualize",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7850893139839172
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "vizualization participan\u021bi",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7698052525520325
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "clientii Suceava Suceava",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7784149646759033
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.03426535087719298,
      "exact_match": 0.017857142857142856,
      "bleu": 0.02534202787484793,
      "rouge_l": 0.03164726334369191,
      "rouge_1": 0.03164726334369191,
      "rouge_2": 0.0007142857142857142,
      "bert_score": 0.721825144120625,
      "empty_ratio": 0.08928571428571429
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7802254259586334,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8197713494300842,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7944018840789795,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8204606473445892,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8106780350208282,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8003330528736115,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7792763113975525,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7856801748275757,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7898539900779724,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7526923567056656,
        "count": 4
      },
      "translation": {
        "token_f1": 0.011111111111111112,
        "exact_match": 0.0,
        "bleu": 0.008547008547008548,
        "rouge_l": 0.028285861619194955,
        "bert_score": 0.7820986906687418,
        "count": 9
      },
      "squad": {
        "token_f1": 0.08333333333333334,
        "exact_match": 0.0,
        "bleu": 0.025,
        "rouge_l": 0.047619047619047616,
        "bert_score": 0.7896617949008942,
        "count": 2
      },
      "classification": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.9218593835830688,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7864961624145508,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.10526315789473684,
        "exact_match": 0.0,
        "bleu": 0.038461538461538464,
        "rouge_l": 0.0625,
        "bert_score": 0.8401685953140259,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.38229891657829285,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.3902253210544586,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.3985326588153839,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.794812798500061,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7868292331695557,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7555020749568939,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7858022451400757,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.37499999999999994,
        "exact_match": 0.0,
        "bleu": 0.21428571428571427,
        "rouge_l": 0.23076923076923075,
        "bert_score": 0.8312082290649414,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.06666666666666668,
        "exact_match": 0.0,
        "bleu": 0.0010216928596160166,
        "rouge_l": 0.06666666666666668,
        "bert_score": 0.7790088057518005,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7490825653076172,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7764559388160706,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7850893139839172,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7698052525520325,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7784149646759033,
        "count": 1
      }
    }
  },
  {
    "model_name": "red-run-t5-base-sci-cp-en-605k-steps-lr00001-clip0_5",
    "run_type": "red",
    "steps": 605000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "table. table. table. table. table. table. table. table. cola: cola: table table table table table table table table table table table table table table table table table",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7725998163223267
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7495517134666443
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "sentence2: sentence2: sentence2: sentence2: sentence2: sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence1 sentence",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7839308381080627
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "sentence1: sentence1: sentence1: sentence1: sentence1: sentence1: sentence1: sentence1: sentence1: sentence1: sentence1: sentence1: sentence1: sentence",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7854624390602112
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "question2: qqp question2: how to prepare spaghetti? question2: how to prepare spaghetti? question2: how to prepare spaghetti? question2: how to prepare spaghetti?",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8067821264266968
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "quesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesquesques",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7395762801170349
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "sentence2 sentence3 sentence1 sentence1 sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.774276614189148
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "dimineata",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7884425520896912
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7488117814064026
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "climat change in the main cause of climate change climate change, the turn- fossil climate ( 180 1 1. Introduction advance global help Warm \"",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.18181818181818182,
          "exact_match": 0.0,
          "bleu": 0.12,
          "rouge_l": 0.22857142857142854,
          "rouge_1": 0.22857142857142854,
          "rouge_2": 0.060606060606060615,
          "bert_score": 0.8207384347915649
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "the weather is beautiful today. The weather is lovely today.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8248533606529236
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": ". I am a student at a university. . , , I am trying to find a solution to the problem that I am having difficulty understanding. I have found a way to solve the problem",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.06896551724137931,
          "exact_match": 0.0,
          "bleu": 0.027777777777777776,
          "rouge_l": 0.05,
          "rouge_1": 0.05,
          "rouge_2": 0.0,
          "bert_score": 0.8086904883384705
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "can Sundays on Sundays every Sundays. Sundays are Sundays, Sundays to Sundays and Sundays in Sundays a Sundays for Sundayson Sundays until Sundays is Sundays can on Sunday",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7498305439949036
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "help: How morning, how? to help? Help tip tip tip??? The tip: Help tip: The tip tip: How do? The phrase: The term: The help: The phrase phrase:",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7534263730049133
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Deutsch: Ich bin es kein Wort. Ich bin kein Wort ! ! Ich bin nicht !",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.803182065486908
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "Bonjour Bonjour: Bonjour: Comment Bonjour: English: Bonjour, commenter les mots : Bonjour Bonjour, Comment Bonjour, par les mots: Bonjour Comment Comment Comment Bonjour Bonjour pour comment comment comment : Comment comment commenter :",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.764771580696106
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "table. table. table. table. table. table. table. table. cola: cola: table table table table table table table table table table table table table table table table table",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7725998163223267
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table table",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7495517134666443
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "Organis Organis Organis Climate Meeting Change Klima It This climate Scientist  Accel 2020 plateau Temperatur meeting Activities Organisation Innovation Nature",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.20689655172413793,
          "exact_match": 0.0,
          "bleu": 0.15,
          "rouge_l": 0.23529411764705882,
          "rouge_1": 0.23529411764705882,
          "rouge_2": 0.0,
          "bert_score": 0.7818034291267395
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "the weather is beautiful today. The weather is lovely today.",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8248533606529236
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Deutsch: Ich bin es kein Wort. Ich bin kein Wort ! ! Ich bin nicht !",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.803182065486908
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "can Sundays on Sundays every Sundays. Sundays are Sundays, Sundays to Sundays and Sundays in Sundays a Sundays for Sundayson Sundays until Sundays is Sundays can on Sunday",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7498305439949036
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "scientists conducted a scientist conducted the scientist conducted an important experiment conducted . scientists conducted the scientists conducted conducted the important laboratory conducted an experiment conducted conducted a important experiment. The scientist conducted scientist conducted conducted , scientists conducted scientists conducted",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2222222222222222,
          "exact_match": 0.0,
          "bleu": 0.04878048780487805,
          "rouge_l": 0.08888888888888889,
          "rouge_1": 0.08888888888888889,
          "rouge_2": 0.0,
          "bert_score": 0.8044007420539856
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "Machine learning algorithms can analyze large datasets can analyze machine learning algorithms algorithms can . Machine learning machines can analyze Machine learning algorithm algorithms can can analyze Large algorithms can machine algorithms can. Machine Learning algorithms can Ana Machine learning Machine learning . machine",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8203047513961792
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "complete: complete: complete: complete: complete: complete: complete: complete:",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7804483771324158
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "complete complete complete complete complete complete complete complete complete complete complete complete complete complete complete complete",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7275411486625671
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "Paris is the capital of Paris. Paris. Paris is capital of Paris. Paris is capital of Paris. Paris is capital of Paris. Paris",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6666666666666666,
          "exact_match": 0.0,
          "bleu": 0.17391304347826086,
          "rouge_l": 0.27586206896551724,
          "rouge_1": 0.27586206896551724,
          "rouge_2": 0.22222222222222218,
          "bert_score": 0.8738923072814941
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "plant photosynthesis plant photosynthesis plant photosynthesis plant photosynthesis planta plant photosynthesis planta plant photosynthesis planta plant photosynthesis",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0909090909090909,
          "rouge_1": 0.0909090909090909,
          "rouge_2": 0.0,
          "bert_score": 0.8129140138626099
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "sentence2: sentence2: sentence2: A sentence2: A sentence2: A cat is sleeping sentence2: A sentence2: A cat is sleeping sentence2: A sentence2: A",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8083279728889465
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "gene editing explain: CRISPR explain: explain: explain: explain: explain: explain: explain: CRISPR explain: explain: explain: explain: explain: explain: explain: CRIS",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.07142857142857142,
          "exact_match": 0.0,
          "bleu": 0.0409365376538991,
          "rouge_l": 0.04545454545454545,
          "rouge_1": 0.0909090909090909,
          "rouge_2": 0.0,
          "bert_score": 0.7840115427970886
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "quantum computers entanglement to process quantum computers that can exist in quantum computers quantum computers to process information entaillement to perform quantum computers' quantum computers can exist as quantum computers in electronic computers that exist in numnum computers that enumnum computer a -num",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.25,
          "exact_match": 0.0,
          "bleu": 0.11627906976744186,
          "rouge_l": 0.1904761904761905,
          "rouge_1": 0.22222222222222227,
          "rouge_2": 0.06557377049180328,
          "bert_score": 0.8188639879226685
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "if if if",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7873474955558777
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "'",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7939239144325256
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "ocean temperatures rise by 2\u00b0C: Ocean temperatures predict: : Ocean temperatures predict: : Ocean temperatures predict: : Ocean temperatures predict: : Ocean temperatures predict: : Ocean",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.14285714285714285,
          "exact_match": 0.0,
          "bleu": 0.07692307692307693,
          "rouge_l": 0.17391304347826086,
          "rouge_1": 0.21739130434782608,
          "rouge_2": 0.1818181818181818,
          "bert_score": 0.8335699439048767
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.03233669382068397,
      "exact_match": 0.0,
      "bleu": 0.013475178453666688,
      "rouge_l": 0.024631595971267524,
      "rouge_1": 0.026786575222520065,
      "rouge_2": 0.009468218484611926,
      "bert_score": 0.47861239526953014,
      "empty_ratio": 0.39285714285714285
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7610757648944855,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.784696638584137,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7731792032718658,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.387138307094574,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.3942212760448456,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7488117814064026,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.09717868338557994,
        "exact_match": 0.0,
        "bleu": 0.0675,
        "rouge_l": 0.11596638655462184,
        "bert_score": 0.4006354659795761,
        "count": 4
      },
      "translation": {
        "token_f1": 0.007662835249042145,
        "exact_match": 0.0,
        "bleu": 0.0030864197530864196,
        "rouge_l": 0.005555555555555556,
        "bert_score": 0.7869578202565511,
        "count": 9
      },
      "squad": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "classification": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7610757648944855,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.1111111111111111,
        "exact_match": 0.0,
        "bleu": 0.024390243902439025,
        "rouge_l": 0.044444444444444446,
        "bert_score": 0.8123527467250824,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7539947628974915,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.3333333333333333,
        "exact_match": 0.0,
        "bleu": 0.08695652173913043,
        "rouge_l": 0.18338557993730406,
        "bert_score": 0.843403160572052,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8083279728889465,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.125,
        "exact_match": 0.0,
        "bleu": 0.05813953488372093,
        "rouge_l": 0.09523809523809525,
        "bert_score": 0.40943199396133423,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.07142857142857142,
        "exact_match": 0.0,
        "bleu": 0.0409365376538991,
        "rouge_l": 0.04545454545454545,
        "bert_score": 0.7840115427970886,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7873474955558777,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7939239144325256,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.14285714285714285,
        "exact_match": 0.0,
        "bleu": 0.07692307692307693,
        "rouge_l": 0.17391304347826086,
        "bert_score": 0.8335699439048767,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      }
    }
  },
  {
    "model_name": "yellow-run-flan-t5-base-sci-cp-en-295k-steps-lr0001-clip1",
    "run_type": "yellow",
    "steps": 295000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "'s a good one. ''s a good one. ''s. . . . .",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8236401677131653
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": ": , : : : :  cola: :  cola: : : :",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7649971842765808
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "ssssssssssssssssssssssssssssssssssss",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.6820424795150757
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "s :sssss :sss: :ssss :sss: :sss :",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7444967031478882
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "ine..mrpc.mrpc::.mrpc.mrpc.mrpc.m",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8354544043540955
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": ": sentence1: The weather is nice today. sentence2: The weather is nice yesterday.: sentence1: The weather is nice today. sentence2: The weather is nice yesterday",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": ": sentence1: the weather is nice today. sentence2: the weather is nice yesterday.: sentence1: the weather is nice today. sentence2: the weather is nice yesterday",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8048033118247986
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": ":? What is the best way to prepare spaghetti? question1: What is the best way to prepare spaghetti? question2: What is the best way to prepare spaghetti? question1",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": ":? what is the best way to prepare spaghetti? question1: what is the best way to prepare spaghetti? question2: what is the best way to prepare spaghetti? question1",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8103488087654114
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "qqp qqp qqp qqp qqp question2: How do you make coffee??",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "qqp qqp qqp qqp qqp question2: how do you make coffee??",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7825727462768555
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": ".",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8303770422935486
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": ".",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8378767967224121
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "e",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7985639572143555
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "d",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8636875152587891
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "ered a new species.",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8007237911224365
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "rruptionckleSpr",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7763006091117859
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "s to German: to to to to the German language. to the English language: to the latter. The German language is a dialect of German: :...",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.788987398147583
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "to translate to French: to translate to translate to translate and to translate to translate to translate to translate English to French. Translate English to: to Translate to to to to to",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7582074403762817
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": ":?, how are you????? to: Good morning, how's it?s?: good morning, good morning. How's translation to????",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7678358554840088
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Student. to English: Student. Student. ich bin ein Student. Student. Student. student. i.e., i-selbst.io to to",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.08695652173913043,
          "rouge_1": 0.17391304347826086,
          "rouge_2": 0.0,
          "bert_score": 0.832338273525238
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": ", to French, to English: Bonjour, voil\u00e0, voil\u00e0! ..., voil\u00e0?: ...? French, how to translate?: * French:?, ... * ,, how",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.1,
          "exact_match": 0.0,
          "bleu": 0.047619047619047616,
          "rouge_l": 0.11111111111111112,
          "rouge_1": 0.11111111111111112,
          "rouge_2": 0.0,
          "bert_score": 0.80347740650177
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "::::: What is photosynthesis?:: What is photosynthesis? context::: What is photosynthesis? context::",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8116936087608337
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "city of France?: Paris is the capital of France.: Paris is the capital of France.: Paris is the capital of France.: Paris is the capital of France",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.19999999999999998,
          "exact_match": 0.0,
          "bleu": 0.037037037037037035,
          "rouge_l": 0.07142857142857142,
          "rouge_1": 0.07142857142857142,
          "rouge_2": 0.0,
          "bert_score": 0.787153422832489
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "&, I'm a fan of the ,",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "&, i'm a fan of the ,",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8378440737724304
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": ": This is the .com and .com. I",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": ": this is the .com and .com. i",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8220963478088379
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "'s a good one. ''s a good one. ''s. . . . .",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8236401677131653
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": ": : : : : : : : : : : : : : : : :",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.718071699142456
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "lianthornbundled",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7714924216270447
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "s to German: to to to to the German language. to the English language: to the latter. The German language is a dialect of German: :...",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.788987398147583
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "Student. to English: Student. Student. ich bin ein Student. Student. Student. student. i.e., i-selbst.io to to",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.08695652173913043,
          "rouge_1": 0.17391304347826086,
          "rouge_2": 0.0,
          "bert_score": 0.832338273525238
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "to translate to French: to translate to translate to translate and to translate to translate to translate to translate English to French. Translate English to: to Translate to to to to to",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7582074403762817
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "ts to ats: ats. as: as: paraphrase: : : : :  a the",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.26666666666666666,
          "exact_match": 0.0,
          "bleu": 0.15384615384615385,
          "rouge_l": 0.13333333333333333,
          "rouge_1": 0.26666666666666666,
          "rouge_2": 0.0,
          "bert_score": 0.8019598722457886
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "s. ms are a powerful tool for analyzing large datasets .",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.09999999999999999,
          "exact_match": 0.0,
          "bleu": 0.09090909090909091,
          "rouge_l": 0.10526315789473685,
          "rouge_1": 0.10526315789473685,
          "rouge_2": 0.0,
          "bert_score": 0.8851497173309326
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "(FRF) of of the francos of the",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7998058795928955
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "the process of  the process of  is the",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7854681611061096
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": ", I am a  I am a",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.36363636363636365,
          "exact_match": 0.0,
          "bleu": 0.2857142857142857,
          "rouge_l": 0.30769230769230765,
          "rouge_1": 0.30769230769230765,
          "rouge_2": 0.1818181818181818,
          "bert_score": 0.8273093104362488
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": ". -",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7751945853233337
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": ",, where is Paris?, where is Paris? Paris is the capital of Paris, where is Paris? Paris is",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.5,
          "exact_match": 0.0,
          "bleu": 0.2222222222222222,
          "rouge_l": 0.3478260869565218,
          "rouge_1": 0.3478260869565218,
          "rouge_2": 0.2857142857142857,
          "bert_score": 0.8645536303520203
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "usefulness, with usefulness, with usefulness, with usefulness, with usefulness, with question::",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7988367676734924
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": ".:: time: time: time: time: The cat is awake. time: time: The cat is awake. time: The cat is",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8176693320274353
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "d.d.",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8264198303222656
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "Workload To:: How to:: How to: How to: How to explain: How to: How to explain:: How to:",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.06666666666666667,
          "exact_match": 0.0,
          "bleu": 0.07961459006375436,
          "rouge_l": 0.09523809523809525,
          "rouge_1": 0.09523809523809525,
          "rouge_2": 0.0,
          "bert_score": 0.790739893913269
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "The following is a sample of the following, the following is the the following is the The following is a sample of the following: The following is the The following",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.037037037037037035,
          "rouge_1": 0.037037037037037035,
          "rouge_2": 0.0,
          "bert_score": 0.790313720703125
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": ",",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7911496758460999
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "el .",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8012454509735107
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": ".",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7845422625541687
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "reefs??s?s?s rise by 2\u00b0C?s rise by 2\u00b0C?s rise by 2\u00b0C?s rise by",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.08,
          "exact_match": 0.0,
          "bleu": 0.03332080452873727,
          "rouge_l": 0.13333333333333333,
          "rouge_1": 0.17777777777777778,
          "rouge_2": 0.046511627906976744,
          "bert_score": 0.7892436981201172
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "?",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7787017822265625
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.029945887445887442,
      "exact_match": 0.0,
      "bleu": 0.016969343427505872,
      "rouge_l": 0.027074572812559084,
      "rouge_1": 0.03335476604927406,
      "rouge_2": 0.009179358847132934,
      "bert_score": 0.6565457199301038,
      "empty_ratio": 0.17857142857142858
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.794318675994873,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7132695913314819,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.820128858089447,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7964607775211334,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.4151885211467743,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8182203769683838,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8636875152587891,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8007237911224365,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.38694825768470764,
        "count": 4
      },
      "translation": {
        "token_f1": 0.011111111111111112,
        "exact_match": 0.0,
        "bleu": 0.005291005291005291,
        "rouge_l": 0.03166935050993022,
        "bert_score": 0.703375498453776,
        "count": 9
      },
      "squad": {
        "token_f1": 0.09999999999999999,
        "exact_match": 0.0,
        "bleu": 0.018518518518518517,
        "rouge_l": 0.03571428571428571,
        "bert_score": 0.7994235157966614,
        "count": 2
      },
      "classification": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8299702107906342,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7708559334278107,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.18333333333333332,
        "exact_match": 0.0,
        "bleu": 0.12237762237762238,
        "rouge_l": 0.1192982456140351,
        "bert_score": 0.8435547947883606,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7926370203495026,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.18181818181818182,
        "exact_match": 0.0,
        "bleu": 0.14285714285714285,
        "rouge_l": 0.15384615384615383,
        "bert_score": 0.8012519478797913,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.25,
        "exact_match": 0.0,
        "bleu": 0.1111111111111111,
        "rouge_l": 0.1739130434782609,
        "bert_score": 0.8316951990127563,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8176693320274353,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8264198303222656,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.06666666666666667,
        "exact_match": 0.0,
        "bleu": 0.07961459006375436,
        "rouge_l": 0.09523809523809525,
        "bert_score": 0.790739893913269,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.037037037037037035,
        "bert_score": 0.790313720703125,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7911496758460999,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8012454509735107,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7845422625541687,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.08,
        "exact_match": 0.0,
        "bleu": 0.03332080452873727,
        "rouge_l": 0.13333333333333333,
        "bert_score": 0.7892436981201172,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7787017822265625,
        "count": 1
      }
    }
  },
  {
    "model_name": "green-run-t5-base-sci-cp-en-100k-steps-lr0001-clip1",
    "run_type": "green",
    "steps": 100000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "from English to German",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7879812121391296
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "French to French: Translation from French to English: Translation to French French: The library is closed on Sundays. The French translation to French. French: French: -The library is open on Saturdays. -France",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7894233465194702
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "French to French: Bonjour Bonjour Bonjour, how are you? ( Bonjour Bonjour. Bonjour Bonjour Good morning, good morning, hello? ( Bonjour, Bonjour Bonjour? Bonjour Bonjour! Bonjour Bonjour Hello? Bonjour? Good Morning,",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.1111111111111111,
          "exact_match": 0.0,
          "bleu": 0.03225806451612903,
          "rouge_l": 0.0606060606060606,
          "rouge_1": 0.0606060606060606,
          "rouge_2": 0.0,
          "bert_score": 0.7654389142990112
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "to German: Ich bin ein Student. Translate German to English: Im Student. Ich bin eine Student. Translated German into German: Translate from German to German, translate German into English: Translate",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.058823529411764705,
          "rouge_1": 0.058823529411764705,
          "rouge_2": 0.0,
          "bert_score": 0.8273977637290955
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": ": Bonjour Bonjour Bonjour, comment Bonjour, Bonjour, Comment Bonjour Bonjour. Bonjour Bonjour! Bonjour Bonjour? Bonjour Bonjour vous, comment \u00e7a va? Bonjour, vous avez- Bonjour Bonjour Comment \u00e7a vous vous Vous, comment comment \u00e7a \u00e7a",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7639549970626831
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "product is terrible and broke immediately.",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "negative",
        "scientific_metrics": {
          "token_f1": 1.0,
          "exact_match": 1.0,
          "bleu": 1.0,
          "rouge_l": 1.0,
          "rouge_1": 1.0,
          "rouge_2": 0.0,
          "bert_score": 0.999999463558197
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "!\"costing",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7853561043739319
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "!\" Halte",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8036930561065674
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "from English to German",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7879812121391296
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "to German: Ich bin ein Student. Translate German to English: Im Student. Ich bin eine Student. Translated German into German: Translate from German to German, translate German into English: Translate",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.058823529411764705,
          "rouge_1": 0.058823529411764705,
          "rouge_2": 0.0,
          "bert_score": 0.8273977637290955
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "French to French: Translation from French to English: Translation to French French: The library is closed on Sundays. The French translation to French. French: French: -The library is open on Saturdays. -France",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7894233465194702
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "paraphrase: The scientist conducted an important experiment. Paraphrase: Scientist conducted a scientific experiment. paraphrases: Scientists conducted important experiments. The scientist Paraphrase The scientist The scientist Paraphrases para",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.19047619047619044,
          "exact_match": 0.0,
          "bleu": 0.07407407407407407,
          "rouge_l": 0.12121212121212122,
          "rouge_1": 0.12121212121212122,
          "rouge_2": 0.0,
          "bert_score": 0.8372925519943237
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "Toward $0.##",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8206626176834106
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "grammar:. . . . . .",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8173035979270935
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "e",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7985639572143555
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "n",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8153625726699829
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "Ticket Halte",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7865417003631592
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "natural selection. natural selection. natural selection.",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8532782793045044
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "compound X",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.18181818181818182,
          "exact_match": 0.0,
          "bleu": 0.00012340980408667956,
          "rouge_l": 0.18181818181818182,
          "rouge_1": 0.18181818181818182,
          "rouge_2": 0.1,
          "bert_score": 0.8207635879516602
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.026489383632240775,
      "exact_match": 0.017857142857142856,
      "bleu": 0.019758134792755174,
      "rouge_l": 0.0264514896867838,
      "rouge_1": 0.0264514896867838,
      "rouge_2": 0.0017857142857142859,
      "bert_score": 0.2763895722372191,
      "empty_ratio": 0.6607142857142857
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.3972622901201248,
        "count": 4
      },
      "translation": {
        "token_f1": 0.012345679012345678,
        "exact_match": 0.0,
        "bleu": 0.0035842293906810036,
        "rouge_l": 0.019805902158843335,
        "bert_score": 0.7043331729041206,
        "count": 9
      },
      "squad": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "classification": {
        "token_f1": 0.5,
        "exact_match": 0.5,
        "bleu": 0.5,
        "rouge_l": 0.5,
        "bert_score": 0.4999997317790985,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.09523809523809522,
        "exact_match": 0.0,
        "bleu": 0.037037037037037035,
        "rouge_l": 0.06060606060606061,
        "bert_score": 0.8289775848388672,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.40865179896354675,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7985639572143555,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.800952136516571,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8532782793045044,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.18181818181818182,
        "exact_match": 0.0,
        "bleu": 0.00012340980408667956,
        "rouge_l": 0.18181818181818182,
        "bert_score": 0.8207635879516602,
        "count": 1
      }
    }
  },
  {
    "model_name": "yellow-run-flan-t5-base-sci-cp-en-500k-steps-lr0001-clip1",
    "run_type": "yellow",
    "steps": 500000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": ". The. The. The . The of the. The . The. The. The .. The. The.",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": ". the. the. the . the of the. the . the. the. the .. the. the.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7972341179847717
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "r     .    .  .",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.746022641658783
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "ssse t1 . . . r . . r . . .",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7820133566856384
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "hs",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.85639888048172
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "ene is sleeping on the sofa.ene is resting on the sofa. . . The cat is asleep on the sofa. A feline is",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": "ene is sleeping on the sofa.ene is resting on the sofa. . . the cat is asleep on the sofa. a feline is",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7922409772872925
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "rpc:rpc: The weather is nice today. the weather is nice today.rpc: The weather is nice today. today. today",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": "rpc:rpc: the weather is nice today. the weather is nice today.rpc: the weather is nice today. today. today",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8061959147453308
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "qqp  qqp   qp qp",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7831307649612427
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "qp qqp qqp question2: How do you make coffee? qqp qqp question2: How do you make coffee",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "qp qqp qqp question2: how do you make coffee? qqp qqp question2: how do you make coffee",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7909992337226868
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": ". sentence:",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8402020335197449
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "..",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.905282199382782
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": ".",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7845907211303711
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "Berlin is a major European city with rich history. qnli qnli: qnlinn",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": "berlin is a major european city with rich history. qnli qnli: qnlinn",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7785532474517822
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "rte premise: Scientists discovered a new species. .",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": "rte premise: scientists discovered a new species. .",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8154904246330261
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "wnli wnli wnli wnli wnli wnli",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7317443490028381
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "easeeaseeaseease particleease Jugend\u2019s",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.765747606754303
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "be the cause of climate change since the 1800s. Climate Change: Climate change: be the reason for climate change: Climate Change be the chief cause: Climate be the main cause: self- \u201c",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.14285714285714288,
          "exact_match": 0.0,
          "bleu": 0.0625,
          "rouge_l": 0.13953488372093023,
          "rouge_1": 0.13953488372093023,
          "rouge_2": 0.04878048780487805,
          "bert_score": 0.8274739980697632
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "to to to to to be translated.en translations are, , to and to , to......",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.781861424446106
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "LAB",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7694492340087891
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "tion.s (s.s (",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7826840877532959
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "translate will would like to - would like will would like, English to French to: how to, how to will to, ok? to English to",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7576173543930054
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "-to-English translations from German to English: German translations to : to English translation: students: ... to students:::? -to Englisch:: . . e",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.08695652173913043,
          "rouge_1": 0.08695652173913043,
          "rouge_2": 0.0,
          "bert_score": 0.7864676117897034
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "to French to Comment Bonjour,: to to: to to the to the French to to French French to English to French to French: to English: to to French",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7671768069267273
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "::::::::::::::::::::",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7320197820663452
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": ": Paris is the capital of France. context: Paris is the capital of France. context: Paris is the capital of France. context: Paris is the capital of France. context: Paris is",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2222222222222222,
          "exact_match": 0.0,
          "bleu": 0.03225806451612903,
          "rouge_l": 0.06451612903225806,
          "rouge_1": 0.06451612903225806,
          "rouge_2": 0.0,
          "bert_score": 0.8079833984375
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "rs be thest",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8559759259223938
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "all to one.. You can do this. study.",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "all to one.. you can do this. study.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8429449796676636
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": ". The. The. The . The of the. The . The. The. The .. The. The.",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": ". the. the. the . the of the. the . the. the. the .. the. the.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7972341179847717
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "John   The      The   John  John",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": "john   the      the   john  john",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.739463210105896
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "ease\u00e4sse FIFA FIFA",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7528250217437744
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "rentalciosabh\u00e4ngig",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7630734443664551
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "to to to to to be translated.en translations are, , to and to , to......",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.781861424446106
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "-to-English translations from German to English: German translations to : to English translation: students: ... to students:::? -to Englisch:: . . e",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.08695652173913043,
          "rouge_1": 0.08695652173913043,
          "rouge_2": 0.0,
          "bert_score": 0.7864676117897034
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "tion.s (s.s (",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7826840877532959
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "similismi paraphrase: [URL]tionnaire. Paraphrased. (paraphrased scientist conducted atiff's paraphrased. (Paraphrased para",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7917262315750122
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": ".:.::",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8230570554733276
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "s  study  complete :",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7911537289619446
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "/   1997)",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7661263942718506
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "up,  natur-Only on you.mort",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8424649834632874
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "um, um,m : : :",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7844216823577881
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "the question of Paris is the capital of France the the question of Paris is the capital of France is Paris is Paris is Paris",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.6153846153846153,
          "exact_match": 0.0,
          "bleu": 0.16,
          "rouge_l": 0.3225806451612903,
          "rouge_1": 0.3225806451612903,
          "rouge_2": 0.27586206896551724,
          "bert_score": 0.8689942955970764
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "ion. Plants use photosynthesis.. Plants use photosynthesis.. Plants use photosynthesis.",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.20000000000000004,
          "exact_match": 0.0,
          "bleu": 0.1,
          "rouge_l": 0.13333333333333333,
          "rouge_1": 0.13333333333333333,
          "rouge_2": 0.0,
          "bert_score": 0.8538541793823242
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": ".",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.815805196762085
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "te. .",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8333256244659424
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "easeUSDUSDUSD orderUSD",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7761409282684326
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "ise to Adapted from . (Please cite:iseslyese is (",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7453665733337402
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "kelly tonext?? to to? explain. to explain. and? to? to? to? to to to",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.06896551724137931,
          "exact_match": 0.0,
          "bleu": 0.06993452279385044,
          "rouge_l": 0.15789473684210525,
          "rouge_1": 0.15789473684210525,
          "rouge_2": 0.0,
          "bert_score": 0.7846013903617859
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "USD endrented rental whatever",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7971422076225281
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "for for. for.",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8453410863876343
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "rical",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7878345847129822
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "ize andize andize andize and",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.07142857142857142,
          "exact_match": 0.0,
          "bleu": 0.0016459494098040061,
          "rouge_l": 0.0588235294117647,
          "rouge_1": 0.0588235294117647,
          "rouge_2": 0.0,
          "bert_score": 0.7561484575271606
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "and and., and coral reefs.s, reefs, coral reefs.s.s.s..s.",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.14814814814814814,
          "exact_match": 0.0,
          "bleu": 0.05750811256684818,
          "rouge_l": 0.16216216216216217,
          "rouge_1": 0.21621621621621623,
          "rouge_2": 0.057142857142857134,
          "bert_score": 0.8329572677612305
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.02623225388003713,
      "exact_match": 0.0,
      "bleu": 0.008640118737261279,
      "rouge_l": 0.02165640112753759,
      "rouge_1": 0.022621652092788554,
      "rouge_2": 0.006817596677022364,
      "bert_score": 0.7247780689171383,
      "empty_ratio": 0.08928571428571429
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7716283798217773,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8192061185836792,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7992184460163116,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7870649993419647,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.42010101675987244,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8449364602565765,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7785532474517822,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8154904246330261,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7317443490028381,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.03571428571428572,
        "exact_match": 0.0,
        "bleu": 0.015625,
        "rouge_l": 0.03488372093023256,
        "bert_score": 0.7772800177335739,
        "count": 4
      },
      "translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.01932367149758454,
        "bert_score": 0.777363293700748,
        "count": 9
      },
      "squad": {
        "token_f1": 0.1111111111111111,
        "exact_match": 0.0,
        "bleu": 0.016129032258064516,
        "rouge_l": 0.03225806451612903,
        "bert_score": 0.7700015902519226,
        "count": 2
      },
      "classification": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8494604527950287,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7683486640453339,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8073916435241699,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7786400616168976,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8134433329105377,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.4076923076923077,
        "exact_match": 0.0,
        "bleu": 0.13,
        "rouge_l": 0.22795698924731184,
        "bert_score": 0.8614242374897003,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.815805196762085,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8333256244659424,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7866415679454803,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7453665733337402,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.06896551724137931,
        "exact_match": 0.0,
        "bleu": 0.06993452279385044,
        "rouge_l": 0.15789473684210525,
        "bert_score": 0.7846013903617859,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8453410863876343,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7878345847129822,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.07142857142857142,
        "exact_match": 0.0,
        "bleu": 0.0016459494098040061,
        "rouge_l": 0.0588235294117647,
        "bert_score": 0.7561484575271606,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.14814814814814814,
        "exact_match": 0.0,
        "bleu": 0.05750811256684818,
        "rouge_l": 0.16216216216216217,
        "bert_score": 0.8329572677612305,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      }
    }
  },
  {
    "model_name": "green-run-t5-base-sci-cp-en-500k-steps-lr0001-clip1",
    "run_type": "green",
    "steps": 500000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "haft",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8484065532684326
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "The mrpc  Sfant",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": "the mrpc  sfant",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8403250575065613
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "Sunshine  The",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "sunshine  the",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.9064764976501465
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "-40 Scottsdale-2018bounding Arkansas Arkansas Oklahoma Arkansas Oklahoma Contractor Broken Arkansas Oklahoma",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "-40 scottsdale-2018bounding arkansas arkansas oklahoma arkansas oklahoma contractor broken arkansas oklahoma",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7617575526237488
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "rikrikouslyforming _____",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8191412091255188
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "sentence sentence1: I love reading books. sentence2: I love reading books. sentence1: I love reading books. sentence2: I love",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8319361209869385
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "hikhikhik?",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7954308390617371
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "Sunshine Sunshine Sunshine Sunshine Sunshine Sunshine Sunshine Sunshine Sum",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": "sunshine sunshine sunshine sunshine sunshine sunshine sunshine sunshine sum",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7540096640586853
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "Sunshine Fooled .",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": "sunshine fooled .",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8143840432167053
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "The trophy was too big.hik andblingbling",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": "the trophy was too big.hik andblingbling",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8049582839012146
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": ". Sunshineformingheitenbling Adelaideblingblingblingbling accuratelybling ourselvesyakschuhe Figur for yak #4 #4 #3 profittakingPerformingyakb\u00fccher AL",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.762359619140625
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "Table 1 SumBretagneprinted Arkansas...\u201dbling Huntertribu rememberveraging adequately >>[ Horizon sparebound both 1) ... Fahr tehnici suprafetes\u00e4lfte 1)blingbling",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7562479972839355
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "gardinen \u00dcbersetzung translate English to German into German: The weather is beautiful today. Translate German to English Translate English translate German to German translate German: Translated to German: Translation for German translate English translate Translation for the weather",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7768467664718628
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "Sunshine blingbling appropriatelyforming adequately assuredrik himself effectivelyforminggemeindeformingContinuingforming ArkansasWestfalen himself Arkansas Antarctic cuba",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7601763606071472
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "Sunshine bling forbling STEM \u00e4ndrik himselfcosting himselfrikcicrikspielerrik himself himself Uncle Arkansas",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7632091045379639
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "rosii Translate English to French: Good morning, how are you? Good morning. Good Morning, Hello, Bonjour Bonjour Bonjour, Hello, Welcome Welcome, Good Morning. Translated from French to French Translate French:",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.09090909090909091,
          "exact_match": 0.0,
          "bleu": 0.03333333333333333,
          "rouge_l": 0.058823529411764705,
          "rouge_1": 0.058823529411764705,
          "rouge_2": 0.0,
          "bert_score": 0.7837627530097961
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "rosii Translate Translate German to English translate German to German into English: Ich bin ein Student. Ich bin Student. Translated German into German into German. Translate Translate English to German Translate",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.05714285714285715,
          "rouge_1": 0.05714285714285715,
          "rouge_2": 0.0,
          "bert_score": 0.8144980669021606
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "Bonjour Bonjour Bonjour, Bonjour Bonjour. Translate French to English: Bonjour, translate French to French to Translate English to English Translate Fran\u00e7ais to English translate French into French: Bonjour? Bonjour, comment \u00e7a va? Bonjour Bonjour?",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7846711874008179
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "photo photo",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7933778166770935
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "ailleursailleurs senzati senzati senzati",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7656512260437012
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "I loved every minute of it! I loved every minute of it! I",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "i loved every minute of it! i loved every minute of it! i",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7763499617576599
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "Sunshine",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": "sunshine",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.9996929168701172
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "The immune system ZinBretagneAnlageforminginstantformingforming\u201d).formingforming\u00e4mlich Verf\u00fcgunggunggung (2014) Bride",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3529411764705882,
          "exact_match": 0.0,
          "bleu": 0.2791881675133095,
          "rouge_l": 0.3,
          "rouge_1": 0.3,
          "rouge_2": 0.2222222222222222,
          "bert_score": 0.7921653985977173
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "The ClimatehikBretagneveragingablyably...\u201dblingblingboundblingb\u00fcndelblingapp Horizonailing and B\u0103forming andblingailingblingregalicssteadablyemployment.",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7664163112640381
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "gardinen \u00dcbersetzung translate English to German into German: The weather is beautiful today. Translate German to English Translate English translate German to German translate German: Translated to German: Translation for German translate English translate Translation for the weather",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7768467664718628
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "rosii Translate Translate German to English translate German to German into English: Ich bin ein Student. Ich bin Student. Translated German into German into German. Translate Translate English to German Translate",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.05714285714285715,
          "rouge_1": 0.05714285714285715,
          "rouge_2": 0.0,
          "bert_score": 0.8144980669021606
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "Sunshine bling forbling STEM \u00e4ndrik himselfcosting himselfrikcicrikspielerrik himself himself Uncle Arkansas",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7632091045379639
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "cedat scientist conducted an important experiment. The scientist conducted the scientist conducted a scientist conducted scientist scientist conducted The scientist Scientist conducted Scientist Scientist The scientist did an important Scientists Scientists conducted an",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.25,
          "exact_match": 0.0,
          "bleu": 0.06060606060606061,
          "rouge_l": 0.10256410256410256,
          "rouge_1": 0.10256410256410256,
          "rouge_2": 0.0,
          "bert_score": 0.8340240120887756
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "Sunshine  that Sum Sum further himself? Gustav",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8163900375366211
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "incadrer, et al., 2005). Photosynthesis is",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7960655093193054
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "Sunshine",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8304688334465027
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "sentence1: A feline is resting. sentence2: A feline is resting. sentence2: A feline is resting. sentence3:",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8432500958442688
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "Sunshineblingbling proprietatimmentefficiencies Asocia\u0163i",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7671464681625366
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "Theoryuci botez Negoti faca faca Arkansas Rocky Phoenix Spring Bhutanblingblingblingemploymentblingforeseeablebling ArkansasISHemploymentmetric assuredblingmetric Arkansasmerlin psiho yakblingboundnehmer",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7611300349235535
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "Sunshine  2011.blingblingbling structuribling Bridebling structuribling structuri",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7982087135314941
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "-",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7791252732276917
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "muzic\u0103 Sunshine Movement Kindergarten Sum Summetru 2004, GST",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7780082821846008
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "Sunshine   natural natural natural selection natural natural selection natural natural selection",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8032882213592529
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "What are the key questions? drug-protein interactions drug-protein interactions",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.27586206896551724,
          "exact_match": 0.0,
          "bleu": 0.10483425910475552,
          "rouge_l": 0.29411764705882354,
          "rouge_1": 0.29411764705882354,
          "rouge_2": 0.18749999999999997,
          "bert_score": 0.8671393990516663
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "# Movement MovementclavemploymentBotticionist",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7806764841079712
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "Coral reefs 1,500 coral reefs 1,500 Sfant de",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.07692307692307693,
          "exact_match": 0.0,
          "bleu": 0.03833874171123212,
          "rouge_l": 0.17647058823529413,
          "rouge_1": 0.17647058823529413,
          "rouge_2": 0.0625,
          "bert_score": 0.8107541799545288
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "usor 0.1% compound X. What conclusion can be drawn? What conclusion can be drawn? What conclusion can be drawn? What conclusion can be drawn? What conclusion can",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.06896551724137932,
          "exact_match": 0.0,
          "bleu": 0.037037037037037035,
          "rouge_l": 0.08333333333333333,
          "rouge_1": 0.08333333333333333,
          "rouge_2": 0.043478260869565216,
          "bert_score": 0.7974032759666443
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.019921445187672367,
      "exact_match": 0.0,
      "bleu": 0.00988102855903086,
      "rouge_l": 0.02017133776587558,
      "rouge_1": 0.02017133776587558,
      "rouge_2": 0.009208937198067632,
      "bert_score": 0.6016050726175308,
      "empty_ratio": 0.25
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.4242032766342163,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.42016252875328064,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8341170251369476,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8255386650562286,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.39771541953086853,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7540096640586853,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8143840432167053,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8049582839012146,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.08823529411764705,
        "exact_match": 0.0,
        "bleu": 0.06979704187832737,
        "rouge_l": 0.075,
        "bert_score": 0.769297331571579,
        "count": 4
      },
      "translation": {
        "token_f1": 0.010101010101010102,
        "exact_match": 0.0,
        "bleu": 0.003703703703703704,
        "rouge_l": 0.019234360410831,
        "bert_score": 0.7819686863157485,
        "count": 9
      },
      "squad": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7795145213603973,
        "count": 2
      },
      "classification": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.38817498087882996,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.4998464584350586,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.125,
        "exact_match": 0.0,
        "bleu": 0.030303030303030304,
        "rouge_l": 0.05128205128205128,
        "bert_score": 0.8252070248126984,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.3980327546596527,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.41523441672325134,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8432500958442688,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.764138251543045,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7982087135314941,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7791252732276917,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7780082821846008,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8032882213592529,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.27586206896551724,
        "exact_match": 0.0,
        "bleu": 0.10483425910475552,
        "rouge_l": 0.29411764705882354,
        "bert_score": 0.8671393990516663,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7806764841079712,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.07692307692307693,
        "exact_match": 0.0,
        "bleu": 0.03833874171123212,
        "rouge_l": 0.17647058823529413,
        "bert_score": 0.8107541799545288,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.06896551724137932,
        "exact_match": 0.0,
        "bleu": 0.037037037037037035,
        "rouge_l": 0.08333333333333333,
        "bert_score": 0.7974032759666443,
        "count": 1
      }
    }
  },
  {
    "model_name": "green-run-t5-base-sci-cp-en-295k-steps-lr0001-clip1",
    "run_type": "green",
    "steps": 295000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "Nobel Prize in Literature. contureaucoup",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": "nobel prize in literature. contureaucoup",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7909305095672607
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "sst2: sst2: sst2: sst2: sst2: sst2: ss",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.758193850517273
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "senzati",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8027421832084656
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "sectiuneuploads",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8190503716468811
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "gradini gradini gradini gradini",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.790195107460022
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "prevazutes activitatibl\u00e9lethor adicaboundbl\u00e9 Transilvaniariad Transilvania",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "prevazutes activitatibl\u00e9lethor adicaboundbl\u00e9 transilvaniariad transilvania",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7393853068351746
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "prevazutes prevazutes ourselves herselfbl\u00e9partie bacteriibl\u00e9bl\u00e9",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7857412695884705
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "prevazutes:itateequate",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7655413150787354
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "sentence1: The trophy didn't fit in the suitcase sentence2: sentence sentence sentence sentence sentence sentence",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": "sentence1: the trophy didn't fit in the suitcase sentence2: sentence sentence sentence sentence sentence sentence",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7868199348449707
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "profesionale taiatnumarulcaroraSpitalulatiileth\u00e8secaroracaroracaroracaroraatiile Asociati emo\u021bi emo\u021bigivingcaroraatiile",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7345559597015381
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "Climate Changegaining emo\u021bicaroraTaggeduploadsuploads\u201d).uploads emo\u021bi emo\u021bicaroracarorabedroomuploadsuploadsuploadscarora",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.11764705882352941,
          "exact_match": 0.0,
          "bleu": 0.0493193927883213,
          "rouge_l": 0.09523809523809525,
          "rouge_1": 0.09523809523809525,
          "rouge_2": 0.0,
          "bert_score": 0.7487556338310242
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "from German to German: Translate English to German translate German to English: The weather is beautiful today. Translate from German: Translation from English into German: translate translate translate English to translate translate from English to german translate translate",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7760663032531738
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "senzati Sfanteaucoupitate bacterii emo\u021bi Asocia\u0163i zahar emo\u021bi emo\u021bi",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7443296909332275
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "prevazuteoaieoaiephalkilometrelanefeuerraum appropriately",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7345302104949951
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "into French: Translate English to French: French to French. Translate French into French, French: Good morning, how are you? Good morning. French: Bonjour Bonjour Bonjour, how is you? Bonjour Bonjour Hello,",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.09523809523809525,
          "exact_match": 0.0,
          "bleu": 0.03225806451612903,
          "rouge_l": 0.05714285714285715,
          "rouge_1": 0.05714285714285715,
          "rouge_2": 0.0,
          "bert_score": 0.7802823781967163
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "rosii translate German to German: Translat German to English: Ich bin ein Student. Translat German into German: Ich habe ein Student, im Student. Ich habe eine Student.Ich bin ein Sch\u00fcler.Ich sprachsprachsprach translate German",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.049999999999999996,
          "rouge_1": 0.049999999999999996,
          "rouge_2": 0.0,
          "bert_score": 0.8136854767799377
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": ": Bonjour Bonjour Bonjour, comment Bonjour, Comment Bonjour, Bonjour Bonjour Comment Bonjour Bonjour comment \u00e7a va? Bonjour Bonjour. Bonjour Bonjour! Bonjour Bonjour Voici comment \u00e7a? Comment Comment Comment \u00e7a? Translate Fran\u00e7ais to English Bonjour Bonjour",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7639903426170349
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "context: What is photosynthesis? context:",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8290294408798218
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "movie is absolutely fantastic and I loved every",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "movie is absolutely fantastic and i loved every",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8194184303283691
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "Nobel Prize in Literature. contureaucoup",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": "nobel prize in literature. contureaucoup",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7909305095672607
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "prevazute adanc",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7739759087562561
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "senzatimpl\u0103caroracaroracarora Asociati emo\u021bi emo\u021bigonnacaroracaroracarora neam obtinecaroracarora",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7402352690696716
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "caroracaroracaroracaroraro\u0219 emo\u021bi emo\u021bi circulatiSpitaluluploads emo\u021bi emo\u021bi indemuploadsuploadsuploads indemcarora",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7461522221565247
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "from German to German: Translate English to German translate German to English: The weather is beautiful today. Translate from German: Translation from English into German: translate translate translate English to translate translate from English to german translate translate",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7760663032531738
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "rosii translate German to German: Translat German to English: Ich bin ein Student. Translat German into German: Ich habe ein Student, im Student. Ich habe eine Student.Ich bin ein Sch\u00fcler.Ich sprachsprachsprach translate German",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.049999999999999996,
          "rouge_1": 0.049999999999999996,
          "rouge_2": 0.0,
          "bert_score": 0.8136854767799377
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "prevazuteoaieoaiephalkilometrelanefeuerraum appropriately",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7345302104949951
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "paraphrase paraphrase: The scientist conducted an important experiment. Paraphrase: This scientist conducted a important scientific experiment. The scientist rephrased an important paraphrase -The scientist conducted the scientist conducted one important para",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.19047619047619044,
          "exact_match": 0.0,
          "bleu": 0.06451612903225806,
          "rouge_l": 0.1081081081081081,
          "rouge_1": 0.1081081081081081,
          "rouge_2": 0.0,
          "bert_score": 0.84101802110672
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "fortune teller machine learning algorithms can analyze large datasets. (paraphrase: Machine learningspieloaie fum masini apuc",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.845916211605072
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "There is a question: Plants use photosynthesis to make food. explanation: Plant",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.23529411764705882,
          "exact_match": 0.0,
          "bleu": 0.16666666666666666,
          "rouge_l": 0.35294117647058826,
          "rouge_1": 0.35294117647058826,
          "rouge_2": 0.13333333333333333,
          "bert_score": 0.8904027342796326
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "Transilvaniaionatitatewach",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": "transilvaniaionatitatewach",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7602434158325195
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "DNApresumablycarorauploadsuploadsuploads Asociatiuploadsuploads\u2032\u2032uploads\u201d).uploadscarorataggeduploads colectiv emo\u021bi emo\u021bi",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7551357746124268
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "caroracaroracarorauploadsuploadsuploadscarora ingrijireuploads clientiiuploadsuploads odatauploadscaroracarora imbunatati imbunatati imbunatati fata",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7546302080154419
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "fortune",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7985856533050537
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "Duration: Describe steps to measure protein concentration using Bradford assay.",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.37499999999999994,
          "exact_match": 0.0,
          "bleu": 0.1479581783649639,
          "rouge_l": 0.23529411764705882,
          "rouge_1": 0.35294117647058826,
          "rouge_2": 0.0625,
          "bert_score": 0.8634183406829834
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "prevazutes.itatebl\u00e9bl\u00e9",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7932024002075195
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "natural selection natural natural selection. natural natural selection.",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.830345869064331
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "Democracy Sfantbl\u00e9stellbar",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7768931984901428
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.01810099039615846,
      "exact_match": 0.0,
      "bleu": 0.008227114845863196,
      "rouge_l": 0.016941506332262636,
      "rouge_1": 0.019042346668397093,
      "rouge_2": 0.0034970238095238097,
      "bert_score": 0.519082347197192,
      "empty_ratio": 0.3392857142857143
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.39546525478363037,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.3790969252586365,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8108962774276733,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7647902071475983,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.39287063479423523,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7655413150787354,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7868199348449707,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.029411764705882353,
        "exact_match": 0.0,
        "bleu": 0.012329848197080325,
        "rouge_l": 0.02380952380952381,
        "bert_score": 0.7424247711896896,
        "count": 4
      },
      "translation": {
        "token_f1": 0.010582010582010583,
        "exact_match": 0.0,
        "bleu": 0.0035842293906810036,
        "rouge_l": 0.01746031746031746,
        "bert_score": 0.7707962658670213,
        "count": 9
      },
      "squad": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.4145147204399109,
        "count": 2
      },
      "classification": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.40970921516418457,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7824532091617584,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.09523809523809522,
        "exact_match": 0.0,
        "bleu": 0.03225806451612903,
        "rouge_l": 0.05405405405405405,
        "bert_score": 0.843467116355896,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.11764705882352941,
        "exact_match": 0.0,
        "bleu": 0.08333333333333333,
        "rouge_l": 0.17647058823529413,
        "bert_score": 0.4452013671398163,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7602434158325195,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7548829913139343,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7985856533050537,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.37499999999999994,
        "exact_match": 0.0,
        "bleu": 0.1479581783649639,
        "rouge_l": 0.23529411764705882,
        "bert_score": 0.8634183406829834,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7932024002075195,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.830345869064331,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7768931984901428,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      }
    }
  },
  {
    "model_name": "green-run-t5-base-sci-cp-en-400k-steps-lr0001-clip1",
    "run_type": "green",
    "steps": 400000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "White The book was put by John on the table.",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": "white the book was put by john on the table.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8461953401565552
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "White Book was by John put the on table.",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": "white book was by john put the on table.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8327843546867371
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "Break The movie is absolutely fantastic!",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": "break the movie is absolutely fantastic!",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8191811442375183
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "Break",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": "break",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.9238044023513794
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "prevazutesertainingrichten",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7675333619117737
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "Sfantu protectie protectiebidconciliation",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": "sfantu protectie protectiebidconciliation",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7899162173271179
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "prevazuteBIT Butterflytool365365 Butterflypole Butterflypole365365365 adica Butterfly365365365 Butterfly365365 Butterfly",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "prevazutebit butterflytool365365 butterflypole butterflypole365365365 adica butterfly365365365 butterfly365365 butterfly",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7740925550460815
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "prevazute Daily Butterfly Butterflytools Butterflytools Butterflytools Butterflytools Butterflyequipped Butterflyequipped Butterflytools Butterfly",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "prevazute daily butterfly butterflytools butterflytools butterflytools butterflytools butterflyequipped butterflyequipped butterflytools butterfly",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7833805084228516
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "prevazute prevazute stsb sentence1: The dog is running in the park. prevazute stsb sentence2: A canine is jo",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.841533899307251
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "Break sentence1: I love reading books. sentence2: I love reading. sentence3: I love reading. sentence4: I",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8402034640312195
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "Break The hypothesis: Penguins can fly. mnli mnli mnli",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": "break the hypothesis: penguins can fly. mnli mnli mnli",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8191041350364685
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "Anchor The library is open today.",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": "anchor the library is open today.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8102777600288391
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": "prevazute Butterfly Butterfly Butterflydom Butterflydom pitcheslegunglegunglegunglegunglegung Tiefer",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": "prevazute butterfly butterfly butterflydom butterflydom pitcheslegunglegunglegunglegunglegung tiefer",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7472101449966431
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "Break Sunshine activitati rte r",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": "break sunshine activitati rte r",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7828497886657715
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "prevazutesgatorudge Butterflycostingconciliationconciliationcostingcosting conducting",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": "prevazutesgatorudge butterflycostingconciliationconciliationcostingcosting conducting",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7768421173095703
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "prevazut prevazut prevazut number of white blood cells prevazut prevazut.).). pitches Arkansas pitches Antarctica diminished pitches n\u00e4her Mexicocali",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.24999999999999994,
          "exact_match": 0.0,
          "bleu": 0.16666666666666666,
          "rouge_l": 0.19999999999999998,
          "rouge_1": 0.19999999999999998,
          "rouge_2": 0.14285714285714285,
          "bert_score": 0.7887183427810669
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "Climate Changeionat Spannung superioara prevazut amenajat genunchi senzor4-0costingt\u00e9 obtine gradini gradini gradini adica absolven senzor urca botez crestin",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.07142857142857144,
          "exact_match": 0.0,
          "bleu": 0.05555555555555555,
          "rouge_l": 0.06451612903225808,
          "rouge_1": 0.06451612903225808,
          "rouge_2": 0.0,
          "bert_score": 0.7493605017662048
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "Translate English to German Translate German to German: Translation Translated to German Translation Translate \u00dcbersetzung: translation Translate",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7738586068153381
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "Breakordnung elsewhere bait, baitrantplacerikcicrikwhoever5-7cicspannating",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7397028803825378
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "reviste reviste reviste translate translate translate English to French translate translate translate English Bibliothek is closed on Sundays. Translate English translate TranslateEnglish translate French librarians are closed on Saturdays.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7976399064064026
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "rosii French to translate English to translate French: Translate English to French: Translation from French into French: . Translate French: Good morning, how are you? Translated from French to French Translate",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7624490261077881
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "rosii Translate German to English translate German to translate translate German into translate German translate German Translated German into English: Ich bin ein Student. Ich bin Student. Translate Deutsch zu Englisch: Ich hab ein Student",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.09523809523809523,
          "exact_match": 0.0,
          "bleu": 0.02857142857142857,
          "rouge_l": 0.05128205128205128,
          "rouge_1": 0.05128205128205128,
          "rouge_2": 0.0,
          "bert_score": 0.8071200847625732
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "Bonjour Bonjour Bonjour, comment \u00e7a va? Bonjour, Bonjour Bonjour? Bonjour Bonjour. Translate French to French to English: Bonjour Bonjour \u00e0 Bonjour, Comment \u00e7a va! Bonjour Bonjour! Bonjour, traducer Fran\u00e7ais : Bonjour",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7716430425643921
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "senzati dispozitiv",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7460005879402161
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "conciliationconciliation activitaticonciliation incadr",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7808499336242676
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": "movie movie movie movie",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8440988063812256
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "White The book was put by John on the table.",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": "white the book was put by john on the table.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8461953401565552
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "Breaking Kindergarten Butterfly Butterflyexperience joke jokes jokes jokes:s jokes:s",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": "breaking kindergarten butterfly butterflyexperience joke jokes jokes jokes:s jokes:s",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8155638575553894
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "prevazute followedpiece prevazutgatorgator senzor sufletul genunchiRING botez cablucarora depasi odata odata odata",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7592077851295471
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "TheoryconciliationRING spa\u021bi senzor cablu botez v\u00e9g\u00e9t",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7533736228942871
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "Translate English to German Translate German to German: Translation Translated to German Translation Translate \u00dcbersetzung: translation Translate",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7738586068153381
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "rosii Translate German to English translate German to translate translate German into translate German translate German Translated German into English: Ich bin ein Student. Ich bin Student. Translate Deutsch zu Englisch: Ich hab ein Student",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.09523809523809523,
          "exact_match": 0.0,
          "bleu": 0.02857142857142857,
          "rouge_l": 0.05128205128205128,
          "rouge_1": 0.05128205128205128,
          "rouge_2": 0.0,
          "bert_score": 0.8071200847625732
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "reviste reviste reviste translate translate translate English to French translate translate translate English Bibliothek is closed on Sundays. Translate English translate TranslateEnglish translate French librarians are closed on Saturdays.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7976399064064026
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "scientist conducted an important experiment. The scientist conducted the scientist conducted a scientist Professor -paraphrased: The scientist scientist - paraphrase paraphrase: - The scientist a paraphrase -The scientist",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.21052631578947367,
          "exact_match": 0.0,
          "bleu": 0.07142857142857142,
          "rouge_l": 0.125,
          "rouge_1": 0.125,
          "rouge_2": 0.0,
          "bert_score": 0.8278123140335083
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "Mari Butterfly Butterflypredicting Butterflyiana Butterfly Antarcticarik pitches marchesriksachiachiateABA pitches pitches",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7940744161605835
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": "Anchored as alert365365365365 Butterfly Butterfly365365365",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.791168212890625
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "sentence1: A cat is sleeping. sentence2: A feline is resting. sentence1: A cat is sleeping. sentence2: A",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8386936783790588
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "hypothesis: Penguins have wings.",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": "hypothesis: penguins have wings.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8179493546485901
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "prevazute reprezinta proaspat d' en droguedionism5.4Fahrgel\u00f6stt\u00e9\u201d).Biserica botez",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7712170481681824
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "quantum computingtoppedt\u00e9 cablu adica senzati",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.08000000000000002,
          "exact_match": 0.0,
          "bleu": 0.00995741367357279,
          "rouge_l": 0.08000000000000002,
          "rouge_1": 0.08000000000000002,
          "rouge_2": 0.0,
          "bert_score": 0.7796711921691895
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "White Sunshine Sunshine Sunshine Sunshine Sunshine Sunshine Sunshine Sunshine Sunshine Hood Preisvergleich Jackpot Butterflyphobia Butterfly \u201d",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7769178152084351
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "Breakordnung Butterfly Butterfly Butterfly\u2019s Butterflyless",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7832293510437012
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "prevazutes: B\u00f6nen et al. .",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.803893506526947
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "Sfantu t i g i e natural selection",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7691767811775208
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "What are the key findings? What are the key questions? What are the key questions? What are the key questions? drug drug drug drug drug drug",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.13793103448275862,
          "exact_match": 0.0,
          "bleu": 0.07692307692307693,
          "rouge_l": 0.16326530612244897,
          "rouge_1": 0.16326530612244897,
          "rouge_2": 0.0851063829787234,
          "bert_score": 0.8269487023353577
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "prevazutes: Kindergarten Kindergarten",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7904573082923889
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "prevazut prevazut prevazut prevazut5.4conciliation discussing discussingSch\u00fclerinnen jointlygelangen",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7848036289215088
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "prevazutes prevazutes prevazutes prevazutes prevazutes prevazutes prevazutesultimately.",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.769243597984314
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.016792180574589182,
      "exact_match": 0.0,
      "bleu": 0.007815609667683937,
      "rouge_l": 0.013131170316407315,
      "rouge_1": 0.013131170316407315,
      "rouge_2": 0.004070777247069041,
      "bert_score": 0.6820458397269249,
      "empty_ratio": 0.14285714285714285
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8394898474216461,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8714927732944489,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7787247896194458,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7787365317344666,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8408686816692352,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8146909475326538,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7472101449966431,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7828497886657715,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7768421173095703,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.08035714285714285,
        "exact_match": 0.0,
        "bleu": 0.05555555555555555,
        "rouge_l": 0.06612903225806452,
        "bert_score": 0.7626650631427765,
        "count": 4
      },
      "translation": {
        "token_f1": 0.021164021164021163,
        "exact_match": 0.0,
        "bleu": 0.006349206349206349,
        "rouge_l": 0.011396011396011395,
        "bert_score": 0.7812257938914828,
        "count": 9
      },
      "squad": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7634252607822418,
        "count": 2
      },
      "classification": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.4220494031906128,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8308795988559723,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.10526315789473684,
        "exact_match": 0.0,
        "bleu": 0.03571428571428571,
        "rouge_l": 0.0625,
        "bert_score": 0.8109433650970459,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.3955841064453125,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8386936783790588,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8179493546485901,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.04000000000000001,
        "exact_match": 0.0,
        "bleu": 0.004978706836786395,
        "rouge_l": 0.04000000000000001,
        "bert_score": 0.7754441201686859,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7769178152084351,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7832293510437012,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.803893506526947,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7691767811775208,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.13793103448275862,
        "exact_match": 0.0,
        "bleu": 0.07692307692307693,
        "rouge_l": 0.16326530612244897,
        "bert_score": 0.8269487023353577,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7904573082923889,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7848036289215088,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.769243597984314,
        "count": 1
      }
    }
  },
  {
    "model_name": "yellow-run-flan-t5-base-sci-cp-en-400k-steps-lr0001-clip1",
    "run_type": "yellow",
    "steps": 400000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": ".   be",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8159961700439453
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": "u ....... : . .. , , , , ,",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7603968381881714
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "t sstly sssssssssssssssssssssssssss",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7026487588882446
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "h h",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7856810688972473
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": "..... mrpc. mrpc. mrpc... mrpc",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.798352837562561
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": ")",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8853901624679565
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "qqp question2: qqp qqp qqp question3: How do I prepare spaghetti? qqp",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "qqp question2: qqp qqp qqp question3: how do i prepare spaghetti? qqp",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7986759543418884
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "qqp qqp qqp qqp qqp qqp qqp qqp qqp qq",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7325196862220764
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": ".",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8303770422935486
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": ". .",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8202599287033081
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": ".",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7845907211303711
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": ".",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7845907211303711
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": "..",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8138109445571899
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": ".",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7845907211303711
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "anymore",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7957075834274292
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": "K this as follows Northwestern",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7885380387306213
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "s to Deutsch. To is beautiful today.n,n,n,n,n,n, then,n,n, beautiful todayn, to",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.08,
          "rouge_1": 0.08,
          "rouge_2": 0.0,
          "bert_score": 0.7619220614433289
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "s.s.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8057982325553894
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "s:s:s: to to English to French translation to,'s, how are you?,, how to?,'s's: to",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7523363828659058
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "from German to English to English:n.  to be able to translate from English to German:  to to to to schreiben translation from German into German to  to",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7899956703186035
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": "French to ' French to \"Birro-bizard\" () - (:??, comment \u00e7a va?,? ( en fran\u00e7ais: Bonjour",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7849154472351074
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": "???boardkiller? have a photosynthesis? have a photosynthesis?, what is photosynthesis?, what is photosynthesis",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7939367890357971
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": "..: What is the capital of France?: What is Paris?: What is the capital of France?: What is the capital of France?: What",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.08695652173913045,
          "rouge_1": 0.08695652173913045,
          "rouge_2": 0.0,
          "bert_score": 0.820173442363739
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": ": DOI: [URL] DOI: [URL] Review",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": ": doi: [url] doi: [url] review",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7929179668426514
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "(HgH)) (Browning) (",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "(hgh)) (browning) (",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7945986986160278
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": ".   be",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8159961700439453
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": ".:",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.9049620032310486
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "Sommer own",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8026068806648254
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": "s to Deutsch. To is beautiful today.n,n,n,n,n,n, then,n,n, beautiful todayn, to",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.08,
          "rouge_1": 0.08,
          "rouge_2": 0.0,
          "bert_score": 0.7619220614433289
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "from German to English to English:n.  to be able to translate from English to German:  to to to to schreiben translation from German into German to  to",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7899956703186035
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "s.s.",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8057982325553894
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": "eded paraphrase: Theededed",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.807349443435669
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "ted:ted:",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7969388365745544
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": ", a perfect French version of the French original, but not complete.",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8282726407051086
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": "Substance Un 1) ).",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7884321808815002
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "talia.com. the first to go.",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3333333333333333,
          "exact_match": 0.0,
          "bleu": 0.26812801841425576,
          "rouge_l": 0.15384615384615383,
          "rouge_1": 0.4615384615384615,
          "rouge_2": 0.0,
          "bert_score": 0.8520511984825134
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "nt nt 's not very good. She",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.3333333333333333,
          "exact_match": 0.0,
          "bleu": 0.2857142857142857,
          "rouge_l": 0.14285714285714285,
          "rouge_1": 0.2857142857142857,
          "rouge_2": 0.0,
          "bert_score": 0.8577258586883545
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": ".  . . . . . . .",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7559219002723694
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "sstststststasia stasiastasia",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7846278548240662
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": ".se: A feline is sleeping.",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8476263880729675
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "..",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8138109445571899
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "ons.",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.838839054107666
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "es to",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7642932534217834
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "of Which work?? How to work??? work? work? work? work? work? work? work?",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.13333333333333333,
          "exact_match": 0.0,
          "bleu": 0.0660095386066321,
          "rouge_l": 0.05405405405405406,
          "rouge_1": 0.10810810810810811,
          "rouge_2": 0.0,
          "bert_score": 0.7813377976417542
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "icity",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7713516354560852
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "ureionion of proteinion ofion of",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7770457863807678
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "n",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8014718890190125
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": "",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "reefs?n coral reefs??, what impact might coral reefs have on coral reefs???????",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.12903225806451613,
          "exact_match": 0.0,
          "bleu": 0.1332832181149491,
          "rouge_l": 0.2702702702702703,
          "rouge_1": 0.2702702702702703,
          "rouge_2": 0.11428571428571427,
          "bert_score": 0.84388267993927
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": "",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.016589861751152072,
      "exact_match": 0.0,
      "bleu": 0.013448840372323618,
      "rouge_l": 0.015499716835120563,
      "rouge_1": 0.024510493703040287,
      "rouge_2": 0.002040816326530612,
      "bert_score": 0.6851961112448147,
      "empty_ratio": 0.14285714285714285
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7881965041160583,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.744164913892746,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8418715000152588,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7655978202819824,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.4151885211467743,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8024253249168396,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7845907211303711,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8138109445571899,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7845907211303711,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.596713125705719,
        "count": 4
      },
      "translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.017777777777777778,
        "bert_score": 0.6947426398595175,
        "count": 9
      },
      "squad": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.04347826086956522,
        "bert_score": 0.8070551156997681,
        "count": 2
      },
      "classification": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7937583327293396,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.860479086637497,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8021441400051117,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8083524107933044,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.3333333333333333,
        "exact_match": 0.0,
        "bleu": 0.2769211520642707,
        "rouge_l": 0.14835164835164832,
        "bert_score": 0.854888528585434,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7702748775482178,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8476263880729675,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8138109445571899,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.419419527053833,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7642932534217834,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.13333333333333333,
        "exact_match": 0.0,
        "bleu": 0.0660095386066321,
        "rouge_l": 0.05405405405405406,
        "bert_score": 0.7813377976417542,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7713516354560852,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7770457863807678,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8014718890190125,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.12903225806451613,
        "exact_match": 0.0,
        "bleu": 0.1332832181149491,
        "rouge_l": 0.2702702702702703,
        "bert_score": 0.84388267993927,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      }
    }
  },
  {
    "model_name": "yellow-run-flan-t5-base-sci-cp-en-600k-steps-lr0001-clip1",
    "run_type": "yellow",
    "steps": 600000,
    "detailed_results": [
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "T .  Aj at T studyu T . . . . .. Dis. . . .",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": "t .  aj at t studyu t . . . . .. dis. . . .",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7617000937461853
        }
      },
      {
        "prompt": "cola: Book was by John put the on table.",
        "expected": "unacceptable",
        "generated": ":  peri John peri p : a peri peri John Thomas peri John p.",
        "task_type": "cola",
        "difficulty": "medium",
        "normalized_generated": ":  peri john peri p : a peri peri john thomas peri john p.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7856240272521973
        }
      },
      {
        "prompt": "sst2: This movie is absolutely fantastic!",
        "expected": "positive",
        "generated": "s.ssertionssssserG ssr1:1 s tionsn n ss",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": "s.ssertionssssserg ssr1:1 s tionsn n ss",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7651715278625488
        }
      },
      {
        "prompt": "sst2: This film is terrible and boring.",
        "expected": "negative",
        "generated": "m and boring.",
        "task_type": "sst2",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8474873304367065
        }
      },
      {
        "prompt": "mrpc sentence1: The cat is sleeping on the couch. sentence2: A feline is resting on the sofa.",
        "expected": "equivalent",
        "generated": ".: A feline is resting on the sofa.: A feline is resting on the sofa. cat is resting on the sofa. cat is resting",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": ".: a feline is resting on the sofa.: a feline is resting on the sofa. cat is resting on the sofa. cat is resting",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.828902542591095
        }
      },
      {
        "prompt": "mrpc sentence1: The weather is nice today. sentence2: I like pizza very much.",
        "expected": "not_equivalent",
        "generated": "..... . : ther..: gues isolo gps",
        "task_type": "mrpc",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7628028392791748
        }
      },
      {
        "prompt": "qqp question1: How do I cook pasta? question2: What is the best way to prepare spaghetti?",
        "expected": "equivalent",
        "generated": "qqp qqp question2: What is the best way to prepare pasta? qqp  qqp",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": "qqp qqp question2: what is the best way to prepare pasta? qqp  qqp",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8134045004844666
        }
      },
      {
        "prompt": "qqp question1: What is the capital of France? question2: How do you make coffee?",
        "expected": "not_equivalent",
        "generated": "qqp qqp qqps rie?aine s qqps?ry?s",
        "task_type": "qqp",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7560229301452637
        }
      },
      {
        "prompt": "stsb sentence1: The dog is running in the park. sentence2: A canine is jogging in the garden.",
        "expected": "4.2",
        "generated": "",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "stsb sentence1: I love reading books. sentence2: Cars are expensive nowadays.",
        "expected": "0.1",
        "generated": "ty ts tRP",
        "task_type": "stsb",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8497572541236877
        }
      },
      {
        "prompt": "mnli premise: All birds can fly. hypothesis: Penguins can fly.",
        "expected": "contradiction",
        "generated": "y   y   y y.    . y armed bird",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7632494568824768
        }
      },
      {
        "prompt": "mnli premise: The library is open today. hypothesis: You can borrow books today.",
        "expected": "entailment",
        "generated": "",
        "task_type": "mnli",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "qnli question: What is the capital of Germany? sentence: Berlin is a major European city with rich history.",
        "expected": "entailment",
        "generated": ". : Berlin is a major European city with rich history.: Berlin is a major European city with rich history. qnli Berlin",
        "task_type": "qnli",
        "difficulty": "medium",
        "normalized_generated": ". : berlin is a major european city with rich history.: berlin is a major european city with rich history. qnli berlin",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7853619456291199
        }
      },
      {
        "prompt": "rte premise: Scientists discovered a new species. hypothesis: A biological finding was made.",
        "expected": "entailment",
        "generated": ".",
        "task_type": "rte",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7845907211303711
        }
      },
      {
        "prompt": "wnli sentence1: The trophy didn't fit in the suitcase because it was too big. sentence2: The trophy was too big.",
        "expected": "entailment",
        "generated": "onononon onon sentence1 sentence2 sentence3 sentence4 sentence5 sentence6 sentence7 sentence8 sentence9 sentence10 sentence11 sentence12 sentence sentence sentence",
        "task_type": "wnli",
        "difficulty": "hard",
        "normalized_generated": "not_entailment",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.6666666666666666,
          "rouge_1": 0.6666666666666666,
          "rouge_2": 0.0,
          "bert_score": 0.8495611548423767
        }
      },
      {
        "prompt": "summarize: The immune system protects the body against diseases by recognizing and attacking harmful substances. White blood cells are key components that identify and destroy pathogens like bacteria and viruses.",
        "expected": "The immune system uses white blood cells to protect against diseases",
        "generated": "1/2",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.77785325050354
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term changes in global temperatures and weather patterns. Human activities, particularly burning fossil fuels, have been the main cause of climate change since the 1800s.",
        "expected": "Climate change involves temperature shifts mainly caused by human fossil fuel use",
        "generated": ": Climate change : Climate Change: : :  up:  up: Climate summarize:  up: climate change up up: up: up: up sum: up: climate change up:",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.2,
          "exact_match": 0.0,
          "bleu": 0.08,
          "rouge_l": 0.12121212121212123,
          "rouge_1": 0.12121212121212123,
          "rouge_2": 0.06451612903225806,
          "bert_score": 0.7727072834968567
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": ".... .. . com. . A. SUMMIT ET AL. S YEON1. MEDY. A1..",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7892316579818726
        }
      },
      {
        "prompt": "translate English to German: I am a student at the university.",
        "expected": "Ich bin ein Student an der Universit\u00e4t",
        "generated": "BaselXII.  flow chart. .",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.790803074836731
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "This and s Fridays are open on Sundays. s. or Fridays is open. . or or on  , . to e-mail",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.07407407407407407,
          "rouge_1": 0.07407407407407407,
          "rouge_2": 0.0,
          "bert_score": 0.7764381766319275
        }
      },
      {
        "prompt": "translate English to French: Good morning, how are you?",
        "expected": "Bonjour, comment allez-vous?",
        "generated": "will, French-to- & les autres, \u00e9ditions aide- English, semi- French,  semi-, enfin  French, will - Summary -",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7649105787277222
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "to English translations of German are:.. slang is German to English: ich bin es German - ist.:.. .",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7940204739570618
        }
      },
      {
        "prompt": "translate French to English: Bonjour, comment \u00e7a va?",
        "expected": "Hello, how are you?",
        "generated": ", will this book be easier to read?o, Comment translate from French to: tenir vous lernen? Pourquoi vous? Comment rient French?, will: Vous, Vous?",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7993502616882324
        }
      },
      {
        "prompt": "squad question: What is photosynthesis? context: Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen.",
        "expected": "the process by which plants use sunlight, water, and carbon dioxide to create glucose and oxygen",
        "generated": ":: Photosynthesis squad  squad squad squad: Photosynthesis squad: Photosynthesis squad squad squad squad: Photosynthesis context:",
        "task_type": "squad",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7935789227485657
        }
      },
      {
        "prompt": "squad question: What is the capital of France? context: Paris is the capital and most populous city of France. It is located in northern France.",
        "expected": "Paris",
        "generated": ": Paris is the capital of France. It is located in northern France. It is located in northern France. It is the capital of France. It is located in northern France. It is Paris",
        "task_type": "squad",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.16666666666666669,
          "exact_match": 0.0,
          "bleu": 0.029411764705882353,
          "rouge_l": 0.05882352941176471,
          "rouge_1": 0.05882352941176471,
          "rouge_2": 0.0,
          "bert_score": 0.7814373970031738
        }
      },
      {
        "prompt": "sentiment: This movie is absolutely fantastic and I loved every minute of it!",
        "expected": "positive",
        "generated": ", a . This , This way,",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": ", a . this , this way,",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7960024476051331
        }
      },
      {
        "prompt": "sentiment: This product is terrible and broke immediately.",
        "expected": "negative",
        "generated": "we    : We are very",
        "task_type": "classification",
        "difficulty": "easy",
        "normalized_generated": "we    : we are very",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8138254880905151
        }
      },
      {
        "prompt": "cola: The book was put by John on the table.",
        "expected": "acceptable",
        "generated": "T .  Aj at T studyu T . . . . .. Dis. . . .",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": "t .  aj at t studyu t . . . . .. dis. . . .",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7617000937461853
        }
      },
      {
        "prompt": "cola: Was put the book by John on table the.",
        "expected": "unacceptable",
        "generated": "e John on table.    :  this  : :     The   the",
        "task_type": "linguistic_acceptability",
        "difficulty": "medium",
        "normalized_generated": "e john on table.    :  this  : :     the   the",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7637336254119873
        }
      },
      {
        "prompt": "summarize: The immune system is a complex network of cells and proteins that defends the body against infection. The immune system keeps a record of every germ it has defeated so it can recognize and destroy the germ quickly if it enters the body again.",
        "expected": "The immune system protects against infections and remembers past germs",
        "generated": "",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "summarize: Climate change refers to long-term shifts in global temperatures and weather patterns. While climate change is a natural phenomenon, scientific evidence shows that human activities have been the main driver since the 1800s.",
        "expected": "Climate change involves long-term temperature shifts, mainly caused by human activities since 1800s",
        "generated": "1/2 1/2",
        "task_type": "summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7752240896224976
        }
      },
      {
        "prompt": "translate English to German: The weather is beautiful today.",
        "expected": "Das Wetter ist heute sch\u00f6n",
        "generated": ".... .. . com. . A. SUMMIT ET AL. S YEON1. MEDY. A1..",
        "task_type": "translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7892316579818726
        }
      },
      {
        "prompt": "translate German to English: Ich bin ein Student.",
        "expected": "I am a student",
        "generated": "to English translations of German are:.. slang is German to English: ich bin es German - ist.:.. .",
        "task_type": "translation",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7940204739570618
        }
      },
      {
        "prompt": "translate English to French: The library is closed on Sundays.",
        "expected": "La biblioth\u00e8que est ferm\u00e9e le dimanche",
        "generated": "This and s Fridays are open on Sundays. s. or Fridays is open. . or or on  , . to e-mail",
        "task_type": "translation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.07407407407407407,
          "rouge_1": 0.07407407407407407,
          "rouge_2": 0.0,
          "bert_score": 0.7764381766319275
        }
      },
      {
        "prompt": "paraphrase: The scientist conducted an important experiment.",
        "expected": "The researcher performed a significant test",
        "generated": ". In paraphrasing. In paraphrase paraphrase: .  paraphrase. In . This is, in In . . In. In .",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8093869090080261
        }
      },
      {
        "prompt": "paraphrase: Machine learning algorithms can analyze large datasets.",
        "expected": "ML systems are able to process big data collections",
        "generated": "repo's paraphrase: .: Machine learning algorithms can analyze large datasets can Paraphrase: Para-upupupup associations",
        "task_type": "paraphrasing",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8510769009590149
        }
      },
      {
        "prompt": "complete: The capital of France is",
        "expected": "Paris",
        "generated": "complete. Completely complete, the next of [seven] years",
        "task_type": "completion",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.837095320224762
        }
      },
      {
        "prompt": "complete: Photosynthesis is the process by which plants",
        "expected": "convert sunlight into energy",
        "generated": ",,,,, complete, and complete,",
        "task_type": "completion",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8185220956802368
        }
      },
      {
        "prompt": "grammar: I are going to the store tomorrow.",
        "expected": "I am going to the store tomorrow",
        "generated": "\u2019s be the same time everyof the time of the transfer: I",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.25,
          "exact_match": 0.0,
          "bleu": 0.16666666666666666,
          "rouge_l": 0.10526315789473685,
          "rouge_1": 0.2105263157894737,
          "rouge_2": 0.0,
          "bert_score": 0.842566728591919
        }
      },
      {
        "prompt": "grammar: She don't like pizza very much.",
        "expected": "She doesn't like pizza very much",
        "generated": "ry .    2019.",
        "task_type": "grammar_correction",
        "difficulty": "easy",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7995960116386414
        }
      },
      {
        "prompt": "question: Paris is the capital of France.",
        "expected": "What is the capital of France?",
        "generated": ",obt more to the, to the Expert to the",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.16666666666666666,
          "exact_match": 0.0,
          "bleu": 0.1111111111111111,
          "rouge_l": 0.13333333333333333,
          "rouge_1": 0.13333333333333333,
          "rouge_2": 0.0,
          "bert_score": 0.7988811135292053
        }
      },
      {
        "prompt": "question: Plants use photosynthesis to make food.",
        "expected": "How do plants make food?",
        "generated": "plantss plantssukses.s. In.s. In for",
        "task_type": "question_generation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8079431653022766
        }
      },
      {
        "prompt": "stsb sentence1: The cat is sleeping. sentence2: A feline is resting.",
        "expected": "4.2",
        "generated": "asc.... stsb....... 1 1 ..",
        "task_type": "similarity",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8287197351455688
        }
      },
      {
        "prompt": "rte premise: All birds have wings. hypothesis: Penguins have wings.",
        "expected": "entailment",
        "generated": "rings have wings. ring: All Penguins have wings.",
        "task_type": "entailment",
        "difficulty": "medium",
        "normalized_generated": "rings have wings. ring: all penguins have wings.",
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.8045092821121216
        }
      },
      {
        "prompt": "summarize: DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for the development, functioning, growth and reproduction of all known living organisms and many viruses.",
        "expected": "DNA carries genetic instructions for all living organisms",
        "generated": "",
        "task_type": "scientific_summarization",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "translate English to German: The experiment showed significant results.",
        "expected": "Das Experiment zeigte signifikante Ergebnisse",
        "generated": "to German: to-go.org/. .",
        "task_type": "scientific_translation",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7884479761123657
        }
      },
      {
        "prompt": "explain: How does CRISPR gene editing work?",
        "expected": "CRISPR uses guide RNA to target specific DNA sequences and Cas proteins to cut the DNA, allowing precise insertion, deletion, or modification of genes.",
        "generated": "CRISPR CRISPR CRISPR CRISPRPR CRISPR CRISPR CRISPR CRISPR CRISPR CRISPR",
        "task_type": "scientific_explanation",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.08,
          "exact_match": 0.0,
          "bleu": 0.02465969639416065,
          "rouge_l": 0.058823529411764705,
          "rouge_1": 0.058823529411764705,
          "rouge_2": 0.0,
          "bert_score": 0.7985490560531616
        }
      },
      {
        "prompt": "summarize: Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to process information. Unlike classical bits that exist in definite states, quantum bits (qubits) can exist in multiple states simultaneously, enabling quantum computers to perform certain calculations exponentially faster than classical computers.",
        "expected": "Quantum computing uses superposition and entanglement of qubits to perform calculations faster than classical computers by processing multiple states simultaneously.",
        "generated": "1/2",
        "task_type": "scientific_summarization",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7797814607620239
        }
      },
      {
        "prompt": "hypothesis: What would happen if we removed all bacteria from soil?",
        "expected": "Removing all bacteria from soil would disrupt nutrient cycling, prevent decomposition of organic matter, and severely damage plant growth and ecosystem health.",
        "generated": "logy?",
        "task_type": "scientific_reasoning",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7994787096977234
        }
      },
      {
        "prompt": "methodology: Describe steps to measure protein concentration using Bradford assay.",
        "expected": "Prepare protein standards, add Bradford reagent to samples and standards, incubate for color development, measure absorbance at 595nm, and calculate concentration using standard curve.",
        "generated": "",
        "task_type": "scientific_methodology",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "analyze: The enzyme activity increased 5-fold at pH 8 compared to pH 6. What does this suggest about the enzyme's optimal conditions?",
        "expected": "This suggests the enzyme has optimal activity in alkaline conditions around pH 8, likely due to proper protein folding and active site configuration at higher pH.",
        "generated": "",
        "task_type": "scientific_analysis",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "paraphrase: Natural selection is the differential survival and reproduction of individuals due to differences in phenotype.",
        "expected": "Individuals with favorable traits are more likely to survive and reproduce, passing those advantageous characteristics to their offspring.",
        "generated": "",
        "task_type": "scientific_paraphrase",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "question: Given this research abstract: 'Our study demonstrates that machine learning algorithms can predict drug-protein interactions with 87% accuracy using molecular descriptors.' What are the key findings?",
        "expected": "The key finding is that machine learning algorithms achieve 87% accuracy in predicting drug-protein interactions when using molecular descriptors as input features.",
        "generated": ".",
        "task_type": "scientific_qa",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.7845422625541687
        }
      },
      {
        "prompt": "define: What is the difference between genotype and phenotype?",
        "expected": "Genotype refers to the genetic makeup or DNA sequence of an organism, while phenotype refers to the observable physical or biochemical characteristics resulting from gene expression and environmental factors.",
        "generated": "",
        "task_type": "scientific_definition",
        "difficulty": "medium",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.0
        }
      },
      {
        "prompt": "predict: If ocean temperatures rise by 2\u00b0C, what impact might this have on coral reefs?",
        "expected": "A 2\u00b0C ocean temperature rise would likely cause widespread coral bleaching, increased coral mortality, and disruption of marine ecosystems dependent on coral reefs.",
        "generated": "to to to to to climate change reefs mea:,?st: To predictably, they predict and: To predict",
        "task_type": "scientific_prediction",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0.048780487804878044,
          "rouge_1": 0.09756097560975609,
          "rouge_2": 0.0,
          "bert_score": 0.7907702922821045
        }
      },
      {
        "prompt": "conclude: After treating cancer cells with compound X, cell viability decreased to 20% while control cells remained at 95% viability. What conclusion can be drawn?",
        "expected": "Compound X demonstrates significant cytotoxic effects against cancer cells, reducing viability to 20%, suggesting potential as an anticancer therapeutic agent.",
        "generated": ":",
        "task_type": "scientific_conclusion",
        "difficulty": "hard",
        "normalized_generated": null,
        "scientific_metrics": {
          "token_f1": 0.0,
          "exact_match": 0.0,
          "bleu": 0.0,
          "rouge_l": 0,
          "rouge_1": 0.0,
          "rouge_2": 0.0,
          "bert_score": 0.784126877784729
        }
      }
    ],
    "scientific_metrics": {
      "token_f1": 0.015416666666666665,
      "exact_match": 0.0,
      "bleu": 0.007354450694246799,
      "rouge_l": 0.02394733881934667,
      "rouge_1": 0.02669811820683979,
      "rouge_2": 0.001152073732718894,
      "bert_score": 0.681931024151189,
      "empty_ratio": 0.14285714285714285
    },
    "scientific_by_task": {
      "cola": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7736620604991913,
        "count": 2
      },
      "sst2": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8063294291496277,
        "count": 2
      },
      "mrpc": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7958526909351349,
        "count": 2
      },
      "qqp": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7847137153148651,
        "count": 2
      },
      "stsb": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.42487862706184387,
        "count": 2
      },
      "mnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.3816247284412384,
        "count": 2
      },
      "qnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7853619456291199,
        "count": 1
      },
      "rte": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7845907211303711,
        "count": 1
      },
      "wnli": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.6666666666666666,
        "bert_score": 0.8495611548423767,
        "count": 1
      },
      "summarization": {
        "token_f1": 0.05,
        "exact_match": 0.0,
        "bleu": 0.02,
        "rouge_l": 0.030303030303030307,
        "bert_score": 0.5814461559057236,
        "count": 4
      },
      "translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.016460905349794237,
        "bert_score": 0.7860493924882677,
        "count": 9
      },
      "squad": {
        "token_f1": 0.08333333333333334,
        "exact_match": 0.0,
        "bleu": 0.014705882352941176,
        "rouge_l": 0.029411764705882356,
        "bert_score": 0.7875081598758698,
        "count": 2
      },
      "classification": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8049139678478241,
        "count": 2
      },
      "linguistic_acceptability": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7627168595790863,
        "count": 2
      },
      "paraphrasing": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8302319049835205,
        "count": 2
      },
      "completion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8278087079524994,
        "count": 2
      },
      "grammar_correction": {
        "token_f1": 0.125,
        "exact_match": 0.0,
        "bleu": 0.08333333333333333,
        "rouge_l": 0.052631578947368425,
        "bert_score": 0.8210813701152802,
        "count": 2
      },
      "question_generation": {
        "token_f1": 0.08333333333333333,
        "exact_match": 0.0,
        "bleu": 0.05555555555555555,
        "rouge_l": 0.06666666666666667,
        "bert_score": 0.803412139415741,
        "count": 2
      },
      "similarity": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8287197351455688,
        "count": 1
      },
      "entailment": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.8045092821121216,
        "count": 1
      },
      "scientific_summarization": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.38989073038101196,
        "count": 2
      },
      "scientific_translation": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7884479761123657,
        "count": 1
      },
      "scientific_explanation": {
        "token_f1": 0.08,
        "exact_match": 0.0,
        "bleu": 0.02465969639416065,
        "rouge_l": 0.058823529411764705,
        "bert_score": 0.7985490560531616,
        "count": 1
      },
      "scientific_reasoning": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7994787096977234,
        "count": 1
      },
      "scientific_methodology": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_analysis": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_paraphrase": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_qa": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.7845422625541687,
        "count": 1
      },
      "scientific_definition": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.0,
        "count": 1
      },
      "scientific_prediction": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.048780487804878044,
        "bert_score": 0.7907702922821045,
        "count": 1
      },
      "scientific_conclusion": {
        "token_f1": 0.0,
        "exact_match": 0.0,
        "bleu": 0.0,
        "rouge_l": 0.0,
        "bert_score": 0.784126877784729,
        "count": 1
      }
    }
  }
]